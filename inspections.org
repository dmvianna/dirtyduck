#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Center of Data Science for Public Policy
#+EMAIL: adolfo@uchicago.edu
#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:shell     :results drawer
#+PROPERTY: header-args:ipython   :session :exports both :results raw drawer
#+PROPERTY: header-args:python    :session food_inspections :results output drawer

* Problem description

The scenario, is the following:  you work for the City of Chicago and you try
  to prioritize your resources (i.e. your inspection workforce), since
  they are limited. So, you will use the data for answering the next question:

#+begin_quote
Which X facilities are likely to violate some code in the
  following Y period of time?
#+end_quote

  In this case maybe you are interested not
  in all the violations but in the more severe ones.

* Creating the labels

Let's remember how the =events= table looks like:

 #+begin_src sql
 \d semantic.events
 #+end_src

 #+RESULTS:
 :RESULTS:
 | Table "semantic.events"                                                                                         |                   |           |
 |-----------------------------------------------------------------------------------------------------------------+-------------------+-----------|
 | Column                                                                                                          | Type              | Modifiers |
 | inspection                                                                                                      | character varying |           |
 | entity_id                                                                                                        | bigint            |           |
 | type                                                                                                            | text              |           |
 | date                                                                                                            | date              |           |
 | risk                                                                                                            | text              |           |
 | result                                                                                                          | text              |           |
 | facility_type                                                                                                    | text              |           |
 | zip_code                                                                                                         | character varying |           |
 | location                                                                                                        | geometry          |           |
 | violations                                                                                                      | jsonb             |           |
 | Indexes:                                                                                                        |                   |           |
 | "events_date_ix" btree (date DESC NULLS LAST)                                                                     |                   |           |
 | "events_entity_ix" btree (entity_id)                                                                               |                   |           |
 | "events_facility_type_ix" btree (facility_type)                                                                     |                   |           |
 | "events_inspection_entity_zip_code_date" btree (inspection DESC NULLS LAST, entity_id, zip_code, date DESC NULLS LAST) |                   |           |
 | "events_inspection_ix" btree (inspection)                                                                         |                   |           |
 | "events_location_gix" gist (location)                                                                             |                   |           |
 | "events_type_ix" btree (type)                                                                                     |                   |           |
 | "events_violations" gin (violations)                                                                             |                   |           |
 | "events_violations_json_path" gin (violations jsonb_path_ops)                                                        |                   |           |
 | "events_zip_code_ix" btree (zip_code)                                                                               |                   |           |
 :END:

We will define two different labels:

- *Which facilities are likely to fail an inspection?*

Facilities who failed an inspection (i.e. =result= = ='fail'=)

- *Which facilities are likely  to fail an inspection with a major  violation?*

Critical violations are coded between =1-14=, serious violations between
=15-29=, everything above =30= is assumed to be a minor violation.

Facilities who failed an inspection (i.e. =result= = ='fail'=) and the
=severity in ('critical', 'serious')=

We could extract the severity of the violation inspected using the
following code:


#+begin_src sql

select inspection, result, array_agg(obj ->>'severity'),
(result = 'fail') as failed,
(result = 'fail' and
('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
) as failed_major_violation
from
(select inspection, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events limit 20)
as t1
group by inspection, result

#+end_src

#+RESULTS:
:RESULTS:
| inspection | result | array_agg                                                 | failed | failed_major_violation |
|------------+--------+----------------------------------------------------------+--------+----------------------|
|    1763967 | fail   | {critical,serious,serious,minor,minor,minor,minor,minor} | t      | t                    |
|    1770568 | pass   | {critical,serious,serious,minor,minor}                   | f      | f                    |
|    1343315 | fail   | {serious,serious,serious,serious,minor,minor}            | t      | t                    |
|     537439 | fail   | {NULL}                                                   | t      | [NULL]               |
:END:


Let's use the previous query to generate our labels in a new
=inspections= schema.

=Triage= has some restrictions (at the current version) about how to
name (some) columns, in specific, our columns should include:

- =entity_id=     :: The entity affected / causing the event (In our
     case the facility)
- =outcome_date=  :: The date in which the event happen / The date in
     which we discover the result (The inpection's date)
- =outcome=       :: The result (label) of the event (One of the labels
     speecified before)

=entity_id= an identifier for which the labels are applied to,
=outcome_date= the date at which some outcome was known, =outcome= a
boolean outcome.

Since we defined two labels, we will create two tables one per each label.

#+BEGIN_SRC sql :tangle ./src/create_inspections_schema.sql

create schema if not exists inspections;

drop table if exists inspections.labels;

create table inspections.labels as (
select inspection, entity_id, date,
   (result = 'fail') as failed,
   (result = 'fail' and
       ('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
   ) as failed_major_violation
from
   (select inspection, entity_id, date, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events)
as t1
group by inspection, entity_id, date, result
);


drop table if exists inspections.failed;

create table inspections.failed as (
select
entity_id,
date as outcome_date,
failed as outcome
from inspections.labels
);


drop table if exists inspections.failed_major_violation;

create table inspections.failed_major_violation as (
select
entity_id,
date as outcome_date,
failed_major_violation as outcome
from inspections.labels
);

#+END_SRC

#+RESULTS:

Also, We need to create a new version of the =semantic.entities=
table. =Triage= refers to this new table as the *states* table. It should
have columns =entity_id=, =start__time, end_time= and =state=.
The states table allows us to only
include rows in your matrices in a specific state. In our case we only want
to inspect *active* facilities. We will replace all the =NULL= values in
the =end_time= column for a date in the future, in particular =2020-12-31=.

#+BEGIN_SRC sql :tangle ./src/create_inspections_schema.sql

drop table if exists inspections.active_facilities;

create table inspections.active_facilities as (
select
distinct
entity_id, 'active'::VARCHAR  as state, start_time, coalesce(end_time, '2020-12-31'::date) as end_time
from semantic.entities
);
#+END_SRC

#+RESULTS:


* Modeling using Machine Learning

It is time of getting all the previous steps and put them
together. Don't worry, actually we are done with coding. =Triage= provides
you with a configuration file for specifying the experiment that we
want to run.

** Creating a simple experiment

We will use the subset table =triage.test= that we were using in
[[file:triage_intro.org][Introduction to triage]] . For this first experiment we will try one of the simplest
machine learning algorithms: a *Decision Tree Classifier*. We need to
write the experiment config file for that, let's break it down and
explain all the sections.

The config file for this first experiment is located in
[[src/inspections_test.yaml]].


The first lines of the experiment config file are related to the
version config file (=v3= at the moment of writing this tutorial), a
comment (=model_comment=), this will end up as
a value in the =results.models= table, and a list of user defined
metadata (=user_metadata=) that could be used for identifying the
resulting model groups. In our test example, if you run experiments that share
a temporal configuration but that use different label definitions
(say, labeling building inspections with *any* violation as positive or
labeling only building inspections with major violations as positive),
you can use the user metadata keys to indicate that the matrices
from these experiments have different labeling criteria. The matrices from the
two experiments will have different filenames (and not be overwritten or
inappropriately reused), and if you add the =label_definition= key to
the =model_group_keys=, models made on different label definition will
have different groups.

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
config_version: 'v3'

model_comment: 'inspections_test'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'test'
#+END_SRC

Next, the *temporal configuration*  section. The first four parameters
are related to the availability of data: How much data you have for
feature creation? How much data you have for label generation? For
simplicity we will assume that we can use the full =triage.test= time
span for both.

#+BEGIN_SRC sql
select min(date), max(date) from triage.test
#+END_SRC

#+RESULTS:
:RESULTS:
|        min |        max |
|------------+------------|
| 2010-02-24 | 2017-02-21 |
:END:



The next parameters are related to the training intervals:
- How frequently to retrain models? (=model_update_frequency=)
- How many rows per entity in the train matrices?
  (=training_as_of_date_frequencies=)
- How much time is covered by labels in the training matrices? (=training_label_timespans=)

The remaining elements are related to the *testing* matrices, in the
particular case of *inspections*, you can choose them as follows:

- =test_as_of_date_frequencies= is planning/scheduling frequency
- =test_durations= is how far out are you scheduling for?
- =test_label_timespan= is equal to =test_durations=

Let's assume that we need to do rounds of inspections every month
(=test_as_of_date_frequencies = 1month=) and we need to complete that
round in exactly one month (=test_durations = test_label_timespan =
1month=)

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
temporal_config:
    feature_start_time: '2015-02-01'
    feature_end_time: '2017-02-01'
    label_start_time: '2015-02-01'
    label_end_time: '2017-02-01'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '1month'
    test_label_timespans: ['1month']
    test_as_of_date_frequencies: '1month'

    max_training_histories: '5y'
#+END_SRC

We can visualize the splitting using the function =show_timechop=
introduced in [[file:triage_intro.org][Introduction to triage]].


#+BEGIN_SRC python :results file drawer
import utils

utils.show_timechop(experiment.chopper, file_name="timechop_inspections_test.png")

"timechop_inspections_test.png"
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:timechop_inspections_test.png]]
:END:



We need to specify the table that keeps our labels, for this first
experiment we will use the label =failed=, stored in =inspections.labels=.

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
events_table: inspections.failed
#+END_SRC

=Triage= will generate the features for us, we need to tell which ones
in the section =feature_aggregations=. Here, each entry describes a
=collate.SpacetimeAggregation= object, and the
arguments needed to create it. For this experiment we will try the following
features:

- Number of different types of inspections  that happened in the
  facility in the last year from a particular day
- Number of different types of inspections  that happened in the
  zip code in the last year from a particular day

If we observe the image generated from the =temporal_config= section,
each particular date is the beginning of the rectangles that describes
the rows in the matrix. In that date (=as_of_date= in =timechop= parlance)
we will calculate both features, and we will repeat that for every
other rectangle in that image.

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
feature_aggregations:
    -
        prefix: 'inspections'
        from_obj: 'triage.test'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'inspection_type'
                choice_query: 'select distinct inspection_type from triage.test where inspection_type is not null'
                metrics:
                    - 'sum'

        intervals:
            - '1y'

        groups:
            - 'entity_id'
            - 'zip_code'
#+END_SRC

We just want to include *active* facilities in our matrices, so we tell
=triage= to take that in account:

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
state_config:
    table_name: 'inspections.active_facilities'
    state_filters:
       - 'active'
#+END_SRC

Now, lets discuss how we will define the different models to try in
the data (Remember that the model is specified by the algorithm, the
hyperparameters, and the subset of features to use). In =triage= you
need to specify in the =grid_config= section, a list of machine learning
algorithms that you want to train, and a set of list of
hyperparameters. You can use any algorithm that you want, the only
requirement is that respects the =sklearn= API.


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
grid_config:
    'sklearn.tree.DecisionTreeClassifier':
        max_depth: [1,null]
        max_features: [1, sqrt, null]
#+END_SRC

Some of the parameters in =sklearn= are =None=, if you want to try those
you need to indicate that with the =yaml= 's =null= keyword.

Besides the algorithm and the hyperparameters, you should specify
which subset of features use. First, in the section
=feature_group_definition= you specify how to group the features (you
can use the =table name= or the =prefix= from the section
=feature_aggregation=) and then choose one /strategy/ for choosing the
subsets: =all= (all the subsets at once), =leave-one-out= (try all the
subsets except one, do that for all the combinations) or =leave-one-in=
(just try subset at the time).


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml

feature_group_definition:
   prefix: ['inspections']

feature_group_strategies: ['all']
#+END_SRC

In this experiment we will end with *6* model groups ($algorithms (1) \times
hyperparameters combinations (2 \times 3)  \times feature groups (1) \times temporal
combinations (1)$). Also, we will create *12* different models (2 per
each model group) given that we have 2 temporal blocks (one model per
temporal group).


=model_group_keys= defines a list of *additional* matrix metadata keys that
should be considered when creating a model group. For example, if the models are
built on matrices with different history lengths, different
labeling windows (e.g., inspection violations in the next month, next year, or
next two years), the frequency of rows for each
entity, or the definition of a positive label (=label_definition=, from
=user_metadata=).

The valid =model_group_keys= are

- =beginning_of_time=,
- =end_time=,
- =indices=,
- =feature_names=,
- =label_name=,
- =label_type=,
- =state=,
- =matrix_id=,
- =matrix_type=

- =matrix_start_time=,
- =matrix_end_time=,
- =as_of_times=,
- =label_window=,
- =example_frequency=,
- =train_duration=


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
model_group_keys:
    - 'label_definition'
    - 'experiment_type'
#+END_SRC

Finally, we should define wich metrics we care for evaluating our
model. Here we will concentrate only in =precision= and =recall=.

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
scoring:
    sort_seed: 5
    metric_groups:
        -
            metrics: [precision@, recall@]
            thresholds:
                percentiles: [5.0, 10.0]
                top_n: [5, 10, 25]
#+END_SRC

We will want as a result of our experiments, a *list* of facilities to
be inspected. The length of our list is contrained by our inspection
resources, i.e. the answer to the question How many facilities can I
inpect in a month?. In this experiment we are assuming that the
maximum capacity is *25* but we are testing also for a list of length
*5*, and *10* (see =top_n= above).

You can execute the experiment as

#+BEGIN_SRC python
experiment.run()
#+END_SRC

This will print a lot of output, and if everything is correct it will
populate the =results= schema, it will create 4 matrices (2 for
training, 2 for testing) in
=triage/matrices= and store 12 models in =triage/trained_models=.

Every matrix will be represented by two files, one with the metadata
of the matrix (a =yaml= file) and the actual matrix (the =csv= file).

You can check with which matrix the models where trained

#+BEGIN_SRC sql
select
model_id, model_group_id, model_hash,
train_end_time, train_matrix_uuid
from results.models
order by model_group_id, train_end_time asc
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | model_group_id | model_hash                        | train_end_time        | train_matrix_uuid                  |
|---------+--------------+----------------------------------+---------------------+----------------------------------|
|       1 |            1 | 243410f30a8f65afe1a973ba56b80f20 | 2015-12-01 00:00:00 | 8190bdffd7f8bf3012fa86f0361e50e4 |
|       7 |            1 | e82dfefda1870a4def36777632278bb7 | 2016-12-01 00:00:00 | b43a980854e099571529470fa939db8a |
|       2 |            2 | 58e5ae64e6bfee31a4bc49ac9dc6cfd6 | 2015-12-01 00:00:00 | 8190bdffd7f8bf3012fa86f0361e50e4 |
|       8 |            2 | 372c74795329124d375071c179b7d02b | 2016-12-01 00:00:00 | b43a980854e099571529470fa939db8a |
|       3 |            3 | c0be0b9246c248fbc2e364e1fded4550 | 2015-12-01 00:00:00 | 8190bdffd7f8bf3012fa86f0361e50e4 |
|       9 |            3 | 05e5f34055c2eea4feb21da9cace695e | 2016-12-01 00:00:00 | b43a980854e099571529470fa939db8a |
|       4 |            4 | 5bf2df1a258f28b3748d5f23e265ebce | 2015-12-01 00:00:00 | 8190bdffd7f8bf3012fa86f0361e50e4 |
|      10 |            4 | b1f99d67b2fd5ee9c6c14f9d15cddfb0 | 2016-12-01 00:00:00 | b43a980854e099571529470fa939db8a |
|       5 |            5 | 681652c9811752c5eb9f05e69c8034d7 | 2015-12-01 00:00:00 | 8190bdffd7f8bf3012fa86f0361e50e4 |
|      11 |            5 | f1df36a2b6d7f61507a15bf5e9b0dc45 | 2016-12-01 00:00:00 | b43a980854e099571529470fa939db8a |
|       6 |            6 | cb49e07115224f80ae72c10472a248da | 2015-12-01 00:00:00 | 8190bdffd7f8bf3012fa86f0361e50e4 |
|      12 |            6 | 3b7e7f380cc6f22cb6f256532e511ce6 | 2016-12-01 00:00:00 | b43a980854e099571529470fa939db8a |
:END:

As expected, we have two models per model group. Each model was trained
with the matrix indicated in the column =train_matrix_uuid=. This =uuid=
also is the file name of the stored matrix. The model itself was
stored under the file named with the =model_hash=.

For example, the model =7= was stored as
=/triage/trained_models/= src_sql{select train_matrix_uuid from
results.models where model_id = 7}
using the standard serialization of sklearn models. This model was
trained with the matrix src_sql{select train_matrix_uuid from
results.models where model_id = 7} stored in the directory
=/triage/matrices=.

The model =7= used the following hyperparameters:

#+BEGIN_SRC sql
select model_parameters from results.models where model_id = 7
#+END_SRC

#+RESULTS:
:RESULTS:
| model_parameters                   |
|-----------------------------------|
| {"max_depth": 1, "max_features": 1} |
:END:


The same model =7= is part of the model group src_sql{select model_group_id
from results.models where model_id = 7}. That model group

#+BEGIN_SRC sql
select model_group_id, model_type, model_config from results.model_groups where model_group_id = 1
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                           | model_config                                             |
|--------------+-------------------------------------+---------------------------------------------------------|
|            1 | sklearn.tree.DecisionTreeClassifier | {"experiment_type": "test", "label_definition": "failed"} |
:END:

The features used by that model are:

#+BEGIN_SRC sql
select unnest(feature_list) from results.model_groups where model_group_id = 1
#+END_SRC

#+RESULTS:
:RESULTS:
| unnest                                          |
|-------------------------------------------------|
| inspections_entity_id_1y_inspection_type_complaint_sum |
| inspections_entity_id_1y_inspection_type__NULL_sum     |
| inspections_zip_code_1y_inspection_type_complaint_sum  |
| inspections_zip_code_1y_inspection_type__NULL_sum      |
:END:

Finally, the performance of the model =7= are:

#+BEGIN_SRC sql
select model_id, metric || parameter as metric, value from results.evaluations where model_id = 7 order by metric || parameter
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | metric            |                value |
|---------+-------------------+----------------------|
|       7 | precision@10.0_pct |  0.16831683168316833 |
|       7 | precision@10_abs   |                  0.0 |
|       7 | precision@25_abs   |                  0.0 |
|       7 | precision@5.0_pct  |  0.15254237288135594 |
|       7 | precision@5_abs    |                  0.0 |
|       7 | recall@10.0_pct    | 0.089473684210526316 |
|       7 | recall@10_abs      |                  0.0 |
|       7 | recall@25_abs      |                  0.0 |
|       7 | recall@5.0_pct     |  0.04736842105263158 |
|       7 | recall@5_abs       |                  0.0 |
:END:

As expected, the model is very bad. Let's continue anyway and assume
that this is our best model, Which is the list of 25 facilities to inspect?


#+BEGIN_SRC sql
select * from results.predictions where model_id = 7;
#+END_SRC

** Defining a baseline

As a second step, lets do a new experiment that defines our
/baseline/. In order to achive this, we will use a similar experiment
config file with the following changes:

#+BEGIN_EXAMPLE yaml
model_comment: 'inspections_baseline'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'baseline'


grid_config:
    'sklearn.dummy.DummyClassifier':
        strategy: [prior,uniform, most_frequent]

model_group_keys:
    - 'label_definition'
    - 'experiment_type'
#+END_EXAMPLE

The complete file is in [[file:src/inspections_baseline.yaml][code/inspections_baseline.yaml]].

If we execute this experiment, we will get 3 more model groups (one
for each strategy), and the corresponding 6 new models (2 per each
model group).

#+BEGIN_SRC python

with open('src/inspections_baseline.yaml') as f:
    experiment_config = yaml.load(f)


from triage.component.catwalk.storage import FSModelStorageEngine
from triage.experiments import SingleThreadedExperiment

experiment = SingleThreadedExperiment(
    config=experiment_config,
    db_engine=sqlalchemy.create_engine(food_db),
    model_storage_class=FSModelStorageEngine,
    project_path='triage'
)

experiment.run()
#+END_SRC

#+BEGIN_SRC sql

with baseline as (
select model_id
from results.models
where model_group_id in (7,8,9)
)

select model_id, metric || parameter as metric, value
from results.evaluations
where
model_id in (select * from baseline)
and
metric || parameter = 'precision@10.0_pct'
order by metric || parameter, model_id
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | metric            |               value |
|---------+-------------------+---------------------|
|      13 | precision@10.0_pct |               0.125 |
|      14 | precision@10.0_pct |               0.125 |
|      15 | precision@10.0_pct |               0.125 |
|      16 | precision@10.0_pct | 0.16831683168316833 |
|      17 | precision@10.0_pct | 0.16831683168316833 |
|      18 | precision@10.0_pct | 0.16831683168316833 |
:END:

Again, nothing impressive.

** A more advanced experiment

Ok, let's add a more complete experiment.


#+BEGIN_SRC yaml :tangle src/inspections_label_failed_01.yaml
config_version: 'v3'

model_comment: 'inspections'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'exploratory'
  org: 'DSaPP'
  team: 'Chicago Data Science'
  author: 'Your name here'
#+END_SRC

#+BEGIN_SRC  yaml :tangle src/inspections_label_failed_01.yaml
temporal_config:
    feature_start_time: '2012-02-01'
    feature_end_time: '2017-02-01'
    label_start_time: '2012-02-01'
    label_end_time: '2017-02-01'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '1month'
    test_label_timespans: ['1month']
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_label_failed_01.yaml
events_table: inspections.failed

feature_aggregations:
    -
        prefix: 'inspections'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'type'
                choice_query: 'select distinct type from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'

    -
        prefix: 'risks'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'risk'
                choice_query: 'select distinct risk from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'


    -
        prefix: 'results'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'result'
                choice_query: 'select distinct result from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'



state_config:
    table_name: 'inspections.active_facilities'
    state_filters:
       - 'active'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_label_failed_01.yaml
grid_config:
    'sklearn.ensemble.RandomForestClassifier':
        max_features: [1, 'sqrt']
        criterion: ['gini']
        n_estimators: [10, 100]
        min_samples_split: [2,5,10]
        max_depth: [2,5,10]
        class_weight: [null, 'balanced']
    'sklearn.ensemble.ExtraTreesClassifier':
        max_features: [1,'sqrt']
        criterion: ['gini']
        n_estimators: [10000]
        min_samples_split: [2, 5,10]
        max_depth: [2,5,10]
        n_jobs: [-1]
        class_weight: [null,'balanced', 'balanced_subsample']
    'sklearn.tree.DecisionTreeClassifier':
        criterion: ['gini']
        min_samples_split: [2,5,10]
        class_weight: [null,'balanced']
        max_depth: [1,null]
        max_features: [1, sqrt, null]

feature_group_definition:
   prefix: ['inspections', 'results', 'risks']

feature_group_strategies: ['all', 'leave-one-out', 'leave-one-in']

model_group_keys:
    - 'label_definition'
    - 'experiment_type'
    - 'org'
    - 'team'

scoring:
    sort_seed: 1234
    metric_groups:
        -
            metrics: ['precision@', 'recall@']
            thresholds:
                percentiles: [1.0, 2.0, 5.0, 10.0, 25.0, 50.0, 75.0, 95.0, 100.0]
                top_n: [5, 10, 25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000]

#+END_SRC


** How can I pick the best one?


We are working in ...

But meanwhile, you can try the following


* Notes
[2018-01-01 Mon 00:50]


 /What are you inspecting?/ (people, places, other)
 /How far do you want to predict?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /How often do you want to update the list?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /What do you want to optimize for?/ (e.g. efficiency, long term
 compliance, novelty)


Inspection the join starts from outcomes (outcome centric) (if you
haven't been inspected, we can not said anything about you)
