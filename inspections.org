#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Center of Data Science for Public Policy
#+EMAIL: adolfo@uchicago.edu
#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:shell     :results drawer
#+PROPERTY: header-args:ipython   :session :exports both :results raw drawer
#+PROPERTY: header-args:python    :session food_inspections :results output Drawer
#+PROPERTY: header-args:sh  :results raw drawer

* Problem description

 We will begin with the the *inspections prioritization* problem: we want to generate a list of
   facilities which are /likely/ to have some *critical* o *serious*
   violation /given that/ they are inspected.

The scenario, is the following:  you work for the City of Chicago and you try
  to prioritize your resources (i.e. your inspection workforce), since
  they are limited. So, you will use the data for answering the next question:

#+begin_quote
Which X facilities are likely to violate some code in the
  following Y period of time?
#+end_quote

  In this case maybe you are interested not
  in all the violations but in the more severe ones.

* Creating the labels

We will define two different labels:

- *Which facilities are likely to fail an inspection?*

Facilities who failed an inspection (i.e. =result= = ='fail'=)

- *Which facilities are likely  to fail an inspection with a major  violation?*

Rremember that critical violations are coded between =1-14=, serious violations between
=15-29=, everything above =30= is assumed to be a minor violation.

Facilities who failed an inspection (i.e. =result= = ='fail'=) and the
=severity in ('critical', 'serious')=

We could extract the severity of the violation inspected using the
following code:


#+begin_src sql

select inspection, 
date,
result, 
array_agg(obj ->>'severity') as violations_severity,
(result = 'fail') as failed,
(result = 'fail' and
('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
) as failed_major_violation
from
(select inspection,date, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events limit 20)
as t1
group by inspection, date, result
order by date desc

#+end_src

#+RESULTS:
:RESULTS:
| inspection |       date | result | violations_severity                                       | failed | failed_major_violation |
|------------+------------+--------+----------------------------------------------------------+--------+----------------------|
|    1770568 | 2016-05-11 | pass   | {critical,minor,minor,serious,serious}                   | f      | f                    |
|    1763967 | 2016-05-03 | fail   | {minor,critical,serious,serious,minor,minor,minor,minor} | t      | t                    |
|    1343315 | 2013-06-06 | fail   | {minor,serious,serious,serious,serious,minor}            | t      | t                    |
|     537439 | 2011-06-10 | fail   | {NULL}                                                   | t      | [NULL]               |
:END:


Let's use the previous query to generate our outcomes in a new
=inspections= Schema.

#+BEGIN_SRC sql :tangle ./sql/create_inspections_schema.sql
create schema if not exists inspections;
#+END_SRC

=Triage= has some restrictions (at the current version) about how to
name (some) columns, in specific, our columns should include:

- =entity_id=     :: The entity affected / causing the event (In our
     case the facility)
- =outcome_date=  :: The date in which the event happen / The date in
     which we discover the result (The inpection's date)
- =outcome=       :: The result (label) of the event (One of the labels
     speecified before)

=entity_id= an identifier for which the labels are applied to,
=outcome_date= the date at which some outcome was known, =outcome= a
boolean outcome.

Since we defined two labels, we will create two tables one per each outcome.

#+BEGIN_SRC sql :tangle ./sql/create_inspections_schema.sql
create schema if not exists inspections;

create temp table inspections_outcomes as (
select inspection, entity_id, date,
   (result = 'fail') as failed,
   (result = 'fail' and
       ('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
   ) as failed_major_violation
from
   (select inspection, entity_id, date, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events)
as t1
group by inspection, entity_id, date, result
);


drop table if exists inspections.failed;

create table inspections.failed as (
select
entity_id,
date as outcome_date,
failed as outcome
from inspections_outcomes
);


drop table if exists inspections.failed_major_violation;

create table inspections.failed_major_violation as (
select
entity_id,
date as outcome_date,
failed_major_violation as outcome
from inspections_outcomes
);

#+END_SRC

#+RESULTS:

Also, We need to create a new version of the =semantic.entities=
table. =Triage= refers to this new table as the *states* table. It should
have columns =entity_id=, =start__time, end_time= and =state=.
The states table allows us to only
include rows in your matrices in a specific state. In our case we only want
to inspect *active* facilities. We will replace all the =NULL= values in
the =end_time= column for a date in the future, in particular =2020-12-31=.

#+BEGIN_SRC sql :tangle ./sql/create_inspections_schema.sql

drop table if exists inspections.active_facilities;

create table inspections.active_facilities as (
select
distinct
entity_id, 'active'::VARCHAR  as state, start_time, coalesce(end_time, '2020-12-31'::date) as end_time
from semantic.entities
);
#+END_SRC

#+RESULTS:


* Modeling using Machine Learning

It is time of getting all the previous steps and put them
together. Don't worry, actually we are done with coding. =Triage= provides
you with a configuration file for specifying the experiment that we
want to run.

** Creating a simple experiment

We will use the subset table =triage.test= that we were using in
[[file:triage_intro.org][Introduction to triage]] . For this first experiment we will try one of the simplest
machine learning algorithms: a *Decision Tree Classifier*. We need to
write the experiment config file for that, let's break it down and
explain all the sections.

The config file for this first experiment is located in
[[file:triage/experiment_config/inspections_test.yaml]]


The first lines of the experiment config file are related to the
version config file (=v3= at the moment of writing this tutorial), a
comment (=model_comment=), this will end up as
a value in the =results.models= table, and a list of user defined
metadata (=user_metadata=) that could be used for identifying the
resulting model groups. In our test example, if you run experiments that share
a temporal configuration but that use different label definitions
(say, labeling building inspections with *any* violation as positive or
labeling only building inspections with major violations as positive),
you can use the user metadata keys to indicate that the matrices
from these experiments have different labeling criteria. The matrices from the
two experiments will have different filenames (and not be overwritten or
inappropriately reused), and if you add the =label_definition= key to
the =model_group_keys=, models made on different label definition will
have different groups.

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml
config_version: 'v3'

model_comment: 'inspections_test'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'test'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Adolfo De Unanue'
#+END_SRC

Next, the *temporal configuration*  section. The first four parameters
are related to the availability of data: How much data you have for
feature creation? How much data you have for label generation? For
simplicity we will assume that we can use the full =triage.test= time
span for both.

#+BEGIN_SRC sql
select min(date), max(date) from triage.test
#+END_SRC

#+RESULTS:
:RESULTS:
|        min |        max |
|------------+------------|
| 2010-02-24 | 2017-02-21 |
:END:

The next parameters are related to the training intervals:
- How frequently to retrain models? (=model_update_frequency=)
- How many rows per entity in the train matrices?
  (=training_as_of_date_frequencies=)
- How much time is covered by labels in the training matrices? (=training_label_timespans=)

The remaining elements are related to the *testing* matrices, in the
particular case of *inspections*, you can choose them as follows:

- =test_as_of_date_frequencies= is planning/scheduling frequency
- =test_durations= is how far out are you scheduling for?
- =test_label_timespan= is equal to =test_durations=

Let's assume that we need to do rounds of inspections every month
(=test_as_of_date_frequencies = 1month=) and we need to complete that
round in exactly one month (=test_durations = test_label_timespan =
1month=)

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml
temporal_config:
    feature_start_time: '2010-02-01'
    feature_end_time: '2017-02-01'
    label_start_time: '2015-02-01'
    label_end_time: '2017-02-01'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1month']
    test_as_of_date_frequencies: '1month'

    max_training_histories: '5y'
#+END_SRC

We can visualize the splitting using the function =show_timechop=
introduced in [[file:triage_intro.org][Introduction to triage]].

#+BEGIN_SRC sh 
./tutorial.sh triage --config_file inspections_test.yaml show_temporal_blocks
#+END_SRC

#+RESULTS:
:RESULTS:
Using the config file /triage/experiment_config/inspections_test.yaml
The output (matrices and models) of this experiment will be stored in triage/output
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Generating temporal blocks image
Image stored in:
/triage/inspections_test.svg
:END:

[[./triage/inspections_test.svg]]


We need to specify the table that keeps our labels, for this first
experiment we will use the label =failed=, stored in =inspections.labels=.

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml
events_table: inspections.failed
#+END_SRC

=Triage= will generate the features for us, we need to tell which ones
in the section =feature_aggregations=. Here, each entry describes a
=collate.SpacetimeAggregation= object, and the
arguments needed to create it. For this experiment we will try the following
features:

- Number of different types of inspections  that happened in the
  facility in the last year from a particular day
-
- Number of different types of inspections  that happened in the
  zip code in the last year from a particular day

If we observe the image generated from the =temporal_config= section,
each particular date is the beginning of the rectangles that describes
the rows in the matrix. In that date (=as_of_date= in =timechop= parlance)
we will calculate both features, and we will repeat that for every
other rectangle in that image.

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml
feature_aggregations:
    -
        prefix: 'inspections'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'type'
                choice_query: 'select distinct type from semantic.events where type is not null'
                metrics:
                    - 'sum'

        intervals:
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
#+END_SRC

We just want to include *active* facilities in our matrices, so we tell
=triage= to take that in account:

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml
state_config:
    table_name: 'inspections.active_facilities'
    state_filters:
       - 'active'
#+END_SRC

Now, lets discuss how we will define the different models to try in
the data (Remember that the model is specified by the algorithm, the
hyperparameters, and the subset of features to use). In =triage= you
need to specify in the =grid_config= section, a list of machine learning
algorithms that you want to train, and a set of list of
hyperparameters. You can use any algorithm that you want, the only
requirement is that respects the =sklearn= API.


#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml
grid_config:
    'sklearn.tree.DecisionTreeClassifier':
        max_depth: [1,null]
        max_features: [1, sqrt, null]
#+END_SRC

Some of the parameters in =sklearn= are =None=, if you want to try those
you need to indicate that with the =yaml= 's =null= keyword.

Besides the algorithm and the hyperparameters, you should specify
which subset of features use. First, in the section
=feature_group_definition= you specify how to group the features (you
can use the =table name= or the =prefix= from the section
=feature_aggregation=) and then choose one /strategy/ for choosing the
subsets: =all= (all the subsets at once), =leave-one-out= (try all the
subsets except one, do that for all the combinations) or =leave-one-in=
(just try subset at the time).


#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml

feature_group_definition:
   prefix: ['inspections']

feature_group_strategies: ['all']
#+END_SRC

In this experiment we will end with *6* model groups ($algorithms (1) \times
hyperparameters combinations (2 \times 3)  \times feature groups (1) \times temporal
combinations (1)$). Also, we will create *12* different models (2 per
each model group) given that we have 2 temporal blocks (one model per
temporal group).

=model_group_keys= defines a list of *additional* matrix metadata keys that
should be considered when creating a model group. For example, if the models are
built on matrices with different history lengths, different
labeling windows (e.g., inspection violations in the next month, next year, or
next two years), the frequency of rows for each
entity, or the definition of a positive label (=label_definition=, from
=user_metadata=).

The valid =model_group_keys= are

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml
model_group_keys:
    - 'label_definition'
    - 'experiment_type'
#+END_SRC

Finally, we should define wich metrics we care for evaluating our
model. Here we will concentrate only in =precision= and =recall=.

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_test.yaml
scoring:
    sort_seed: 5
    metric_groups:
        -
            metrics: [precision@, recall@]
            thresholds:
                percentiles: [5.0, 10.0]
                top_n: [5, 10, 25]
#+END_SRC

You should be warned that precision and recall at $k$ in this setting
is kind of ill-defined (because you will end with a lot of =NULL=
labels, remember, only a few of facilities are inspected in each
period) ...

We will want as a result of our experiments, a *list* of facilities to
be inspected. The length of our list is contrained by our inspection
resources, i.e. the answer to the question How many facilities can I
inpect in a month?. In this experiment we are assuming that the
maximum capacity is *25* but we are testing also for a list of length
*5*, and *10* (see =top_n= Above).

The execution of the experiments could take a long time, so, it is a
good practice to  /validate/ the configuration file, /before/ running
the model. You don't want to wait for hours (or days) and then
discover that there was something wrong

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_test.yaml validate
#+END_SRC

#+RESULTS:
:RESULTS:
Using the config file /triage/experiment_config/inspections_test.yaml
The output (matrices and models) of this experiment will be stored in triage/output
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Validating experiment's configuration
Experiment validation ran to completion with no errors

----TIME SPLIT SUMMARY----

Number of time splits: 2
Split index 0:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2015-12-01 00:00:00 (11 total)
            Testing as_of_time range: 2016-01-01 00:00:00 to 2016-01-01 00:00:00 (1 total)


Split index 1:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2016-12-01 00:00:00 (23 total)
            Testing as_of_time range: 2017-01-01 00:00:00 to 2017-01-01 00:00:00 (1 total)


For more detailed information on your time splits, inspect the experiment `split_definitions` Property

           
           The experiment configuration doesn't contain any obvious errors.
           Any error that occurs from now on, possibly will be related to hit the maximum 
           number of columns allowed or collision in
           the column names, both due to PostgreSQL limitations.
    
The experiment looks in good shape. May the force be with you
:END:

You can execute the experiment as

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_test.yaml run
#+END_SRC

#+RESULTS:
:RESULTS:
Using the config file /triage/experiment_config/inspections_test.yaml
The output (matrices and models) of this experiment will be stored in triage/output
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Executing experiment
Done
Experiment completed in 0:01:41.427599 seconds
:END:

This will print a lot of output, and if everything is correct it will create 4 matrices (2 for 
training, 2 for testing) in =triage/matrices=, every matrix will be
represented by two files, one with the metadata  of the matrix (a
=yaml= file) and the actual matrix (the =csv= file). 

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results raw drawer
ls /triage/output/matrices | awk -F . '{print $NF}' | sort | uniq -c
#+END_SRC

#+RESULTS:
:RESULTS:
      4 csv
      4 yaml
:End:

=Triage= also will store 12 trained models in =triage/trained_models=:

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results raw drawer
ls /triage/output/trained_models | wc -l
#+END_SRC

#+RESULTS:
:RESULTS:
12
:END:

And it will populate the =results= schema in the database, as
commented above, we will get =6= /model groups/:

#+BEGIN_SRC sql
select model_group_id, model_type, model_parameters from results.model_groups;
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                           | model_parameters                           |
|--------------+-------------------------------------+-------------------------------------------|
|            1 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": 1}         |
|            2 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": "sqrt"}    |
|            3 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": null}      |
|            4 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": 1}      |
|            5 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": "sqrt"} |
|            6 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": null}   |
:END:

And =12= /Models/:


#+BEGIN_SRC sql 
select
model_group_id, model_id, train_end_time
from results.models
order by model_group_id, train_end_time asc
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_id | train_end_time        |
|--------------+---------+---------------------|
|            1 |       1 | 2016-01-01 00:00:00 |
|            1 |       7 | 2017-01-01 00:00:00 |
|            2 |       2 | 2016-01-01 00:00:00 |
|            2 |       8 | 2017-01-01 00:00:00 |
|            3 |       3 | 2016-01-01 00:00:00 |
|            3 |       9 | 2017-01-01 00:00:00 |
|            4 |       4 | 2016-01-01 00:00:00 |
|            4 |      10 | 2017-01-01 00:00:00 |
|            5 |       5 | 2016-01-01 00:00:00 |
|            5 |      11 | 2017-01-01 00:00:00 |
|            6 |       6 | 2016-01-01 00:00:00 |
|            6 |      12 | 2017-01-01 00:00:00 |
:END:

From that last query, you should note that the order in which =triage= train
the models is by block (=train_end_time=) from oldest to recent, and
from =model_group=, also in ascending order. It will not go to the
next block, until all the /models groups/ were trained.

You can check with which matrix the models where trained

#+BEGIN_SRC sql
select
model_id, model_group_id, train_end_time, 
model_hash,train_matrix_uuid
from results.models
order by model_group_id, train_end_time asc
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | model_group_id | train_end_time        | model_hash                        | train_matrix_uuid                  |
|---------+--------------+---------------------+----------------------------------+----------------------------------|
|       1 |            1 | 2016-01-01 00:00:00 | 5062bb64aecdbf0f6875de579c4b4845 | c834bd5ba5b9c3ebdf8d6e9ac37abee9 |
|       7 |            1 | 2017-01-01 00:00:00 | 6bb1c66e613fc9f19f3992ec36d743ab | 53ccca25d2096ad453831883e1e50e1d |
|       2 |            2 | 2016-01-01 00:00:00 | fd96142f002bdbdfe518dff477048bb9 | c834bd5ba5b9c3ebdf8d6e9ac37abee9 |
|       8 |            2 | 2017-01-01 00:00:00 | 4473c1076e1479bb1aec875913b354c7 | 53ccca25d2096ad453831883e1e50e1d |
|       3 |            3 | 2016-01-01 00:00:00 | d140cd9a9de944ab8587efbba8692c99 | c834bd5ba5b9c3ebdf8d6e9ac37abee9 |
|       9 |            3 | 2017-01-01 00:00:00 | 3f33a1dd1d1047fd4cc7a28695a83514 | 53ccca25d2096ad453831883e1e50e1d |
|       4 |            4 | 2016-01-01 00:00:00 | a1b6ea17f74ea1877212ea740d1d46d7 | c834bd5ba5b9c3ebdf8d6e9ac37abee9 |
|      10 |            4 | 2017-01-01 00:00:00 | bbea50a714622b612e6da12722c34ec3 | 53ccca25d2096ad453831883e1e50e1d |
|       5 |            5 | 2016-01-01 00:00:00 | a85d9be461e9c41d21aee29cbcf421f3 | c834bd5ba5b9c3ebdf8d6e9ac37abee9 |
|      11 |            5 | 2017-01-01 00:00:00 | 000a39b2678469280132e5b7b791ad42 | 53ccca25d2096ad453831883e1e50e1d |
|       6 |            6 | 2016-01-01 00:00:00 | 48204ce78ec79c479090c332fab73e26 | c834bd5ba5b9c3ebdf8d6e9ac37abee9 |
|      12 |            6 | 2017-01-01 00:00:00 | 8cc1037025ef7c97295381434b24977e | 53ccca25d2096ad453831883e1e50e1d |
:END:

As expected, we have two models per model group. Each model was trained
with the matrix indicated in the column =train_matrix_uuid=. This =uuid=
also is the file name of the stored matrix. The model itself was
stored under the file named with the =model_hash=.

For example, the model =7= was stored as
=/triage/trained_models/= src_sql[:results output]{select train_matrix_uuid from
results.models where model_id = 7}
using the standard serialization of sklearn models. This model was
trained with the matrix src_sql[:results output]{select train_matrix_uuid from
results.models where model_id = 7} stored in the directory
=/triage/Matrices=.



The model =7= used the following hyperparameters:

#+BEGIN_SRC sql
select 
model_parameters 
from results.models 
where model_id = 7
#+END_SRC

#+RESULTS:
:RESULTS:
| model_parameters                   |
|-----------------------------------|
| {"max_depth": 1, "max_features": 1} |
:END:


We can visualize the model 

#+BEGIN_SRC ipython

#+END_SRC

#+RESULTS:
:RESULTS:

[[file:ipython-inline-images/ob-ipython-4e78b5edecb9955351d00224b4ab56d5.png]]
<IPython.core.display.Image object>
:END:


The same model =7= is part of the model group src_sql{select model_group_id
from results.models where model_id = 7}. That model group

#+BEGIN_SRC sql
select 
model_group_id, model_type, model_config 
from 
results.model_groups 
where model_group_id = 1
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                           | model_config                                             |
|--------------+-------------------------------------+---------------------------------------------------------|
|            1 | sklearn.tree.DecisionTreeClassifier | {"experiment_type": "test", "label_definition": "failed"} |
:END:

The features used by that model are:

#+BEGIN_SRC sql
select 
unnest(feature_list) as features 
from 
results.model_groups 
where model_group_id = 1
#+END_SRC

#+RESULTS:
:RESULTS:
| features                                       |
|------------------------------------------------|
| inspections_entity_id_3month_type_canvass_sum        |
| inspections_entity_id_3month_type_complaint_sum      |
| inspections_entity_id_3month_type_consultation_sum   |
| inspections_entity_id_3month_type_food poisoning_sum |
| inspections_entity_id_3month_type_license_sum        |
| inspections_entity_id_3month_type__NULL_sum          |
| inspections_entity_id_3month_type_tag removal_sum    |
| inspections_entity_id_3month_type_task force_sum     |
| inspections_zip_code_3month_type_canvass_sum         |
| inspections_zip_code_3month_type_complaint_sum       |
| inspections_zip_code_3month_type_consultation_sum    |
| inspections_zip_code_3month_type_food poisoning_sum  |
| inspections_zip_code_3month_type_license_sum         |
| inspections_zip_code_3month_type__NULL_sum           |
| inspections_zip_code_3month_type_tag removal_sum     |
| inspections_zip_code_3month_type_task force_sum      |
:END:

Finally, the performance of the model =7=  are:

#+BEGIN_SRC sql
select
model_id,
metric || parameter as metric,
value,
num_labeled_examples, 
num_labeled_above_threshold,
num_positive_labels
from results.evaluations where model_id = 7
order by num_labeled_above_threshold asc,
metric || parameter
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | metric            |                 value | num_labeled_examples | num_labeled_above_threshold | num_positive_labels |
|---------+-------------------+-----------------------+--------------------+--------------------------+-------------------|
|       7 | precision@5_abs    |                   0.0 |               1173 |                        0 |               269 |
|       7 | recall@5_abs       |                   0.0 |               1173 |                        0 |               269 |
|       7 | precision@10_abs   |                   1.0 |               1173 |                        1 |               269 |
|       7 | recall@10_abs      | 0.0037174721189591076 |               1173 |                        1 |               269 |
|       7 | precision@25_abs   |                   0.6 |               1173 |                        5 |               269 |
|       7 | recall@25_abs      |  0.011152416356877323 |               1173 |                        5 |               269 |
|       7 | precision@5.0_pct  |   0.29333333333333333 |               1173 |                       75 |               269 |
|       7 | recall@5.0_pct     |   0.08178438661710037 |               1173 |                       75 |               269 |
|       7 | precision@10.0_pct |   0.25190839694656486 |               1173 |                      131 |               269 |
|       7 | recall@10.0_pct    |   0.12267657992565056 |               1173 |                      131 |               269 |
:END:

The columns  =num_labeled_examples, num_labeled_above_threshold,
num_positive_labels= represents the number of selected entities in the
prediction date which are labeled (there are =1173= entities in the
test matrix with a label (=1= or =0=)), the
number of entities with a positive label above the threshold
(e.g. there are =1= entity with a positive label =1= in the first 10
entities ordered by score) and the number of entities with positive labels among all the
labeled entities (=269= of =1173=) respectively. We can translate this
to english: in our case /label/ mean that between the /as of
date/ (=2017-01-01=) and one month later (until =2017-02-01=) there
were =1173= facilities *inspected* and =269= *failed* the inspection.

We could check that the numbers make sense, the number of /active
facilities/ at =2017-01-01= (the prediction date) is

#+BEGIN_SRC sql
select count(*)
from inspections.active_facilities
where '2017-01-01'::date <@ daterange(start_time, end_time)
#+END_SRC

#+RESULTS:
:RESULTS:
| count |
|-------|
| 19396 |
:END:

And this number matches with the predictions made by the model =7=, as expected.

#+BEGIN_SRC sql
select count(*) from results.predictions where model_id = 7 
#+END_SRC

#+RESULTS:
:RESULTS:
| count |
|-------|
| 19396 |
:END:

The number of /labels/ (=num_labeled_examples= = =1173=) is different,
 because only =1173= facilities were inspected in that time span. so,
 many of the facilities weren't inspected, then their label is =NULL=.


#+BEGIN_SRC sql
select count(distinct entity_id)
from inspections.failed
where outcome_date <@ '[2017-01-01, 2017-02-01)'::daterange
#+END_SRC

#+RESULTS:
:RESULTS:
| count |
|-------|
|  1316 |
:END:

Still far from the =1173=. Do you remember the /states/ table? Using
it to filter we got the correct number:

#+BEGIN_SRC sql
select outcome,count(distinct entity_id)
from inspections.failed
inner join (
      select entity_id
      from inspections.active_facilities
      where '2017-01-01'::date <@ daterange(start_time, end_time)
) as t
using (entity_id)
where outcome_date <@ '[2017-01-01, 2017-02-01)'::daterange
group by rollup(outcome)
#+END_SRC

#+RESULTS:
:RESULTS:
| outcome | count |
|---------+-------|
| f       |  1085 |
| t       |   269 |
| [NULL]  |  1173 |
:END:

Let's assume that this is our best model, Which is the list of 25 facilities to inspect?

#+BEGIN_SRC sql
select *
from results.predictions
where model_id = 7 and rank_abs is not null
order by score desc
limit 25
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | entity_id | as_of_date | score | label_value | rank_abs | rank_pct | matrix_uuid | test_label_timespan |
|---------+----------+----------+-------+------------+---------+---------+------------+-------------------|
:END:

** Defining a baseline

As a second step, lets do a new experiment that defines our
/baseline/. In order to achive this, we will use a similar experiment
config file with the following changes:

#+BEGIN_EXAMPLE yaml
model_comment: 'inspections_baseline'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'baseline'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'

grid_config:
    'sklearn.dummy.DummyClassifier':
        strategy: [prior,uniform, most_frequent]

model_group_keys:
    - 'label_definition'
    - 'experiment_type'
#+END_EXAMPLE

The complete file is in [[./triage/experiment_config/inspections_baseline.yaml][triage/experiment_config/inspections_baseline.yaml]]

If we execute this experiment, we will get 3 more model groups (one
for each strategy), and the corresponding 6 new models (2 per each
model group).

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_baseline.yaml validate
#+END_SRC

#+RESULTS:
:RESULTS:
Using the config file /triage/experiment_config/inspections_baseline.yaml
The output (matrices and models) of this experiment will be stored in triage/output
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Validating experiment's configuration
Experiment validation ran to completion with no errors

----TIME SPLIT SUMMARY----

Number of time splits: 2
Split index 0:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2015-12-01 00:00:00 (11 total)
            Testing as_of_time range: 2016-01-01 00:00:00 to 2016-01-01 00:00:00 (1 total)


Split index 1:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2016-12-01 00:00:00 (23 total)
            Testing as_of_time range: 2017-01-01 00:00:00 to 2017-01-01 00:00:00 (1 total)


For more detailed information on your time splits, inspect the experiment `split_definitions` property
           
           The experiment configuration doesn't contain any obvious errors.
           Any error that occurs from now on, possibly will be related to hit the maximum 
           number of columns allowed or collision in
           the column names, both due to PostgreSQL limitations.
    
The experiment looks in good shape. May the force be with you
:END:

You can execute the experiment as

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_baseline.yaml run
#+END_SRC

#+RESULTS:
:RESULTS:
Using the config file /triage/experiment_config/inspections_baseline.yaml
The output (matrices and models) of this experiment will be stored in triage/output
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Executing experiment
Done
Experiment completed in 0:00:28.682451 seconds
:End:

#+BEGIN_SRC sql
select model_group_id, model_type, model_parameters from results.model_groups;
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                           | model_parameters                           |
|--------------+-------------------------------------+-------------------------------------------|
|            1 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": 1}         |
|            2 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": "sqrt"}    |
|            3 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": null}      |
|            4 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": 1}      |
|            5 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": "sqrt"} |
|            6 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": null}   |
|            7 | sklearn.dummy.DummyClassifier       | {"strategy": "prior"}                     |
|            8 | sklearn.dummy.DummyClassifier       | {"strategy": "uniform"}                   |
|            9 | sklearn.dummy.DummyClassifier       | {"strategy": "most_frequent"}              |
:END:

#+BEGIN_SRC sql

with baseline as (
select model_id, model_group_id
from results.models
where model_type ~ 'DummyClassifier'
)

select 
model_group_id, model_id, metric || parameter as metric, value
from results.evaluations
inner join baseline using(model_id)
where
metric || parameter = 'precision@25_abs'
order by metric || parameter, model_id
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_id | metric          | value |
|--------------+---------+-----------------+-------|
|            7 |      13 | precision@25_abs |   0.0 |
|            8 |      14 | precision@25_abs |   0.0 |
|            9 |      15 | precision@25_abs |   0.0 |
|            7 |      16 | precision@25_abs |   0.6 |
|            8 |      17 | precision@25_abs |   0.6 |
|            9 |      18 | precision@25_abs |   0.6 |
:END:


** A neat trick

Add small, medium, full grid (Rayid magic loop example)


** A more advanced experiment

Ok, let's add a more complete experiment.


#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_label_failed_01.yaml
config_version: 'v3'

model_comment: 'inspections'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'exploratory'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'
#+END_SRC

#+BEGIN_SRC  yaml :tangle ./triage/experiment_config/inspections_label_failed_01.yaml
temporal_config:
    feature_start_time: '2012-02-01'
    feature_end_time: '2017-02-01'
    label_start_time: '2012-02-01'
    label_end_time: '2017-02-01'

    model_update_frequency: '1y'
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '1month'  
    test_label_timespans: ['1y'] #
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'
#+END_SRC

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_label_failed_01.yaml
events_table: inspections.failed

feature_aggregations:
    -
        prefix: 'inspections'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'type'
                choice_query: 'select distinct type from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'

    -
        prefix: 'risks'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'risk'
                choice_query: 'select distinct risk from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'


    -
        prefix: 'results'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'result'
                choice_query: 'select distinct result from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'



state_config:
    table_name: 'inspections.active_facilities'
    state_filters:
       - 'active'
#+END_SRC

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/inspections_label_failed_01.yaml
grid_config:
    'sklearn.ensemble.RandomForestClassifier':
        max_features: ['sqrt']
        criterion: ['gini']
        n_estimators: [1000]
        min_samples_leaf: [1]
        min_samples_split: [50]
        class_weight: ['balanced']

feature_group_definition:
   prefix: ['inspections', 'results', 'risks']

feature_group_strategies: ['all', 'leave-one-in', 'leave-one-out']

model_group_keys:
    - 'label_definition'
    - 'experiment_type'
    - 'org'
    - 'team'

scoring:
    sort_seed: 1234
    metric_groups:
        -
            metrics: ['precision@', 'recall@']
            thresholds:
                percentiles: [1.0, 2.0, 5.0, 10.0, 25.0, 50.0, 75.0, 95.0, 100.0]
                top_n: [5, 10, 25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000]

#+END_SRC

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_label_failed_01.yaml validate
#+END_SRC

#+RESULTS:
:RESULTS:
Using the config file /triage/experiment_config/inspections_label_failed_01.yaml
The output (matrices and models) of this experiment will be stored in triage/output
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Validating experiment's configuration
Experiment validation ran to completion with no errors

----TIME SPLIT SUMMARY----

Number of time splits: 3
Split index 0:
            Training as_of_time_range: 2012-02-01 00:00:00 to 2013-01-01 00:00:00 (12 total)
            Testing as_of_time range: 2014-01-01 00:00:00 to 2014-01-01 00:00:00 (1 total)


Split index 1:
            Training as_of_time_range: 2012-02-01 00:00:00 to 2014-01-01 00:00:00 (24 total)
            Testing as_of_time range: 2015-01-01 00:00:00 to 2015-01-01 00:00:00 (1 total)


Split index 2:
            Training as_of_time_range: 2012-02-01 00:00:00 to 2015-01-01 00:00:00 (36 total)
            Testing as_of_time range: 2016-01-01 00:00:00 to 2016-01-01 00:00:00 (1 total)


For more detailed information on your time splits, inspect the experiment `split_definitions` property

           The experiment configuration doesn't contain any obvious errors.
           Any error that occurs possibly is related to number of columns or collision in
           the column names, both due to PostgreSQL limitations.
    
The experiment looks in good shape. May the force be with you
:END:

You can execute the experiment as

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_label_failed_01.yaml run
#+END_SRC

** How can I pick the best one?


We are working in ...

But meanwhile, you can try the following

* What's next?

Do you want to know why the [[file:eis.org][Early Intervention System]] is different
from the inspections problem? If so, continue reading.


* Notes
[2018-01-01 Mon 00:50]


 /What are you inspecting?/ (people, places, other)
 /How far do you want to predict?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /How often do you want to update the list?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /What do you want to optimize for?/ (e.g. efficiency, long term
 compliance, novelty)


Inspection the join starts from outcomes (outcome centric) (if you
haven't been inspected, we can not said anything about you)
