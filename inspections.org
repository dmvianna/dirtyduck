#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Center of Data Science for Public Policy
#+EMAIL: adolfo@uchicago.edu
#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:shell     :results drawer
#+PROPERTY: header-args:ipython   :session :exports both :results raw drawer
#+PROPERTY: header-args:python   :session :exports both :results raw drawer

* Problem description

 /What are you inspecting?/ (people, places, other)
 /How far do you want to predict?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /How often do you want to update the list?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /What do you want to optimize for?/ (e.g. efficiency, long term
 compliance, novelty)


#+begin_quote
Will my restaurant be inspected in the
/next X period of time?/
#+end_quote

Where $X$ could be 1 month, 1 week, 1 year,
etc.

  Knowing the answer to this question, allows you (as the restaurant's
  owner) to be prepared and take the pertinent actions.


* Creating the labels

 #+begin_src sql
 select * from semantic.events limit 1
 #+end_src

 #+RESULTS:
 :RESULTS:
 | inspection | type    | license_num | facility_type | zip_code | city    |       date | risk   | result | violations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
 |------------+---------+------------+--------------+---------+---------+------------+--------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 |     100209 | canvass |    1226806 | liquor       |   60622 | chicago | 2010-01-20 | medium | fail   | [{"code" : "13", "severity" : "critical", "description" : "NO EVIDENCE OF RODENT OR INSECT INFESTATION, NO BIRDS, TURTLES OR OTHER ANIMALS", "comment" : "All necessary control measures shall be used to effectively minimize or eliminate the presence of rodents, roaches, and other vermin/insect infestations"}, {"code" : "32", "severity" : "minor", "description" : "FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED", "comment" : "All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair"}, {"code" : "33", "severity" : "minor", "description" : "FOOD AND NON-FOOD CONTACT EQUIPMENT UTENSILS CLEAN, FREE OF ABRASIVE DETERGENTS", "comment" : "All food and non-food contact surfaces of equipment and all food storage utensils shall be thoroughly cleaned and sanitized daily"}, {"code" : "34", "severity" : "minor", "description" : "FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED", "comment" : "The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair"}, {"code" : "38", "severity" : "minor", "description" : "VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED", "comment" : "Ventilation: All plumbing fixtures, such as toilets, sinks, washbasins, etc"}, {"code" : "41", "severity" : "minor", "description" : "PREMISES MAINTAINED FREE OF LITTER, UNNECESSARY ARTICLES, CLEANING  EQUIPMENT PROPERLY STORED", "comment" : "All parts of the food establishment and all parts of the property used in connection with the operation of the establishment shall be kept neat and clean and should not produce any offensive odors"}, {"code" : "42", "severity" : "minor", "description" : "APPROPRIATE METHOD OF HANDLING OF FOOD (ICE) HAIR RESTRAINTS AND CLEAN APPAREL WORN", "comment" : "All employees shall be required to use effective hair restraints to confine hair"}] |
 :END:



** Which facilities are likely to fail an inspection?

Facilities who failed an inspection (i.e. =result= = ='fail'=)

** Which facilities are likely  to fail an inspection with a major  violation?

Critical violations are coded between =1-14=, serious violations between
=15-29=, everything above =30= is assumed to be a minor violation.

Facilities who failed an inspection (i.e. =result= = ='fail'=) and the
=severity in ('critical', 'serious')=


#+begin_src sql

select inspection, result, array_agg(obj ->>'severity'),
(result = 'fail') as failed,
(result = 'fail' and
('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
) as failed_major_violation
from
(select inspection, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events limit 20)
as t1
group by inspection, result

#+end_src

#+RESULTS:
:RESULTS:
| inspection | result             | array_agg                                                                             | failed | failed_major_violation |
|------------+--------------------+--------------------------------------------------------------------------------------+--------+----------------------|
|     285190 | pass w/ conditions | {minor,critical}                                                                     | f      | f                    |
|     285193 | fail               | {critical,critical,serious,critical,minor,minor,minor,critical,minor,minor,critical} | t      | t                    |
|     285191 | pass               | {minor,minor,minor,minor}                                                            | f      | f                    |
|     285192 | pass               | {NULL}                                                                               | f      | f                    |
|     285196 | fail               | {serious,critical}                                                                   | t      | t                    |
:END:


Let's use the previous query to generate our labels in the inspections schema

#+begin_src sql
create schema if not exists inspections
#+end_src

#+RESULTS:


#+begin_src sql

drop table if exists inspections.events;

create table inspections.events as (
with inspection_labels as (
select inspection, result,
   (result = 'fail') as failed,
   (result = 'fail' and
       ('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
   ) as failed_major_violation
from
   (select inspection, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events)
as t1
group by inspection, result
)


select e.inspection, e.type, e.license_num, e.facility_type, e.zip_code, e.city, e.date, e.risk, e.result, failed, failed_major_violation
from
semantic.events as e
join
inspection_labels as l
on e.inspection = l.inspection
)
#+end_src

#+RESULTS:

* ▶ TODO Temporal crossvalidation

Avoid leakage ...

Enter =timechop=

Timechop requires the following parameters:

- =feature_start_time= - data aggregated into features begins at this point
# earliest date included in features
- =feature_end_time= - data aggregated into features is from before this
  point
# latest date included in features
- =label_start_time= - data aggregated into labels begins at this point
# earliest event date included in any label (event date >= label_start_time)
- =label_end_time= - data aggregated is from before this point
# event date < label_end_time to be included in any label
- =model_update_frequency= - amount of time between train/test splits
# how frequently to retrain models (days, months, years)
- =training_as_of_date_frequencies= - how much time between rows for a
  single entity in a training matrix
# list - time between rows for same entity in train matrix
- =max_training_histories= - the maximum amount of history for each
  entity to train on (early matrices may contain less than this time
  if it goes past label/feature start times)
# max length of time for labels included in a train matrix - default = max (label_start_time to now)
- =training_label_timespans= - how much time is covered by training
  labels (e.g., outcomes in the next 1 year? 3 days? 2 months?)
  (training prediction span)
# time period across which outcomes are determined in train matrices
- =test_as_of_date_frequencies= - how much time between rows for a
  single entity in a test matrix
# time between rows for same entity in test matrix  - inspections -  planning/scheduling frequency, eis = reviewing frequency (default = 1week)
- =test_durations= - how far into the future should a model be used to
  make predictions (in the typical case of wanting a single prediction
  set immediately after model training, this should be set to 0 days)
(test span)
# length of time included in a test matrix (default = training_prediction_span) inspections = how far out are you scheduling for? eis = model_update_frequency
- =test_label_timespans= - how much time is covered by test predictions
  (e.g., outcomes in the next 1 year? 3 days? 2 months?)
(test prediction span)
# time period across which outcomes are labeled in test matrices (default for eis = training_prediction_span, inspections = test_data_span)

In the particular case of *inspections*,

- =test_as_of_date_frequencies= is planning/scheduling frequency
- =test_durations= is how far out are you scheduling for?
- =test_label_timespan= is equal to =test_durations=

/What are you inspecting?/ (people, places, other)
/How far do you want to predict?/ (e.g. 1 mo, 6mo, 12 mo, etc)
/How often do you want to update the list?/ (e.g. 1 mo, 6mo, 12 mo, etc)




#+BEGIN_EXAMPLE
    model_update_frequency='1year',
    training_label_timespans='3month',
    training_as_of_date_frequencies='3month',
    max_training_histories='2year',

    test_durations='3month',
    test_label_timespans='3month',
    test_as_of_date_frequencies='1month'
#+END_EXAMPLE


#+BEGIN_SRC python
import utils

utils.show_timechop(utils.chopper)
#+END_SRC

#+RESULTS:
: None


With that configuration our time splits looks like:

[[file:timechop.png][Timechop blocks for inspection]]



* ▶ TODO Feature engineering

/We will show how to create features, we will use the same subset (one
facility) and only one variable, and some spatial and temporal
dimensions.

For this, we will create a new =schema=

#+BEGIN_SRC sql
create schema if not exists triage
#+END_SRC

#+RESULTS:

We need a table that represents if the facility is "active" in the
moment of the inspection (we don't want to predict a facility that is
not active). We don't want to complicate the calculation here, so, we
will assume that all the facilities are active in *every inspection*


#+BEGIN_SRC sql
drop table if exists triage.all_facilities;

create table triage.all_facilities as (
    select license_num, date
    from (select distinct license_num from semantic.events) a
    cross join (select distinct date as date from semantic.events) b
-- select
-- distinct license_num,
-- 'active'::text as state,
-- min(date) as start,
-- max(date) as end
-- from semantic.events
-- group by license_num
) ;


create index on triage.all_facilities(license_num, date);
#+END_SRC

#+RESULTS:


#+BEGIN_SRC sql
select * from triage.all_facilities limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num |       date |
|------------+------------|
|    2506828 | 2016-11-10 |
|    2506828 | 2015-05-05 |
|    2506828 | 2013-05-06 |
|    2506828 | 2015-12-24 |
|    2506828 | 2017-05-26 |
:END:

#+BEGIN_SRC sql
select count(*) from triage.all_facilities
#+END_SRC

#+RESULTS:
:RESULTS:
|    count |
|----------|
| 57683187 |
:END:


Almost all the components of =triage= works with =SQL= tables stored  in
=PostgreSQL= (this is very important to remember), so, let's create our
test table with the =license_num= =1974745=:

#+BEGIN_SRC  sql
drop table if exists triage.test;

create table triage.test as (
select
license_num,  -- entity
type as inspection_type, risk, -- variables
date, zip_code, -- spatio temporal dimensions
failed, failed_major_violation -- labels
from inspections.events
where license_num = 1974745
)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC sql
select * from triage.test order by date desc  limit 5
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num | inspection_type | risk |       date | zip_code | failed | failed_major_violation |
|------------+----------------+------+------------+---------+--------+----------------------|
|    1974745 | canvass        | high | 2016-10-17 |   60612 | f      | f                    |
|    1974745 | canvass        | high | 2015-10-20 |   60612 | f      | f                    |
|    1974745 | complaint      | high | 2015-01-12 |   60612 | f      | f                    |
|    1974745 | canvass        | high | 2014-10-08 |   60612 | f      | f                    |
|    1974745 | canvass        | high | 2014-10-06 |   60612 | t      | t                    |
:END:


=Collate= is the python library that we will use (and =triage= also) for
controlling the generation of features (including the imputation rules
for each feature generated). =Collate= helps the modeler to
create features based on /spatio-temporal aggregations/ (which is what
we need in our modeling strategy based on *events*)

Let's start with an example of an aggregate feature is the number of failed inspections.

#+BEGIN_SRC sql
select license_num,
--count(*) filter (where  inspection_type = 'complaint') as complaints
sum((inspection_type = 'complaint')::int) as complaints
from triage.test
group by license_num
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num | complaints |
|------------+------------|
|    1974745 |          3 |
:END:

This =SQL= query is composed by three parts:
- The filter (=inspection_type = 'complaint'=)
- The aggregation function (=sum()=)
- The name of the resulting transformation (=complaints=)

In collate, this aggregated column would be defined as:

#+BEGIN_SRC python :session
from  triage.component.collate import Aggregate, SpacetimeAggregation

complaints = Aggregate({"complaints": "(inspection_type = 'complaint')::int"}, # column name and filter
                      "sum", # aggregation function
                      {'coltype':'aggregate', 'all': {'type': 'mean'}} # imputation rules
)

#+END_SRC

#+RESULTS:
:RESULTS:
:END:

The aggregation function (in the example =sum=) is confusing, but =sum=
means "count" or "how many" and =avg= means proportion.


Note also that we specify the imputation strategy for how to handle
the null values in the resulting fields, in this example we use the
=mean= value.

We are not still ready for use this aggregated variable as a feature,
we need to take in account the spatio and temporal context.


Let's add first the spatial context:

#+BEGIN_SRC sql
select license_num,
--count(*) filter (where  inspection_type = 'complaint') as complaints
sum((inspection_type = 'complaint')::int) as complaints
from triage.test
group by license_num
#+END_SRC


#+BEGIN_SRC python
import sqlalchemy

db_url = f"postgresql://food_user:some_password@0.0.0.0:5434/food"

engine = sqlalchemy.create_engine(db_url, client_encoding='utf8')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


Aggregations in collate easily aggregate this single feature across
different spatiotemporal groups

#+BEGIN_SRC python
st = SpacetimeAggregation([complaints],
                          from_obj='triage.test', # FROM
                          groups=['license_num','zip_code'],  # GROUP BY
                          intervals={"license_num": ["1 year"], "zip_code": ["1 year", "2months"]},
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='license_num', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='test'
)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


The =SpacetimeAggregation= object is in charge of create the
agregations, another way of see it, is that it encapsulates the FROM section of the
query (=from_obj=) as well as the
GROUP BY columns (=groups=).

In the example above it will create features based on individual
restaurants (using =license_num=) but also /contextual/ features related
to information about the zip code (=zip_code=) in which the facility is
operating.

The state table (=state_table=) specified here should contain the
comprehensive set of facilities and dates for which output should be
generated for them, regardless if they exist in the =from_obj=.

The attribute =intervals= specifies the date range partitioning for the
feature: it will create the aggregation over the past =1 year= for the
grouping given by the =license_num= nad for the =zip_code=, and
additionally  will give an extra grouping statistic of two months for
the =zip_code=.


Before execute the queries, you could actually look them using the following

#+BEGIN_SRC python
for sql_list in st.get_selects().values():
    for sql in sql_list:
        print(str(sql))
#+END_SRC


#+BEGIN_SRC python
st.execute(engine.connect()) # with a SQLAlchemy engine object
#+END_SRC


#+RESULTS:
:RESULTS:
:END:

This will create 3 tables (One for the =license_num=, one for =zip_code=
and one for the combination: =license_num + zip_code=) and one extra
table for the imputated values.

The names of the generated tables are constructed as follows:

#+BEGIN_EXAMPLE
schema.prefix_{group, aggregation}
#+END_EXAMPLE


The =triage.test_zip_code= table
have two feature columns for every zip code in our table =triage.test=,
looking at the total and average number of complaints in that
=zip_code= over the year prior and 2 months prior to the date in the =date= column.

#+BEGIN_SRC sql
select * from triage.test_zip_code order by date desc limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | test_zip_code_1 year_complaints_sum | test_zip_code_2months_complaints_sum |
|---------+------------+--------------------------------+---------------------------------|
|   60612 | 2016-10-17 |                              0 | [NULL]                          |
|   60612 | 2015-10-20 |                              1 | [NULL]                          |
|   60612 | 2015-01-12 |                              0 | [NULL]                          |
|   60612 | 2014-10-08 |                              0 | 0                               |
|   60612 | 2014-10-06 |                              0 | 0                               |
:END:

The table =triage.test_license_num= contains two feature columns for each
license that describe the total number of complaints
the past one year.


#+BEGIN_SRC sql
select * from triage.test_license_num order by date desc limit 5;
#+END_SRC



#+RESULTS:
:RESULTS:
| license_num |       date | test_license_num_1 year_complaints_sum |
|------------+------------+-----------------------------------|
|    1974745 | 2016-10-17 |                                 0 |
|    1974745 | 2015-10-20 |                                 1 |
|    1974745 | 2015-01-12 |                                 0 |
|    1974745 | 2014-10-08 |                                 0 |
|    1974745 | 2014-10-06 |                                 0 |
:END:


The =triage.test_aggregation= table joins these results together to make
it easier to look at both zip_code and facility-level effects
for any given facility.

#+BEGIN_SRC sql
select * from triage.test_aggregation order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | license_num | test_license_num_1 year_complaints_sum | test_zip_code_1 year_complaints_sum | test_zip_code_2months_complaints_sum |
|---------+------------+------------+-----------------------------------+--------------------------------+---------------------------------|
|   60612 | 2014-10-06 |    1974745 |                                 0 |                              0 | 0                               |
|   60612 | 2014-10-08 |    1974745 |                                 0 |                              0 | 0                               |
|   60612 | 2015-01-12 |    1974745 |                                 0 |                              0 | [NULL]                          |
|   60612 | 2015-10-20 |    1974745 |                                 1 |                              1 | [NULL]                          |
|   60612 | 2016-10-17 |    1974745 |                                 0 |                              0 | [NULL]                          |
:END:

Finally, the =triage.test_aggregated_imputed= table fills in null values using the
imputation rules specified in the =Aggregate= constructor.

Besides the =Aggregator= objects, there is the =Categorical= object. This
name is a misnomer, in reality is a shorthand for doing several
operations at once. For example, for getting the proportion of "high",
"medium" and "low" risks flags received by a facility at the
inspection we could use

#+BEGIN_SRC python
from  triage.component.collate import Categorical

risks = Categorical("risk", # the column
                    ["high", "medium", "low"], # compare to, i.e. 'risk = high', 'risk=low', etc
                    "avg", # aggregation function
                    {'coltype':'aggregate', 'all': {'type': 'mean'}} # imputation rules
)
#+END_SRC

Now, the =SpacetimeAggregation= looks like

#+BEGIN_SRC python
st = SpacetimeAggregation([risks],
                          from_obj='triage.test', # FROM
                          groups=['license_num','zip_code'],  # GROUP BY
                          intervals={"license_num": ["1 year"], "zip_code": ["1 year", "2 year"]},
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='license_num', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='test_risks'
)

st.execute(engine.connect())
#+END_SRC

This will execute queries as the following:

#+BEGIN_EXAMPLE sql
SELECT
zip_code, '2016-10-17'::date AS date,
sum((risk = 'high')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_high_sum",
sum((risk = 'medium')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_medium_sum",
sum((risk = 'low')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_low_sum",
sum((risk = 'high')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '2 year') AS "test_risks_zip_code_2 year_risk_high_sum",
sum((risk = 'medium')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '2 year') AS "test_risks_zip_code_2 year_risk_medium_sum",
sum((risk = 'low')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '2 year') AS "test_risks_zip_code_2 year_risk_low_sum"
FROM triage.test
WHERE date < '2016-10-17'AND date >= '2016-10-17'::date - greatest(interval '1 year',interval '2 year') GROUP BY zip_code
#+END_EXAMPLE



#+BEGIN_SRC sql
select * from triage.test_risks_zip_code
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | test_risks_zip_code_1 year_risk_high_avg | test_risks_zip_code_1 year_risk_medium_avg | test_risks_zip_code_1 year_risk_low_avg | test_risks_zip_code_2 year_risk_high_avg | test_risks_zip_code_2 year_risk_medium_avg | test_risks_zip_code_2 year_risk_low_avg |
|---------+------------+-----------------------------------+-------------------------------------+----------------------------------+-----------------------------------+-------------------------------------+----------------------------------|
|   60612 | 2014-10-06 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2014-10-08 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2015-01-12 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2015-10-20 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2016-10-17 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
:END:


#+BEGIN_SRC sql
select * from triage.test_risks_license_num
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num |       date | test_risks_license_num_1 year_risk_high_avg | test_risks_license_num_1 year_risk_medium_avg | test_risks_license_num_1 year_risk_low_avg |
|------------+------------+--------------------------------------+----------------------------------------+-------------------------------------|
|    1974745 | 2014-10-06 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
|    1974745 | 2014-10-08 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
|    1974745 | 2015-01-12 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
|    1974745 | 2015-10-20 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
|    1974745 | 2016-10-17 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
:END:


* Modeling using Machine Learning

It is time of getting all the previous steps and put them
together. Don't worry, actually we are done coding. =triage= provides
you with a configuration file for specifying the experiment that we
want to run.

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
config_version: 'v3'

# EXPERIMENT METADATA
# model_comment (optional) will end up in the model_comment column of the
# models table for each model created in this experiment
model_comment: 'inspections_test'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
temporal_config:
  feature_start_time='np.min(df.date)'
  feature_end_time='np.max(df.date)'
  label_start_time='np.min(df.date)'
  label_end_time='np.max(df.date)'
  model_update_frequency='3months'
  training_label_timespans='1day'
  training_as_of_date_frequencies='1day'
  max_training_histories='1year'
  test_durations='1day'
  test_label_timespans='3month'
  test_as_of_date_frequencies='1day'
#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
events_table: inspections.events
#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
feature_aggregations:
    -
        # prefix given to the resultant tables
        prefix: 'prefix'
        # from_obj is usually a source table but can be an expression, such as
        # a join (ie 'cool_stuff join other_stuff using (stuff_id)')
        from_obj: 'cool_stuff'
        # The date column to use for specifying which records to include
        # in temporal features. It is important that the column used specifies
        # the date at which the event is known about, which may be different
        # from the date the event happened.
        knowledge_date_column: 'open_date'

        # top-level imputation rules that will apply to all aggregates functions
        # can also specify categoricals_imputation or array_categoricals_imputation
        #
        # You must specified at least one of the top-level or feature-level imputation
        # to cover ever feature being defined.
        aggregates_imputation:
            # The `all` rule will apply to all aggregation functions, unless over-
            # ridden by a more specific one
            all:
                # every imputation rule must have a `type` parameter, while some
                # (like 'constant') have other required parameters (`value` here)
                type: 'constant'
                value: 0
            # specifying `max` here will take precedence over the `all` rule for
            # aggregations using a MAX() function
            max:
                type: 'mean'

        # aggregates and categoricals define the actual features created. So
        # at least one is required
        #
        # Aggregates of numerical columns. Each quantity is a number of some
        # sort, and the list of metrics are applied to each quantity
        aggregates:
            -
                quantity: 'homeless::INT'
                # Imputation rules specified at the level of specific features
                # will take precedence over the higer-level rules specified
                # above. Note that the 'count' and 'sum' metrics will be
                # imputed differently here.
                imputation:
                    count:
                        type: 'mean'
                    sum:
                        type: 'constant'
                        value: 137
                metrics:
                    - 'count'
                    - 'sum'
            -
                # since we're specifying `aggregates_imputation` above,
                # a feature-specific imputation rule can be omitted
                quantity: 'some_flag'
                metrics:
                    - 'max'
                    - 'sum'
        # Categorical features. The column given can be of any type, but the
        # choices must comparable to that type for equality within SQL
        # The result will be one feature for each choice/metric combination
        categoricals:
            -
                column: 'color'
                # note that we haven't specified a top level `categoricals_imputation`
                # set of rules, so we have to include feature-specific imputation
                # rules for both of our categoricals here.
                imputation:
                    sum:
                        type: 'null_category'
                    max:
                        type: 'mean'
                choices:
                    - 'red'
                    - 'blue'
                    - 'green'
                metrics:
                    - 'sum'
            -
                column: 'shape'
                # as with the top-level imputation rules, `all` can be used
                # for the feature-level rules to specify the same type of
                # imputation for all aggregation functions
                imputation:
                    all:
                        type: 'zero'
                choice_query: 'select distinct shape from cool_stuff'
                metrics:
                    - 'sum'
        # The time intervals over which to aggregate features
        intervals:
            - '1 year'
            - '2 years'
            - 'all'
        # A list of different columns to separately group by
        groups:
- 'entity_id'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml

# FEATURE GROUPING
# define how to group features and generate combinations
# feature_group_definition allows you to create groups/subset of your features
# by different criteria.
# for instance,
# - 'tables' allows you to send a list of collate feature tables (collate builds these by appending 'aggregation_imputed' to the prefix)
# - 'prefix' allows you to specify a list of feature name prefixes
feature_group_definition:
   tables: ['prefix_aggregation_imputed']
#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
# STATE MANAGEMENT (optional)
# If you want to only include rows in your matrices in a specific state,
# provide:
# 1. a dense state table that defines when entities were in specific states
#   should have columns entity_id/state/start/end
# 2. a list of state filtering SQL clauses to iterate through. Assuming the
#   states are boolean columns (the experiment will convert the one you pass in
#   to this format), write a SQL expression for each state
#   configuration you want, ie '(permitted OR suspended) AND licensed'
state_config:
    table_name: 'states'
    state_filters:
        - 'state_one AND state_two'
        - '(state_one OR state_two) AND state_three'
#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
# USER METADATA
# These are arbitrary keys/values that you can have Triage apply to the
# metadata for every matrix in the experiment. Any keys you include here can
# be used in the 'model_group_keys' below. For example, if you run experiments
# that share a temporal configuration but that use different label definitions
# (say, labeling building inspections with *any* violation as positive or
# labeling only building inspections with severe health and safety violations
# as positive), you can use the user metadata keys to indicate that the matrices
# from these experiments have different labeling criteria. The matrices from the
# two experiments will have different filenames (and not be overwritten or
# inappropriately reused), and if you add the label_definition key to the model
# group keys, models made on different label definition will have different
# groups. In this way, user metadata can be used to expand Triage beyond its
# explicitly supported functionality.
user_metadata:
  label_definition: 'severe_violations'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
model_group_keys:
    - 'train_duration'
    - 'label_window'
    - 'example_frequency'
    - 'label_definition'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
grid_config:
    'sklearn.ensemble.ExtraTreesClassifier':
        n_estimators: [100,100]
        criterion: [gini, entropy]
        max_depth: [1,5,10,20,50]
        max_features: [sqrt,log2]
        min_samples_split: [2,5,10]
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
scoring:
    sort_seed: 5
    metric_groups:
        -
            metrics: [precision@, recall@]
            thresholds:
                percentiles: [5.0, 10.0]
                top_n: [5, 10]
        -
            metrics: [f1]
        -
            metrics: [fbeta@]
            parameters:
                -
                    beta: 0.75
                -
                    beta: 1.25

#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
# INDIVIDUAL IMPORTANCES
# How feature importances for individuals should be computed
# There are two variables here:
# methods: Refer to *how to compute* individual importances.
#   Each entry in this list should represent a different method.
#   Available methods are in the catwalk library's:
#   `catwalk.individual_importance.CALCULATE_STRATEGIES` list
#   Will default to 'uniform', or just the global importances.
#
# n_ranks: The number of top features per individual to compute importances for
#   Will default to 5
#
# This entire section can be left blank,
# in which case the defaults will be used.
individual_importance:
    methods: ['uniform']
    n_ranks: 5
#+END_SRC



** ▶ TODO Creating a simple experiment

Using the same subset as before, we will try one of the simplest
machine learning algorithms: a Decision Tree Classifier

We began with this data set:

Our train matrices look like:

And the test matrices:

We can check the results of the experiment here:


Now let's do a real model

** Defining a baseline
It is always a good idea define a baseline, we will use

** The grid

** How can I pick the best one?


We are working in ...

But meanwhile, you can try the following
