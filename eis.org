#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Center of Data Science for Public Policy
#+EMAIL: adolfo@uchicago.edu
#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:shell     :results drawer
#+PROPERTY: header-args:ipython   :session :exports both :results raw drawer
#+PROPERTY: header-args:python    :session food_inspections :results output Drawer
#+PROPERTY: header-args:sh  :results raw drawer

* Problem description

Our final stop is using =triage= to build a /Early Intervention (or Warning) System/. There are
 several differences between the the *EIS* and the *inspection
 prioritization* problem. The difference that has more impact is that
 the /entity/ in *EIS* is  /active/ 
 i.e. it is doing stuff, and for that stuff the the entity is doing
 some outcome will happen. In the *inspection prioritization* the
 /entity/ is acted on. This will change, from starters the way that
 the /outcome/ is build.

The question that we want to resolve is the following:

#+begin_quote
Will my restaurant be inspected in the
/next X period of time?/
#+end_quote

Where $X$ could be 1 month, 1 week, 1 year,
etc.

  Knowing the answer to this question, allows you (as the restaurant's
  owner) to be prepared and take the pertinent actions.


* The EIS problem as a multientity/multivariate time series problem




* Creating the labels

The trick to note is that every day there are two possible outcomes:
/the facility was inspected/ and /the facility wasn't inspected/. So,
our /outcomes/ table will be more larger, since we need for every date
in our dataset for every /active/ facility an /outcome/: Does the
facility Inspected?

#+BEGIN_SRC sql :tangle ./sql/create_eis_schema.sql
create schema if not exists eis;
#+END_Src

#+RESULTS:

For achiving that, we need to create a table (na√Øvely) of /number of
days/ \times /number of entities/. Lets see how many rows do we generate.

#+BEGIN_SRC sql
select min(date), max(date) from semantic.events
#+END_SRC

#+RESULTS:
:RESULTS:
|        min |        max |
|------------+------------|
| 2010-01-04 | 2018-02-13 |
:End:


#+NAME: all_days
#+BEGIN_SRC sql
select count(days) from generate_series((select min(date) from semantic.events), current_date, '1 day'::interval) days;
#+END_SRC

#+RESULTS: all_days
:RESULTS:
| count |
|-------|
|  2968 |
:END:


#+NAME: all_entities
#+BEGIN_SRC sql 
select count(*) from semantic.entities;
#+END_SRC

#+RESULTS: all_entities
:RESULTS:
| count |
|-------|
| 34812 |
:END:

#+BEGIN_SRC ipython
34812*2968
#+END_SRC

#+RESULTS:
:RESULTS:

103322016
:END:

The result is src_python[:var entities=all_entities[2] :var
days=all_days[2]]{print(int(entities[0])*int(days[0]))}
{{{results(=103322016=)}}} That's a lot of rows! 

We could improve that taking only the /active/
Entities
#+NAME: actives
#+BEGIN_SRC sql 
with dates as (
    select days::date
    from 
    generate_series(
       (select min(date) from semantic.events),
       current_date, '1 day'::interval) days
)

select count(*)
from semantic.entities
join dates d
on d.days <@ daterange(start_time, end_time)
#+END_SRC

#+RESULTS: actives
:RESULTS:
|    count |
|----------|
| 46772382 |
:END:


That's almost a reduction of src_python[:var entities=all_entities[2]
:var days=all_days[2] :var
active_entities=actives[2]]{print(100*float(active_entities[0])/(int(entities[0])*int(days[0])))}
{{{results(=45.2685534126628=)}}}%, but still a huge number of rows
given our infrastructure.

So we will cheat, and we will only consider /outcomes/ from =2015=

#+BEGIN_SRC sql 
with dates as (
    select days::date
    from 
    generate_series(
       '2015-01-01'::date,
       current_date, '1 day'::interval) days
)

select count(*)
from semantic.entities
join dates d
on d.days <@ daterange(start_time, end_time)
#+END_SRC

#+RESULTS:
:RESULTS:
|    count |
|----------|
| 21952711 |
:END:


#+BEGIN_SRC sql :tangle ./sql/create_eis_schema.sql
drop table if exists eis.inspected;

create table eis.inspected as (
 with dates as (
    select days::date as outcome_date
    from
    generate_series(
       '2015-01-01'::date,
       current_date, '1 day'::interval) days
),
active_entities as (
   select entity_id, outcome_date
   from dates d
   left  join semantic.entities
   on outcome_date <@ daterange(start_time, end_time)
   --where entity_id = 2379
)
select
a.entity_id, outcome_date,
case when
inspection is null then FALSE
else TRUE
end as outcome
from active_entities as a
left join semantic.events as e 
on a.entity_id = e.entity_id and a.outcome_date = e.date
);

create index inspected_entity_ix on eis.inspected (entity_id);
create index inspected_outcome_date_ix on eis.inspected(outcome_date desc nulls last);
create index inspected_outcome_ix on eis.inspected(outcome);

create index inspected_entity_date_ix on eis.inspected(entity_id, outcome_date);
create index inspected_date_entity_ix on eis.inspected(outcome_date, entity_id);

#+END_SRC

#+RESULTS:


The /states/ table is the same that in the inspection's case.

#+BEGIN_SRC sql :tangle ./sql/create_eis_schema.sql
drop table if exists eis.active_facilities;

create table eis.active_facilities as (
select
distinct
entity_id, 
'active'::VARCHAR  as state, 
start_time, 
coalesce(end_time, current_date) as end_time
from semantic.entities
);
#+END_SRC

#+RESULTS:


* Modeling using Machine Learning


** Creating a simple Experiment

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/eis_01.yaml
config_version: 'v3'

model_comment: 'eis'

user_metadata:
  label_definition: 'inspected'
  experiment_type: 'eis'
  purpose: 'exploring'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'
#+END_SRC

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/eis_01.yaml
events_table: eis.inspected
#+END_SRC

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/eis_01.yaml
state_config:
    table_name: 'eis.active_facilities'
    state_filters:
       - 'active'
#+END_SRC

*** Temporal configuration
#+BEGIN_SRC yaml :tangle ./triage/experiment_config/eis_01.yaml
temporal_config:
    feature_start_time: '2010-01-04'
    feature_end_time: '2017-02-13'
    label_start_time: '2015-02-01'
    label_end_time: '2017-02-13'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '1month'
    test_label_timespans: ['1month']
    test_as_of_date_frequencies: '1month'

    max_training_histories: '5y'
#+END_SRC

#+BEGIN_SRC sh 
./tutorial.sh triage --config_file eis_01.yaml show_temporal_blocks
#+END_SRC

#+RESULTS:
:RESULTS:
Using the config file /triage/experiment_config/eis_01.yaml
The output (matrices and models) of this experiment will be stored in triage/output
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Generating temporal blocks image
Image stored in:
/triage/eis.svg
:End:

[[./triage/eis.svg]]

*** Features

Regarding the features, we will use the same ones that were used in [[file:inspections.org][inspections prioritization]]

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/eis_01.yaml
feature_aggregations:
    -
        prefix: 'inspections'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'type'
                choice_query: 'select distinct type from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'

    -
        prefix: 'risks'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'risk'
                choice_query: 'select distinct risk from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'


    -
        prefix: 'results'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'result'
                choice_query: 'select distinct result from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'

#+END_SRC

As before, we declare that we want to use all possible combinations for training:

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/eis_01.yaml
feature_group_definition:
   prefix: ['inspections', 'results', 'risks']

feature_group_strategies: ['all', 'leave-one-in', 'leave-one-out']
#+END_SRC



*** Algorithm and hyperparameters

We will collapse the baseline (=DummyClassifier=)  and the exploratory configuration together:

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/eis_01.yaml
grid_config:
    'sklearn.tree.DecisionTreeClassifier':
        max_depth: [1,null]
    'sklearn.ensemble.RandomForestClassifier':
        max_features: ['sqrt']
        criterion: ['gini']
        n_estimators: [1000]
        min_samples_leaf: [1]
        min_samples_split: [50]
        class_weight: ['balanced']
    'sklearn.dummy.DummyClassifier':
        strategy: [prior,uniform, most_frequent]
#+END_SRC

The number of /model groups/ to be generated are 6 algorithms and
hyperparameters (2 =DecisionTreeClassifier=, 1
=RandomForestClassifier=, 3 =DummyClassifier=) \times 7 features groups ( 1
=all=, 3 =leave-one-in=, 3 =leave-one-out=), which give a total of
*42*.The total number of /models/ is the double of that (we have 2
time blocks) i.e. *84*.

We don't want to mix different experiments together, it is very important that the =model_group_keys= distinguish between models, so use it wisely:

#+BEGIN_SRC yaml :tangle ./triage/experiment_config/eis_01.yaml
model_group_keys:
    - 'label_definition'
    - 'experiment_type'
    - 'purpose'

scoring:
    sort_seed: 1234
    metric_groups:
        -
            metrics: ['precision@', 'recall@']
            thresholds:
                percentiles: [1.0, 2.0, 5.0, 10.0, 25.0, 50.0, 75.0, 95.0, 100.0]
                top_n: [5, 10, 25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000]
#+END_SRC

As a last step, we validate that the configuration file is correct:

#+BEGIN_SRC sh
./tutorial.sh triage --config_file eis_01.yaml validate
#+END_SRC

#+RESULTS:
:RESULTS:
Using the config file /triage/experiment_config/eis_01.yaml
The output (matrices and models) of this experiment will be stored in triage/output
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Validating experiment's configuration
Experiment validation ran to completion with no errors

----TIME SPLIT SUMMARY----

Number of time splits: 2
Split index 0:
            Training as_of_time_range: 2015-02-13 00:00:00 to 2015-11-13 00:00:00 (10 total)
            Testing as_of_time range: 2015-12-13 00:00:00 to 2015-12-13 00:00:00 (1 total)


Split index 1:
            Training as_of_time_range: 2015-02-13 00:00:00 to 2016-11-13 00:00:00 (22 total)
            Testing as_of_time range: 2016-12-13 00:00:00 to 2016-12-13 00:00:00 (1 total)


For more detailed information on your time splits, inspect the experiment `split_definitions` property

           The experiment configuration doesn't contain any obvious errors.
           Any error that occurs from now on, possibly will be related to hit the maximum 
           number of columns allowed or collision in
           the column names, both due to PostgreSQL limitations.
    
The experiment looks in good shape. May the force be with you
:END:

#+BEGIN_EXAMPLE sh
./tutorial.sh triage --config_file eis_01.yaml --no-replace --debug run
#+END_EXAMPLE

This will take a *lot* amount of time (on my computer took 3h 42m),
so, grab your coffee, chat with 
your coworkers or check your email (or all the above). Is taking that
amount of time for several reasons:

1. There are a lot of models, parameters, etc
2. We are running in serial mode (i.e. not in parallel)
3. Our database, is running in your laptop

You can solve 2 and 3. For the second point you could use the =docker=
container that has the multicore option enabled. For 3, I recommed you
to use a PostgreSQL database in the cloud, for example in *AWS* the
service is call *PostgreSQL RDS*.

After the experiment had finished we could create the following table

#+BEGIN_SRC sql
with features_groups as (
select model_group_id, split_part(unnest(feature_list), '_', 1) as feature_groups
from results.model_groups
), 

features_arrays as (
select model_group_id, array_agg(distinct feature_groups) as feature_groups
from features_groups
group by model_group_id
)

select
model_group_id,
model_type,
model_parameters,
feature_groups,
array_agg(model_id) as models
from results.models
join features_arrays using(model_group_id)
where model_comment = 'eis'
group by model_group_id, model_type, model_parameters, feature_groups order by model_group_id 
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                               | model_parameters                                                                                                                          | feature_groups               | models   |
|--------------+-----------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------|
|           10 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {inspections,results,risks} | {61,19}  |
|           11 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {inspections,results,risks} | {62,20}  |
|           12 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections,results,risks} | {63,21}  |
|           13 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {inspections,results,risks} | {64,22}  |
|           14 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {inspections,results,risks} | {65,23}  |
|           15 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {inspections,results,risks} | {66,24}  |
|           16 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {inspections}               | {67,25}  |
|           17 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {inspections}               | {68,26}  |
|           18 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections}               | {69,27}  |
|           19 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {inspections}               | {70,28}  |
|           20 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {inspections}               | {71,29}  |
|           21 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {inspections}               | {72,30}  |
|           22 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {results}                   | {73,31}  |
|           23 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {results}                   | {74,32}  |
|           24 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {results}                   | {75,33}  |
|           25 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {results}                   | {76,34}  |
|           26 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {results}                   | {77,35}  |
|           27 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {results}                   | {78,36}  |
|           28 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {risks}                     | {79,37}  |
|           29 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {risks}                     | {80,38}  |
|           30 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {risks}                     | {81,39}  |
|           31 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {risks}                     | {82,40}  |
|           32 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {risks}                     | {83,41}  |
|           33 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {risks}                     | {84,42}  |
|           34 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {results,risks}             | {85,43}  |
|           35 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {results,risks}             | {86,44}  |
|           36 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {results,risks}             | {87,45}  |
|           37 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {results,risks}             | {88,46}  |
|           38 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {results,risks}             | {89,47}  |
|           39 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {results,risks}             | {90,48}  |
|           40 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {inspections,risks}         | {91,49}  |
|           41 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {inspections,risks}         | {92,50}  |
|           42 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections,risks}         | {93,51}  |
|           43 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {inspections,risks}         | {94,52}  |
|           44 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {inspections,risks}         | {95,53}  |
|           45 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {inspections,risks}         | {96,54}  |
|           46 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {inspections,results}       | {97,55}  |
|           47 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {inspections,results}       | {98,56}  |
|           48 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections,results}       | {99,57}  |
|           49 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {inspections,results}       | {100,58} |
|           50 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {inspections,results}       | {101,59} |
|           51 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {inspections,results}       | {102,60} |
:END:



#+BEGIN_SRC sql
select
model_id, evaluation_start_time,
metric || parameter as metric,
value,
num_labeled_examples, 
num_labeled_above_threshold,
num_positive_labels
from results.evaluations 
where model_id in (21, 63)
and metric || parameter = 'precision@25_abs'
order by num_labeled_above_threshold asc,
metric || parameter
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | evaluation_start_time | metric          | value | num_labeled_examples | num_labeled_above_threshold | num_positive_labels |
|---------+---------------------+-----------------+-------+--------------------+--------------------------+-------------------|
|      21 | 2015-12-13 00:00:00 | precision@25_abs |  0.84 |              18668 |                       25 |               790 |
|      63 | 2016-12-13 00:00:00 | precision@25_abs |  0.88 |              19358 |                       25 |               958 |
:END:

#+BEGIN_SRC sql
select *
from results.predictions
where model_id = 21
order by score desc
limit 25
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | entity_id | as_of_date            |              score | label_value | rank_abs | rank_pct | matrix_uuid                       | test_label_timespan |
|---------+----------+---------------------+--------------------+------------+---------+---------+----------------------------------+-------------------|
|      21 |    25854 | 2015-12-13 00:00:00 | 0.9484136677797923 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     3019 | 2015-12-13 00:00:00 | 0.9436469079812785 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    13792 | 2015-12-13 00:00:00 | 0.9419947954600382 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    17502 | 2015-12-13 00:00:00 | 0.9312907285784298 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     2466 | 2015-12-13 00:00:00 | 0.9243102785460537 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    10382 | 2015-12-13 00:00:00 | 0.9206737935216891 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    24297 | 2015-12-13 00:00:00 | 0.9200144700893553 |          0 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |      143 | 2015-12-13 00:00:00 | 0.9199824040634403 |          0 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    29174 | 2015-12-13 00:00:00 | 0.9141830973245461 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     1816 | 2015-12-13 00:00:00 | 0.9116947701339125 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    15936 | 2015-12-13 00:00:00 |  0.910420304141231 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     4019 | 2015-12-13 00:00:00 | 0.9101049552562018 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     8835 | 2015-12-13 00:00:00 |  0.909611746159294 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    30294 | 2015-12-13 00:00:00 | 0.9053675764327609 |          0 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    27955 | 2015-12-13 00:00:00 | 0.9046749372346382 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    27956 | 2015-12-13 00:00:00 | 0.9046749372346382 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    13783 | 2015-12-13 00:00:00 | 0.9015988243973061 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     8760 | 2015-12-13 00:00:00 | 0.8936300434147297 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    27071 | 2015-12-13 00:00:00 | 0.8889020831400419 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    27073 | 2015-12-13 00:00:00 | 0.8889020831400419 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     1381 | 2015-12-13 00:00:00 | 0.8856752816172949 |          0 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    23063 | 2015-12-13 00:00:00 | 0.8838436286127278 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     1355 | 2015-12-13 00:00:00 | 0.8828514873911587 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    19735 | 2015-12-13 00:00:00 | 0.8806239565602527 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     6959 | 2015-12-13 00:00:00 | 0.8736905232325605 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
:END:



** How can I pick the best one?

Well now you have *42* /model groups/ Which one must you select to
use? Which one is the best one? This is not as easy as it sounds, due
several Factors:

- You could try to pick the best one using just one of the metrics
  that we establish in the config file (=precision@= and =recall@=)
  but at what point of time? Maybe different model groups are the best
  at different prediction times.
- You could just use the one that perfom better in the last temporal point
- Or you could value that the model is stable across time? Maybe is
  not the best but is consistent.
- Also, even if there are several model groups that perform similar,
  the lists generated are more or less similar, so, it doesn't really
  matter which one do you Pick.

=Triage= provides that functionality in =audition=


* Postmodeling

  - Postmodeling?
  - Bias analysis?

* What's next?

  - Add the shape file
    https://data.cityofchicago.org/api/geospatial/gdcf-axmw?method=export&format=Shapefile
    and generate geospatial variables using =location=
  - Text analysis on the /violations/' =comments= column and generate
    new /outcomes/ or /features/?
  - Run =pgdedup= and had a better =semantic.entities=?
  - Routing based on the inspection list?
  - Add more data sources (Census, Schools, bus stops, ACS data, Yelp!)? 

* Notes
[2018-01-01 Mon 00:50]


 /What are you inspecting?/ (people, places, other)
 /How far do you want to predict?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /How often do you want to update the list?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /What do you want to optimize for?/ (e.g. efficiency, long term
 compliance, novelty)


Inspection the join starts from outcomes (outcome centric) (if you
haven't been inspected, we can not said anything about you)



** A neat trick

Add small, medium, full grid (Rayid magic 
