#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sh  :results verbatim org
#+PROPERTY: header-args:sh+ :prologue exec 2>&1 :epilogue :
#+PROPERTY: header-args:ipython   :session food_inspections :results org


* Triage

Predictive analytics projects require the coordination of many
different tasks, such as feature generation, classifier training,
evaluation, and list generation. These tasks are complicated in their
own right, but in addition have to be combined in different ways
throughout the course of the project.

=triage= aims to provide interfaces to these different phases of a
project, such as an Experiment. Each phase is defined by configuration
specific to the needs of the project, and an arrangement of core data
science components that work together to produce the output of that
phase.

The domain  problems that =triage= had in mind are (1) early warning systems
/ early intervention systems and (2) prioritization of resources for
inspections.

=triage= was created to facilitate the creation of supervised learning
models, in particular classification models with an strong temporal
component in the data.

The temporal component in the data set affects the modeling mainly in
two ways, first, you need to be very careful and avoid /leakage/ of
information in the data, and second, in the possible temporal drifting of the
data. =triage= solves the first splitting the data in temporal blocks to be
used in the temporal crossvalidation and using those blocks for the
feature generation.

=triage= uses the concept of /experiment/. An /experiment/ consists in a
series of steps which aim to generate a good model for predicting the
/label/ of an new instance of the data set. The steps are /feature generation/,
/label generation/, /model training/ and /model scoring/. In each of all
this steps, =triage= will take care of the temporal nuances of the data.

You need to specify (via a configuration file) how you want to time
split your data, which combination of machine learning algorithms and
their hyperparameters, which kind of features you want to generate and which
subset of those features you want to try in each model. So, the
experiment consists in try every combination of algorithm,
hyperparameters and feature subset, and evaluate their performance
using a set of metrics (also specified in the config file).

=triage= will train one model for each block generated, so when the
experiment finishes you will have several models for each algorithm
and selection of hyperparameters. =triage= calls this a =model_group=.

** Triage interface
:PROPERTIES:
:ORDERED:  t
:END:

=triage= is very simple to use, but it contains a lot of complex
concepts that we will try to clarify in this section of the tutorial.

For running a =triage= experiment you need the following:

- =triage= installed in your environment. You can verify that =triage= is installed (and check
  its version) typying the following inside an =ipython= session *in =bastion=*:

#+BEGIN_SRC ipython
import triage

triage.__version__
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org

'2.2.0'
#+END_SRC

- A database with (obviously) and two tables (at minimum): one that
  describes the /outcome/ of each event on the database and a table
  that contains the state of each entity.

- An /experiment config file/, this is where the magic happen. We will
  discuss this file at length in this section of the tutorial.

With this three components you can create your =experiment= object and
=run= it. In this tutorial we are providing a =docker= container that
executes =triage= experiments. 

You can run the container from your laptop (i.e. outside =bastion=) as follows:

#+BEGIN_SRC sh
./tutorial.sh triage --help
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Usage: triage_experiment [OPTIONS] COMMAND [ARGS]...

Options:
  --config_file PATH        Triage's experiment congiguration file name 
                            NOTE:
                            It's assumed that the file is located inside
                            triage/experiment_config)  [required]
  --triage_db TEXT          DB URL, in the form of
                            'postgresql://user:password@host_db:host_port/db',
                            by default it gets this from the environment
                            (TRIAGE_DB_URL)  [required]
  --replace / --no-replace  Triage will (or won't) replace all the matrices
                            and models
  --debug                   Activate to get a lot of information in your
                            screen
  --help                    Show this message and exit.

Commands:
  audit_models
  run
  show_feature_generators
  show_model_plot
  show_temporal_blocks
  validate
#+END_SRC

You already had the database (you were working on it the last two
sections of the tutorial) and the tutorial provide a recent container
with =triage= installed. So, here, like in a real project you just
need to worry about the /experiment's configuration file/. But before,
we need to setup two more tables.

*** A tale of two tables

The first thing that =triage= will do is split the time that the data
covers in blocks considering the time horizon for the /label/
(i.e. the thing that we want to predict: /Which facilities will fail an inspection in the following 3 months?/
In the case of *inspection prioritization* or /Would be my restaurant inspected in the following month?/ 
If you are working in a *early warning system* problem.) This time
horizon is calculated from a set of specific dates (=as_of_date= in
triage parlance) that divide the blocks in past (for training the
model) and future (for testing the model).

=triage= will create those /labels/ using information about the /outcome/ of
the event, taking in account the temporal structure of the data. 
As an example of an /outcome/ consider this  if a inspection is
realized (the event) and the facility fails the inspection (outcome
/true/) or not (outcome /false/). 

So, for a given /as of date/, in our data, for each entity, =triage=
will ask: Are positive outcomes in
the future time horizon? If so, =triage= will generate a positive
/label/ for that specific entity on that /as of date/. Henceforth, we
need to create an outcomes table.

The table that is needed describe the /states/ of each entity. 
The table  should have columns =entity_id=, =start__time, end_time= and =state=.
The states table allows us to only include rows in your matrices in a
specific state. The rationale of this comes from the need of only
predict for entities in a particular state: Do the restaurant still
open? Do the restaurant is new? etc.

In order of exemplify and explain the working of =triage=, we will
create a subset of the =semantic.events= : two facilities (=entity_id=s =9582= and =10854=),
two variables (=inspection_type, risk=), and spatial and temporal dimensions for aggregation (=location=,
=zip_code= and =date=).

Create a new =schema= called =testing_triage=

#+BEGIN_SRC sql :tangle ./sql/create_testing_triage.sql
create schema if not exists testing_triage;
#+END_SRC

#+RESULTS:

Almost all the components of =triage= works with =SQL= tables stored  in
=PostgreSQL= (this is very important to remember), so, let's create our
test table with the entities =9582= and =10854=

#+BEGIN_SRC  sql :tangle ./sql/create_testing_triage.sql
drop table if exists testing_triage.events;

create table testing_triage.events as (
select
event_id, 
entity_id,
facility_type,
result,
type as inspection_type, risk, -- variables
violations, -- json array of variables
date, location, zip_code -- spatio temporal dimensions
from semantic.events
where entity_id in (9582, 10854)
)
#+END_SRC

#+RESULTS:

=testing_triage.events= contains three categorical variables (=inspection_type,risk, result=),
two differnent groups for aggregation (=location, zip_code=), and the date
when the inspection happened (=date=).

#+BEGIN_SRC sql
select 
entity_id, 
inspection_type, risk, result,
date, 
zip_code 
from testing_triage.events
order by date desc
limit 5
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | inspection_type | risk   | result |       date | zip_code |
|----------+----------------+--------+--------+------------+---------|
|    10854 | complaint      | high   | pass   | 2017-10-04 |   60636 |
|    10854 | complaint      | high   | fail   | 2017-09-26 |   60636 |
|    10854 | canvass        | high   | pass   | 2017-06-20 |   60636 |
|     9582 | complaint      | medium | pass   | 2017-02-21 |   60621 |
|     9582 | complaint      | medium | fail   | 2017-02-10 |   60621 |
:END:

For this test, we will keep things simple and define the /outcome/ as
=TRUE= if the inspection got a result adverse and =FALSE= Otherwise.

=triage= requires that the table =outcomes= has the column names
=entity_id=, =outcome_date= and =outcome=:  

#+BEGIN_SRC sql :tangle ./sql/create_testing_triage.sql
drop table if exists testing_triage.outcomes;

create table testing_triage.outcomes as (
select 
entity_id, 
date as outcome_date, 
(result = 'fail') as outcome
from testing_triage.events
);

#+END_SRC

#+RESULTS:

#+BEGIN_SRC sql
select * from testing_triage.outcomes limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | outcome_date | outcome |
|----------+-------------+---------|
|     9582 |  2016-02-17 | f       |
|     9582 |  2016-02-25 | f       |
|     9582 |  2011-04-22 | f       |
|     9582 |  2012-02-29 | t       |
|     9582 |  2012-02-21 | t       |
:END:

#+BEGIN_SRC sql
select 
outcome, count(*) 
from testing_triage.outcomes
group by outcome;
#+END_SRC

#+RESULTS:
:RESULTS:
| outcome | count |
|---------+-------|
| f       |    70 |
| t       |    19 |
:END:

We will only consider only one /state/: Is the
facility "active" or not?[fn:1]. =triage= also impose some constraints
to the table that  represents the state: it must include columns Named
=entity_id=, =start_time=, =end_time= ans =state=

#+BEGIN_SRC sql :tangle ./sql/create_testing_triage_schema.sql
drop table if exists testing_triage.active_facilities cascade;

create table testing_triage.active_facilities as (
    select 
    entity_id, facility_type, location, 
    start_time, 
    case
    when end_time is NULL
    then '2020-01-01'
    else end_time
    end as end_time,
    'active' as state 
    from semantic.entities
    where entity_id in (9582, 10854)
);

#+END_SRC

#+Results:

=triage= doesn't support open date intervals, so we had to impute
=end_time= with the date '2020-01-01'

#+BEGIN_SRC sql
select entity_id, start_time, end_time, state
 from testing_triage.active_facilities ;
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id |  start_time |    end_time | state  |
|----------+------------+------------+--------|
|     9582 | 2010-02-24 | 2017-09-15 | active |
|    10854 | 2010-01-08 | 2020-01-01 | active |
:END:

Note that the entity =10854= is still active and =9582= is not active
after =2017-09-15=.

*** Experiment's configuration file

The /experiment configuration file/ is used to create the =experiment=
object. Here, you will specify the temporal configuration, the
features to be generated, the labels to learn and the models that you
want to train in your data.

The configuration file is a =yaml= file with the following main sections:

- [[Temporal crossvalidation][temporal_config]] :: Temporal specification of the data, used for
     creating the blocks for temporal crossvalidation.

- =events_table= :: Table that contains the information about the labels
                    to be predicted. This is the =outcomes= table that
                    we describe earlier.

- [[Feature engineering][feature_generation]] :: Which spatio-temporal aggregations of the
     columns in the data set do you want to generate as features for
     the models?

- =state_config=  :: Specify which objects are in a given state in a
     particular interval of time, you can use this for filter which
     objects should be included in the training and prediction. This
     is the =states= table described above.

- =model_group_keys= :: How do you want to identify the =model_group= in
     the database (so you can run analysis on them)

- =grid_config= :: Which combination of hyperparameters and algorithms
                   will be trained and evaluated in the data set?

- =scoring= :: Which metrics will be calculated?


Two of the more important sections (and the more confusing too) are
=temporal_config= and =feature_generation=. We will explain them at
detail in the next sections.

**** Temporal crossvalidation

Cross validation is a common technique to reduce overfitting and model and
 hyperparameter. Standard cross validation randomly splits the
 training data into subsets, fits models on all except one of them,
 and calculates the metric of interest (e.g. precision/recall) on the
 remaining, rotating through the subsets and leaving each out
 once. You then select the model that performed best on the test sets,
 and then retrain them.  

Unfortunately, standard cross validation is usually inappropriate for
real-world data science problems like the ones that we are facing. If
you are testing your model on temporally 
correlated data, standard cross validation lets  you peek ahead into
the future, due to the random split, using time points both before and
after the target date. To avoid this problem, you should design  your training and
testing to mimic how your model will be used, making predictions with
only the data that would be available at  that time (i.e. from the past). 

In temporal crossvalidation, rather than randomly splitting the
dataset into training and test splits, temporal cross validation
splits the data by time. For example, we can fit models on training
sets up to  1/1/2010 and see how well those models would have
predicted 2010; fit more models on
training sets up to 1/1/2011 and see how  well those models would have
predicted 2011; and so on. That way, we choose models that have
historically performed best at our task,  forecasting. It’s why this
approach is sometimes called “evaluation on a rolling forecast
origin.”  

The most acute problem is avoiding leakaging information
in a temporal setting. Note that this spliting will affect the
generation of features, because you don't want to have leakage on the
generation. 


=triage= uses the handy =timechop= library for this purpose. =Timechop=
will build ("chop") the data set in several temporal blocks. These
blocks will be used for creating the features and matrices for
the training and evaluation of the machine learning models.

Timechop has several parameters, first, you need to specify The 
 limits of your data:

- =feature_start_time= :: data aggregated into features begins at this
     point (earliest date included in features)
- =feature_end_time= :: data aggregated into features is from before this
  point (latest date included in features)
- =label_start_time= :: data aggregated into labels begins at this
     point (earliest event date included in any label (event date >= label_start_time)
- =label_end_time= :: data aggregated is from before this point (event
     date < label_end_time to be included in any label)  

Other parameters controls the /labels/' time horizon, you have two
'knobs', one for training and one for testing.

- =training_label_timespans= :: how much time is covered by training
     labels (e.g., outcomes in the next 1 year? 3 days? 2 months?)
     (training prediction span) 

- =test_label_timespans= :: how much time is covered by test
     prediction (e.g., outcomes in the next 1 year? 3 days? 2 months?)
     (test prediction span)

These parameters will be used, together with the /outcomes/ table to
generate the /labels/. In an *EIS* setting regularly both will have
the same value. For *inspections prioritization* this value is most of
the time equal to =test_durations= and to =model_update_frequency=.

- =model_update_frequency= :: amount of time between train/test splits
     (how frequently to retrain models)

- =test_durations= :: how far into the future should a model be used
     to make predictions (test span)
     *NOTE*: in the typical case of wanting a single
     prediction set immediately after model training, this should be
     set to 0 days

This last parameter is other that differes if the problem is an *EIS*
or an *inspections prioritization*. In the former is recommended to be
equal to =model_update_frequency=,  in the latter is determined by the
organizational process: /how far out are you scheduling for?/.

The equivalent of =test_durations= for the training matrices is =max_training_histories=

- =max_training_histories= :: the maximum amount of history for each
  entity to train on (early matrices may contain less than this time
  if it goes past label/feature start times)

Finally, we should specify how many rows per =entity_id= in the train
 and test matrix

- =training_as_of_date_frequencies= :: how much time between rows for a
  single entity in a training matrix (list time between rows for
  same entity in train matrix) 

- =test_as_of_date_frequencies= :: how much time between rows for a
  single entity in a test matrix (time between rows for same entity in test matrix)


The following images (We will show you how to generate them later)
shows the time blocks of several configurations. We will change one
parameter at the time so you could see how that affects the blocks.

***** ={feature, label}_{end, start}_Time=

The image below shows these ={feature, label}_start_time= equal, and the same for the
={feature, label}_end_time= ones. These parameters show in the image
as dashed vertical black lines. This setup would be our *base*
example.

The plot is divided in two horizontal lines ("Block 0" and "Block
1"). Each line is divided by vertical dashed lines, the grey ones are
the boundaries of the data for features and data for labels, and in
this image they coincide. The black dash lines represents the
beginning and the end of the test set. In the "Block 0" those lines
are =2017= and =2018=, in "Block 1" they are =2016= and =2017=.

The shaded areas (in this image there is just one per block, but you
will see another examples below) represents the span of all the /as of dates/
They start with the oldest /as of date/ and end in the latest. Each
line inside that area represents the span for the label
calculation. Those lines begin at the /as of date/. In each /as of
date/ all the entities will get calculated their features (to the
past) and the labels (to the future). So in the image, we will have
two sets of train/test, in the "Block 0" our entity =9587= will have
13 rows of features,  and 12 on "Block 1". The trainned models will
predict the label using the features calculated in that /as of date/
in the  test data set, the solitary line represents the label's time
horizon in testing.


#+NAME: fig:timechop_1
#+CAPTION: feature and label start, end time equal
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_1.svg]]

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '1y'  
#+END_EXAMPLE

But they can be different (maybe you have more data for features that
data for labels)

#+NAME: fig:timechop_2
#+CAPTION: feature_start_time different different that label_start_time.
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_2.svg]]


#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2010-01-01'   # <------- The change happened here!
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '1y'  
#+END_EXAMPLE

***** =model_update_frequency= 
From our *base* =temporal_config= example ([[fig:timechop_1]]), we will
change how often we want a new model, so we need more train/test sets:

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '6month' # <------- The change happened here!
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '1y'  
#+END_Example

#+NAME: fig:timechop_3
#+CAPTION: A smaller model_update_frequency (from 1y to 6month) (The number of blocks grew)
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_3.svg]]


***** =max_training_histories=

With this parameter you could get a /growing window/ for training
(depicted in [[fig:timechop_4]]) or as in all the other examples,  
/fixed training windows/.

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  # <------- The change happened here!
#+END_Example


#+NAME: fig:timechop_4
#+CAPTION: The size of the block is bigger now
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_4.svg]]

***** =_as_of_date_frequencies= and =test_durations=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '3month' # <------- The change happened here!

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_5
#+CAPTION: More rows per entity in the training block
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_5.svg]]

Now, change =test_as_of_date_frequencies=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month' 

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '3month'<------- The change happened here!

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_6
#+CAPTION: We should get more rows per entity in the test matrix, but that didn't happen. Why?
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_6.svg]]

Nothing change, that's because the test set doesn't have "space", that
is controlled by =test_durations=, let's move that to to =6month=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month' 

    test_durations: '6month' <------- The change happened here!
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_7
#+CAPTION: The test duration is bigger now, so we got 6 rows (since the "base" frequency is 1 month)
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_7.svg]]

So, now we will move both parameters: =test_durations=, =test_as_of_date_frequencies=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month' 

    test_durations: '6month' <------- The change happened here!
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '3month' <------- and also here!

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_8
#+CAPTION: With more room in testing, now test_as_of_date_frequencies has some effect.
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_8.svg]]

***** =-label_timespans=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month' 

    test_durations: '0d' 
    test_label_timespans: ['3month']  <------- The change happened here!
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_9
#+CAPTION: The label time horizon in testing is smaller
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_9.svg]]

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['3month'] <------- The change happened here!
    training_as_of_date_frequencies: '1month' 

    test_durations: '0d' 
    test_label_timespans: ['1y']  
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_10
#+CAPTION: The label time horizon is smaller in trainning, also, now we have more room for more rows per entity.
#+ATTR_ORG: :width 100 :height 100
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/timechop_10.svg]]


**** TODO Feature engineering

We will show how to create features using the /experiments config
file/. =triage= for this end, uses =collate=. =Collate= is the python
library that controls the generation of features (including the imputation rules
for each feature generated). =Collate= helps the modeler to
create features based on /spatio-temporal aggregations/ (which is what
we need in our modeling strategy based on *events*)

As a first feature we want to know in a given interval of time, in
a given specific date (remember /as of date/), /how many Inspections
 do each facility had?/ and /how many flags resulted in "high risk"
after the last inspection?/ (the =risk= column), 
happened to that facility and the same questions but aggregated in the
zip code in which the facility operates. 

Let's try to construct that in =SQL=:

#+BEGIN_SRC sql
select entity_id, zip_code,
count(*) as inspections,
count(*) filter (where risk='high') as flagged_as_high_risk
from testing_triage.events
group by grouping sets(entity_id, zip_code)
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | zip_code | inspections | flagged_as_high_risk |
|----------+---------+-------------+-------------------|
| 9581     | [NULL]  |          45 |                 0 |
| [NULL]   | 60621   |          45 |                 0 |
:END:

This query is making an /aggregation/.Note that the previous =SQL=
query is composed by four parts: 
  - The filter ((=risk = 'high')::int=)
  - The aggregation function (=count()=)
  - The name of the resulting transformation (=flagged_as_high_risk=)
  - The context in which it is aggregated (by =entity_id= and =zip_code=).

What about if we want to add the proportion of all the inspections
that resulted in be flagged as "high risk"?

#+BEGIN_SRC sql
select entity_id, zip_code,
count(*) as inspections,
count(*) filter (where risk='high') as flagged_as_high_risk,
avg((risk='high')::int) as proportion_of_flags_as_high_risk
from testing_triage.events
group by grouping sets(entity_id, zip_code)
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | zip_code | inspections | flagged_as_high_risk | proportion_of_flags_as_high_risk |
|----------+---------+-------------+-------------------+-----------------------------|
| 9581     | [NULL]  |          45 |                 0 |      0.00000000000000000000 |
| [NULL]   | 60621   |          45 |                 0 |      0.00000000000000000000 |
:END:

But, what if we want to add also "medium" and "low" risk? And note
that we didn't add the temporal interval neither. You can see that the
event this simple set of features will require a very complex =SQL= to
be constructed.




** Machine learning governance: The =RESULTS= schema

While =triage= is executing the experiment, it will create a new schema,
called =results=. This schema has the goal of storing the output of the
models and describing the features, parameters and hyperparameters
used in their training.

The tables contained in =results= are:

#+BEGIN_SRC sql
\dt results.*
#+END_SRC

#+RESULTS:
:RESULTS:
| List of relations |                       |       |          |
|-------------------+-----------------------+-------+----------|
| Schema            | Name                  | Type  | Owner    |
| results           | evaluations           | table | food_user |
| results           | experiments           | table | food_user |
| results           | feature_importances    | table | food_user |
| results           | individual_importances | table | food_user |
| results           | list_predictions       | table | food_user |
| results           | model_groups           | table | food_user |
| results           | models                | table | food_user |
| results           | predictions           | table | food_user |
:END:

*** What are all the results tables about?
=model_groups= stores the algorithm (=model_type=), the
hyperparameters (=model_parameters=) and the features shared by a
particular set of models. =models= contains data specific to a model of
the =model_group= (you can use =model_group_id= for linking the model to a
model group) this table also includes temporal information (like
=train_end_time=) and a reference to the  train matrix
(=train_matrix_uuid=). This *UUID* is important
since that is the name of the file in which the matrix is stored.

Lastly, =results.predictions= contains all the /scores/ generated by every
model for every entity. =results.evaluation= stores the value of all the
*metrics* for every model. These metrics were specified in the =scoring=
section in the config file.

**** =results.experiments=
This table has the two columns: =experiment_hash= and =config=

#+BEGIN_SRC sql
\d results.experiments
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "results.experiments"                                                                                                              |                   |           |
|------------------------------------------------------------------------------------------------------------------------------------------+-------------------+-----------|
| Column                                                                                                                                   | Type              | Modifiers |
| experiment_hash                                                                                                                           | character varying | not null  |
| config                                                                                                                                   | jsonb             |           |
| Indexes:                                                                                                                                 |                   |           |
| "experiments_pkey" PRIMARY KEY, btree (experiment_hash)                                                                                    |                   |           |
| Referenced by:                                                                                                                           |                   |           |
| TABLE "results.models" CONSTRAINT "models_experiment_hash_fkey" FOREIGN KEY (experiment_hash) REFERENCES results.experiments(experiment_hash) |                   |           |
:END:

=experiment_hash= contains the hash of the config that we used for our
=triage= run. =config= that  contains the 
configuration experiment file  that we used for our Triage run, stored
as =jsonb= 

We can note for our future selfs: If we are interested in all models 
that resulted from a certain config, we could  lookup that config In
=results.experiments= and then use its =experiment_hash=  on other tables
to find all the models that resulted from that configuration.

**** =results.model_groups=

Do you remember how we defined in =grid_config= the different
classifiers that we want =triage= to train? For example, we  said:

#+BEGIN_EXAMPLE yaml
    'sklearn.tree.DecisionTreeClassifier':
        criterion: ['entropy']
        max_depth: [1, 2, 5, 10]
        random_state: [2193]
#+END_EXAMPLE

By doing so, we are saying that we want to train 4 decision trees
(=max_depth= is one of =1, 2, 5, 10=). However, remember that  we are using
temporal cross-validation to build our models. That  means that we are
going to have different slices of time that we  are training our
models on, e.g., 2010-2011, 2011-2012, etc. 

Therefore, we are going to train our four configurations of the
decision trees on each time slice. Therefore, the  trained model (or
the instance of that model) will change across time  splits, but the
configuration will remain the same. This table lets  us keep track of
the different configurations (=model_groups=) and gives  us an =id= for
each configuration (=model_group_id=). We can leverage the =model_group_id=
to find all the models that were trained by using the  same config,
but across different slices of time. 

#+BEGIN_SRC sql
select 
model_group_id, model_type, model_parameters, model_config
from 
results.model_groups
limit 1
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                           | model_parameters                   | model_config                                             |
|--------------+-------------------------------------+-----------------------------------+---------------------------------------------------------|
|            1 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": 1} | {"experiment_type": "test", "label_definition": "failed"} |
:END:

You can see that a model group is defined by the classifier
(=model_type=), its parameters (=model_parameters=), the features
(=feature_list=) (not shown), and the =model_config=. The =model_config= follows
from the =model_group_keys= we had defined in the configuration file:

    - 'train_duration'
    - 'label_window'
    - 'example_Frequency'

/What can we learn from that?/ For example, if we add a new feature and
rerun =triage=, =triage= will create a new =model_group= even if the
classifier and the =model_parameters= are the same as before. 

**** =results.models=

This table stores the information about our actual /models/, i.e.,
instances of our classifiers trained on specific time Slices. 
#+BEGIN_SRC sql
\d results.models
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "results.models"                                                                                                                       |                             |                                                                |
|----------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------------------------------------------------------------|
| Column                                                                                                                                       | Type                        | Modifiers                                                      |
| model_id                                                                                                                                      | integer                     | not null default nextval('results.models_model_id_seq'::regclass) |
| model_group_id                                                                                                                                 | integer                     |                                                                |
| model_hash                                                                                                                                    | character varying           |                                                                |
| run_time                                                                                                                                      | timestamp without time zone |                                                                |
| batch_run_time                                                                                                                                 | timestamp without time zone |                                                                |
| model_type                                                                                                                                    | character varying           |                                                                |
| model_parameters                                                                                                                              | jsonb                       |                                                                |
| model_comment                                                                                                                                 | text                        |                                                                |
| batch_comment                                                                                                                                 | text                        |                                                                |
| config                                                                                                                                       | json                        |                                                                |
| experiment_hash                                                                                                                               | character varying           |                                                                |
| train_end_time                                                                                                                                 | timestamp without time zone |                                                                |
| test                                                                                                                                         | boolean                     |                                                                |
| train_matrix_uuid                                                                                                                              | text                        |                                                                |
| training_label_timespan                                                                                                                        | interval                    |                                                                |
| Indexes:                                                                                                                                     |                             |                                                                |
| "models_pkey" PRIMARY KEY, btree (model_id)                                                                                                    |                             |                                                                |
| "ix_results_models_model_hash" UNIQUE, btree (model_hash)                                                                                         |                             |                                                                |
| Foreign-key constraints:                                                                                                                     |                             |                                                                |
| "models_experiment_hash_fkey" FOREIGN KEY (experiment_hash) REFERENCES results.experiments(experiment_hash)                                       |                             |                                                                |
| "models_model_group_id_fkey" FOREIGN KEY (model_group_id) REFERENCES results.model_groups(model_group_id)                                             |                             |                                                                |
| Referenced by:                                                                                                                               |                             |                                                                |
| TABLE "results.evaluations" CONSTRAINT "evaluations_model_id_fkey" FOREIGN KEY (model_id) REFERENCES results.models(model_id)                     |                             |                                                                |
| TABLE "results.feature_importances" CONSTRAINT "feature_importances_model_id_fkey" FOREIGN KEY (model_id) REFERENCES results.models(model_id)       |                             |                                                                |
| TABLE "results.individual_importances" CONSTRAINT "individual_importances_model_id_fkey" FOREIGN KEY (model_id) REFERENCES results.models(model_id) |                             |                                                                |
| TABLE "results.list_predictions" CONSTRAINT "list_predictions_model_id_fkey" FOREIGN KEY (model_id) REFERENCES results.models(model_id)             |                             |                                                                |
| TABLE "results.predictions" CONSTRAINT "predictions_model_id_fkey" FOREIGN KEY (model_id) REFERENCES results.models(model_id)                     |                             |                                                                |
:END:

Noteworthy columns are:

    - =model_id= :: The id of the model (i.e., instance...). We will
                    use this ID to trace back a model's performance
                    evaluation to a =model_group= and vice versa. 
    - =model_group_id= :: The id of the models model_group we encountered above.
    - =model_hash= :: The /hash/ of our model. We can use the hash to
                      load the actual model. It gets stored under
                      =TRIAGE_OUTPUT_PATH/trained_models/{model_hash}=. We
                      are going to this later to look at a trained
                      decision tree. 
    - =run_time= ::  Time when the model was trained.
    - =model_type= ::  The algorithm used for trainning
    - =model_parameters= :: Hyperparameters used for the model configuration.
    - =experiment_hash= :: The hash of our experiment. We encountered this value in the =results.experiments= table before.
    - =train_end_time= :: When building the training matrix, we included training samples up until this date.
    - =train_matrix_uuid= :: The /hash/ of the matrix that we used to
         train this model. The matrix gets stored as =csv= under 
        =TRIAGE_OUTPUT_PATH/matrices/{train_matrix_uuid}.csv=. This is very helpful
        when trying to inspect the matrix and features that were used
        for training. 
    - =train_label_window= :: How big was our window to get the labels for our training
        matrix? For example, a =train_label_window= of 1 year would
        mean that we look one year from a given date in the training
        matrix into the future to find the label for that training
        sample. 

**** =results.evaluations=

This table lets us analyze how well our models are doing. Based on the
config that we used for our =triage= run, =triage= is calculating metrics
and storing them in this table, e.g., our model's precision at top 10%. 

#+BEGIN_SRC sql 
\d results.evaluations
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "results.evaluations"                                                                                                  |                             |           |
|------------------------------------------------------------------------------------------------------------------------------+-----------------------------+-----------|
| Column                                                                                                                       | Type                        | Modifiers |
| model_id                                                                                                                      | integer                     | not null  |
| evaluation_start_time                                                                                                          | timestamp without time zone | not null  |
| evaluation_end_time                                                                                                            | timestamp without time zone | not null  |
| as_of_date_frequency                                                                                                            | interval                    | not null  |
| metric                                                                                                                       | character varying           | not null  |
| parameter                                                                                                                    | character varying           | not null  |
| value                                                                                                                        | numeric                     |           |
| num_labeled_examples                                                                                                           | integer                     |           |
| num_labeled_above_threshold                                                                                                     | integer                     |           |
| num_positive_labels                                                                                                            | integer                     |           |
| sort_seed                                                                                                                     | integer                     |           |
| Indexes:                                                                                                                     |                             |           |
| "evaluations_pkey" PRIMARY KEY, btree (model_id, evaluation_start_time, evaluation_end_time, as_of_date_frequency, metric, parameter) |                             |           |
| Foreign-key constraints:                                                                                                     |                             |           |
| "evaluations_model_id_fkey" FOREIGN KEY (model_id) REFERENCES results.models(model_id)                                            |                             |           |
:END:

Its columns are:

    - =model_id= :: Our beloved =model_id= that we have encountered before.
    - =evaluation_start_time= :: After training the model, we evaluate
         it on a test matrix. This column tells us the earliest time
         that an example in our test matrix could have. 
    - =evaluation_end_time= ::  After training the model, we evaluate
      it on a test matrix. This column tells us the latest time that
      an example in our test matrix could have. 
    - =metric= :: Indicates which metric we are evaluating, e.g., =precision@=.
    - =parameter= ::Indicates at which parameter we are evaluating our
      metric, e.g., a metric of precision@ and a parameter of
      =100.0_pct= shows us the =precision@100pct=
    - =value= :: The value observed for our metric@parameter.
    - =num_labeled_examples= :: The number of labeled examples in our
         test matrix. Why does it matter? It could be the case that we
         have entities that we did not observe a label for during our
         test timeframe (for example in the [[file:inspections.org][inspections prioritization]]
         problem) . We still want to make predictions for these 
         entities, but can't include them when calculating performance
         metrics. 
    - =num_labeled_above_threshold= ::    How many examples were labeled as above our treshold?
    - =num_positive_labels= :: The number of rows that had a true positive labels.

A look at the table shows that we have multiple rows for each model to
show the different performance metrics.

#+BEGIN_SRC sql
select * from
results.evaluations
limit 5
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | evaluation_start_time | evaluation_end_time   | as_of_date_frequency | metric     | parameter |               value | num_labeled_examples | num_labeled_above_threshold | num_positive_labels | sort_seed |
|---------+---------------------+---------------------+-------------------+------------+-----------+---------------------+--------------------+--------------------------+-------------------+----------|
|       1 | 2016-01-01 00:00:00 | 2016-01-01 00:00:00 | 1 mon             | precision@ | 5.0_pct    |  0.2653061224489796 |               1034 |                       49 |               247 |        5 |
|       1 | 2016-01-01 00:00:00 | 2016-01-01 00:00:00 | 1 mon             | recall@    | 5.0_pct    | 0.05263157894736842 |               1034 |                       49 |               247 |        5 |
|       1 | 2016-01-01 00:00:00 | 2016-01-01 00:00:00 | 1 mon             | precision@ | 10.0_pct   |  0.2641509433962264 |               1034 |                      106 |               247 |        5 |
|       1 | 2016-01-01 00:00:00 | 2016-01-01 00:00:00 | 1 mon             | recall@    | 10.0_pct   | 0.11336032388663968 |               1034 |                      106 |               247 |        5 |
|       1 | 2016-01-01 00:00:00 | 2016-01-01 00:00:00 | 1 mon             | precision@ | 5_abs      |                 0.0 |               1034 |                        0 |               247 |        5 |
:END:

This table lets us answer: /how a model_group is performing across the different time slices?/:

#+BEGIN_SRC sql
select
model_id, 
evaluation_start_time, 
evaluation_end_time,
metric,
parameter,
value
from results.evaluations
where model_id in (
      select model_id from results.models where model_group_id=1
      )
and metric='precision@' and parameter in ('100.0_pct', '5.0_pct')
order by model_id, evaluation_start_time, parameter;
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | evaluation_start_time | evaluation_end_time   | metric     | parameter |               value |
|---------+---------------------+---------------------+------------+-----------+---------------------|
|       1 | 2016-01-01 00:00:00 | 2016-01-01 00:00:00 | precision@ | 5.0_pct    |  0.2653061224489796 |
|       7 | 2017-01-01 00:00:00 | 2017-01-01 00:00:00 | precision@ | 5.0_pct    | 0.29333333333333333 |
:END:

/What does this query tell us?/

We can now see how the different instances (trained on different time
slices, but with same model params) of one of our models perform over
time. We are including the =precision@5.0_pct= to see what the *baseline*
is. As you can see above, our model is beating the baseline in every
year. Note how we only included the /models/ that belong to Our
/model group/ =1=. 

**** =results.predictions=

You can think of the previous table =results.evaluations= as a summary
of individuals predictions that our model is making. But where can you
find the individual predictions that our model is making? (So you can
generate a list from here). And where can we find the test matrix that
the  predictions are based on? Let us introduce you to The
=results.predictions= table.  

Here is what its first row looks Like:

#+NAME: prediction-example
#+BEGIN_SRC sql
select *
from results.predictions
limit 1
#+END_SRC

#+RESULTS: prediction-example
:RESULTS:
| model_id | entity_id | as_of_date            |               score | label_value | rank_abs | rank_pct | matrix_uuid                       | test_label_timespan |
|---------+----------+---------------------+---------------------+------------+---------+---------+----------------------------------+-------------------|
|       1 |        1 | 2016-01-01 00:00:00 | 0.21631588415182587 | [NULL]     | [NULL]  | [NULL]  | f5ba9602cf9da2cc2fa74eede6ef6d17 | 1 mon             |
:END:



As you can see, the table contains our models' predictions for a given
entity and date. In the case above, our /model/ (=model_id   1=)
predicted a score of src_emacs-lisp[:var d=prediction-example[2,3]]{d}
{{{results(=0.21631588415182587=)}}} . The true label was
src_emacs-lisp[:var d=prediction-example[2,4]]{d}
{{{results(=[NULL]=)}}}. 

And do you notice the field =matrix_uuid=? Doesn't it look similar to
the fields from above that gave us the names of our training matrices?
In fact, it is the same. You can find the test matrix that was used to
make this prediction under  =TRIAGE_OUTPUT_PATH/matrices/{matrix_uuid}.csv=

**** TODO: =results.feature_importances=

**** TODO: =results.individual_importances=

**** TODO: =results.list_predictions=



** Audition

*Audition* is a tool for helping you to select a subset of trained
classifiers from a triage experiment. Often, production-scale experiments
will come up with thousands of trained models, and sifting through all
of those results can be time-consuming even after calculating the
usual basic metrics like precision and recall.

You will be facing questions as:

- Which metrics matter most?
- Should you prioritize the best metric value over time or treat
  recent data as most important?
- Is low metric variance important?

The answers to questions like these may not be obvious up front. *Audition*
introduces a structured, semi-automated way of filtering models based
on what you consider important

** Post-modeling

As the name indicates, *postmodeling* occurs *after* you have modeled
(potentially) thousands of models (different hyperparameters, different
time windows, different algorithms, etc), and using =audition= you /pre/
selected a small number of models.

Now, with the *postmodeling* tools you will be able to select your final
model for using it in /production/.

Triage's postmodeling capabilities include:

- Show the score distribution
- Compare the list generated by a set of models
- Compare the feature importance between a set of models
- Diplay the probability calibration curves
- Error analysis using a decision treee trained in the errors of the model.
- Cross-tab analysis
- Bias analysis

If you want to see *Audition* and *Postmodeling* in action please refer
[[file:inspections.org][Inspections modeling]] or to [[file:eis.org][EIS modeling]] for practical examples.


** What's next?

We will begin with [[file:inspections.org][Inspections problem]], let's go for It

* Footnotes

[fn:1] We could consider different states, for example: we can use the column
=risk= as an state. Another possibility is define a new state called
=failed= that indicates if the facility failed in the last time it was inspected. 

