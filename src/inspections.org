#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:ipython   :session :exports both :results raw drawer
#+PROPERTY: header-args:python    :session food_inspections :results output Drawer
#+PROPERTY: header-args:sh  :results verbatim org
# +PROPERTY: header-args:sh+ :prologue exec 2>&1 :epilogue :
#+PROPERTY: header-args:sh+  :dir ..

* Inpections prioritization
** Problem description

 We will begin with the the *inspections prioritization* problem: we want to generate a list of
   facilities which are /likely/ to have some *critical* o *serious*
   violation /given that/ they are inspected.

The scenario, is the following:  you work for the City of Chicago and you try
  to prioritize your resources (i.e. your inspection workforce), since
  they are limited. So, you will use the data for answering the next question:

#+begin_quote
Which X facilities are likely to violate some code in the
  following Y period of time?
#+end_quote

  In this case maybe you are interested not
  in all the violations but in the more severe ones.

** Creating the labels

We will define two different labels:

- *Which facilities are likely to fail an inspection?*

Facilities who failed an inspection (i.e. =result= = ='fail'=)

- *Which facilities are likely  to fail an inspection with a major  violation?*

Rremember that critical violations are coded between =1-14=, serious violations between
=15-29=, everything above =30= is assumed to be a minor violation.

Facilities who failed an inspection (i.e. =result= = ='fail'=) and the
=severity in ('critical', 'serious')=

We could extract the severity of the violation inspected using the
following code:


#+begin_src sql
select 
event_id,
date,
result, 
array_agg(obj ->>'severity') as violations_severity,
(result = 'fail') as failed,
(result = 'fail' and
('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
) as failed_major_violation
from
(select event_id, date, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events limit 20)
as t1
group by event_id, date, result
order by date desc

#+end_src

#+RESULTS:
:RESULTS:
| event_id |       date | result | violations_severity                                       | failed | failed_major_violation |
|---------+------------+--------+----------------------------------------------------------+--------+----------------------|
| 1770568 | 2016-05-11 | pass   | {critical,minor,minor,serious,serious}                   | f      | f                    |
| 1763967 | 2016-05-03 | fail   | {minor,critical,serious,serious,minor,minor,minor,minor} | t      | t                    |
| 1343315 | 2013-06-06 | fail   | {minor,serious,serious,serious,serious,minor}            | t      | t                    |
|  537439 | 2011-06-10 | fail   | {NULL}                                                   | t      | [NULL]               |
:END:

Remember from [[A tale of two tables]] that the /outcome/ will be used by
=triage= in order of generate the labels. The following image tries to
show the meaning of the /outcomes/ for /inspection failed/ problem definition.

#+NAME: fig:outcomes-inspections
#+CAPTION: The image shows three facilities and next to each of them, a temporal line with 6 days (0-5) each dot represents an inspection. Green means that the inspection had a result "Pass", red that the inspection "Fail". Each of the facilities in the image had two inspections, but only the facility in the middle completed both without fails.
#+ATTR_ORG: :width 600 :height 400
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/outcomes-inspections.png]]

Let's use the previous query to generate our outcomes in a new
=inspections= schema.

#+BEGIN_SRC sql :tangle ./sql/create_inspections_schema.sql
create schema if not exists inspections;
#+END_SRC

#+RESULTS:

=Triage= has some restrictions (at the current version) about how to
name (some) columns, in specific, our columns should include:

- =entity_id=     :: The entity affected / causing the event (In our
     case the facility)
- =outcome_date=  :: The date in which the event happen / The date in
     which we discover the result (The inpection's date)
- =outcome=       :: The result (label) of the event (One of the labels
     speecified before)

=entity_id= an identifier for which the labels are applied to,
=outcome_date= the date at which some outcome was known, =outcome= a
boolean outcome.

Since we defined two labels, we will create two tables one per each outcome.

#+BEGIN_SRC sql :tangle ./sql/create_inspections_schema.sql

create temp table inspections_outcomes as (
select event_id, entity_id, date,
   (result = 'fail') as failed,
   (result = 'fail' and
       ('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
   ) as failed_major_violation
from
   (select event_id, entity_id, date, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events)
as t1
group by event_id, entity_id, date, result
);


drop table if exists inspections.failed;

create table inspections.failed as (
select
entity_id,
date as outcome_date,
failed as outcome
from inspections_outcomes
);


drop table if exists inspections.failed_major_violation;

create table inspections.failed_major_violation as (
select
entity_id,
date as outcome_date,
failed_major_violation as outcome
from inspections_outcomes
);

#+END_SRC

#+RESULTS:

Also, We need to create a new version of the =semantic.entities=
table. =Triage= refers to this new table as the *states* table. It should
have columns =entity_id=, =start__time, end_time= and =state=.
The states table allows us to only
include rows in your matrices in a specific state. In our case we only want
to inspect *active* facilities. We will replace all the =NULL= values in
the =end_time= column for a date in the future, in particular =2020-12-31=.

#+BEGIN_SRC sql :tangle ./sql/create_inspections_schema.sql

drop table if exists inspections.active_facilities;

create table inspections.active_facilities as (
select
distinct
entity_id, 'active'::VARCHAR  as state, start_time, coalesce(end_time, '2020-12-31'::date) as end_time
from semantic.entities
);
#+END_SRC

#+RESULTS:


** Modeling using Machine Learning

It is time of getting all the previous steps and put them
together. Don't worry, actually we are done with coding. =Triage= provides
you with a configuration file for specifying the experiment that we
want to run.

*** Creating a simple experiment

For this first experiment we will try one of the simplest
machine learning algorithms: a *Decision Tree Classifier*. We need to
write the experiment config file for that, let's break it down and
explain all the sections.

The config file for this first experiment is located in
[[./triage/experiment_config/inspections_test.yaml]]


The first lines of the experiment config file are related to the
version config file (=v3= at the moment of writing this tutorial), a
comment (=model_comment=), this will end up as
a value in the =results.models= table, and a list of user defined
metadata (=user_metadata=) that could be used for identifying the
resulting model groups. In our test example, if you run experiments that share
a temporal configuration but that use different label definitions
(say, labeling building inspections with *any* violation as positive or
labeling only building inspections with major violations as positive),
you can use the user metadata keys to indicate that the matrices
from these experiments have different labeling criteria. The matrices from the
two experiments will have different filenames (and not be overwritten or
inappropriately reused), and if you add the =label_definition= key to
the =model_group_keys=, models made on different label definition will
have different groups.

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
config_version: 'v3'

model_comment: 'inspections_test'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'inspections prioritization'
  purpose: 'test'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'
#+END_SRC

Next, the *temporal configuration*  section. The first four parameters
are related to the availability of data: How much data you have for
feature creation? How much data you have for label generation? For
simplicity we will assume that we can use the full =semantic.events= time
span for both.

#+BEGIN_SRC sql
select min(date), max(date) from semantic.events
#+END_SRC

#+RESULTS:
:RESULTS:
|        min |        max |
|------------+------------|
| 2010-01-04 | 2018-03-01 |
:END:

The next parameters are related to the training intervals:
- How frequently to retrain models? (=model_update_frequency=)
- How many rows per entity in the train matrices?
  (=training_as_of_date_frequencies=)
- How much time is covered by labels in the training matrices? (=training_label_timespans=)

The remaining elements are related to the *testing* matrices, in the
particular case of *inspections*, you can choose them as follows:

- =test_as_of_date_frequencies= is planning/scheduling frequency
- =test_durations= is how far out are you scheduling for?
- =test_label_timespan= is equal to =test_durations=

Let's assume that we need to do rounds of inspections every month
(=test_as_of_date_frequencies = 1month=) and we need to complete that
round in exactly one month (=test_durations = test_label_timespan =
1month=)

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
temporal_config:
    feature_start_time: '2010-01-04'
    feature_end_time: '2018-03-01'
    label_start_time: '2015-02-01'
    label_end_time: '2018-03-01'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '1month'
    test_label_timespans: ['1month']
    test_as_of_date_frequencies: '1month'

    max_training_histories: '5y'
#+END_SRC

We can visualize the splitting using the function =show_timechop=
introduced in [[file:triage_intro.org][Introduction to triage]]. 

#+BEGIN_SRC sh 

# Remember to run this in your laptop NOT in bastion!

./tutorial.sh triage --config_file inspections_test.yaml show_temporal_blocks
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/inspections_test.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Generating temporal blocks image
Image stored in:
/triage/output/images/inspections_test.svg
#+END_SRC

#+ATTR_ORG: :width 600 :height 400
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/triage/inspections_test.svg]]

We need to specify the table that keeps our labels, for this first
experiment we will use the label =failed=, stored in =inspections.labels=.

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
events_table: inspections.failed
#+END_SRC

=Triage= will generate the features for us, we need to tell which ones
in the section =feature_aggregations=. Here, each entry describes a
=collate.SpacetimeAggregation= object, and the
arguments needed to create it. For this experiment we will try the following
features:

- Number of different types of inspections  that happened in the
  facility in the last year from a particular day
-
- Number of different types of inspections  that happened in the
  zip code in the last year from a particular day

If we observe the image generated from the =temporal_config= section,
each particular date is the beginning of the rectangles that describes
the rows in the matrix. In that date (=as_of_date= in =timechop= parlance)
we will calculate both features, and we will repeat that for every
other rectangle in that image.

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
feature_aggregations:
    -
        prefix: 'inspections'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'type'
                choice_query: 'select distinct type from semantic.events where type is not null'
                metrics:
                    - 'sum'

        intervals:
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
#+END_SRC

We just want to include *active* facilities in our matrices, so we tell
=triage= to take that in account:

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
state_config:
    table_name: 'inspections.active_facilities'
    state_filters:
       - 'active'
#+END_SRC

Now, lets discuss how we will define the different models to try in
the data (Remember that the model is specified by the algorithm, the
hyperparameters, and the subset of features to use). In =triage= you
need to specify in the =grid_config= section, a list of machine learning
algorithms that you want to train, and a set of list of
hyperparameters. You can use any algorithm that you want, the only
requirement is that respects the =sklearn= API.


#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
grid_config:
    'sklearn.tree.DecisionTreeClassifier':
        max_depth: [1,null]
        max_features: [1, sqrt, null]
#+END_SRC

Some of the parameters in =sklearn= are =None=, if you want to try those
you need to indicate that with the =yaml= 's =null= keyword.

Besides the algorithm and the hyperparameters, you should specify
which subset of features use. First, in the section
=feature_group_definition= you specify how to group the features (you
can use the =table name= or the =prefix= from the section
=feature_aggregation=) and then choose one /strategy/ for choosing the
subsets: =all= (all the subsets at once), =leave-one-out= (try all the
subsets except one, do that for all the combinations) or =leave-one-in=
(just try subset at the time).


#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
feature_group_definition:
   prefix: ['inspections']

feature_group_strategies: ['all']
#+END_SRC

In this experiment we will end with *6* model groups ($algorithms (1) \times
hyperparameters combinations (2 \times 3)  \times feature groups (1) \times temporal
combinations (1)$). Also, we will create *12* different models (2 per
each model group) given that we have 2 temporal blocks (one model per
temporal group).

=model_group_keys= defines a list of *additional* matrix metadata keys that
should be considered when creating a model group. For example, if the models are
built on matrices with different history lengths, different
labeling windows (e.g., inspection violations in the next month, next year, or
next two years), the frequency of rows for each
entity, or the definition of a positive label (=label_definition=, from
=user_metadata=).

The valid =model_group_keys= are

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
model_group_keys:
    - 'label_definition'
    - 'experiment_type'
    - 'purpose'
#+END_SRC

Finally, we should define wich metrics we care for evaluating our
model. Here we will concentrate only in =precision= and =recall=.

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_test.yaml
scoring:
    sort_seed: 5
    metric_groups:
        -
            metrics: [precision@, recall@]
            thresholds:
                percentiles: [5.0, 10.0]
                top_n: [5, 10, 25]
#+END_SRC

You should be warned that precision and recall at $k$ in this setting
is kind of ill-defined (because you will end with a lot of =NULL=
labels, remember, only a few of facilities are inspected in each
period) ...

We will want as a result of our experiments, a *list* of facilities to
be inspected. The length of our list is contrained by our inspection
resources, i.e. the answer to the question How many facilities can I
inpect in a month?. In this experiment we are assuming that the
maximum capacity is *25* but we are testing also for a list of length
*5*, and *10* (see =top_n= Above).

The execution of the experiments could take a long time, so, it is a
good practice to  /validate/ the configuration file, /before/ running
the model. You don't want to wait for hours (or days) and then
discover that there was something wrong

#+BEGIN_SRC sh 
./tutorial.sh triage --config_file inspections_test.yaml validate
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/inspections_test.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Validating experiment's configuration
Experiment validation ran to completion with no errors

----TIME SPLIT SUMMARY----

Number of time splits: 3
Split index 0:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2015-12-01 00:00:00 (11 total)
            Testing as_of_time range: 2016-01-01 00:00:00 to 2016-01-01 00:00:00 (1 total)


Split index 1:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2016-12-01 00:00:00 (23 total)
            Testing as_of_time range: 2017-01-01 00:00:00 to 2017-01-01 00:00:00 (1 total)


Split index 2:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2017-12-01 00:00:00 (35 total)
            Testing as_of_time range: 2018-01-01 00:00:00 to 2018-01-01 00:00:00 (1 total)


For more detailed information on your time splits, inspect the experiment `split_definitions` property

           The experiment configuration doesn't contain any obvious errors.
           Any error that occurs from now on, possibly will be related to hit the maximum 
           number of columns allowed or collision in
           the column names, both due to PostgreSQL limitations.
    
The experiment looks in good shape. May the force be with you
#+END_SRC

You can execute the experiment as 

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_test.yaml run
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/inspections_test.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Executing experiment
Done
Experiment completed in 0:02:36.844420 seconds
#+END_SRC

This will print a lot of output, and if everything is correct it will create  matrices (3 for 
training, 3 for testing) in =triage/matrices=, every matrix will be
represented by two files, one with the metadata  of the matrix (a
=yaml= file) and the actual matrix (the =csv= file). 

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results raw drawer
ls /triage/output/matrices | awk -F . '{print $NF}' | sort | uniq -c
#+END_SRC

#+RESULTS:
:RESULTS:
      6 csv
      6 yaml
:END:

=Triage= also will store 18 trained models in =triage/trained_models=:

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results raw drawer
ls /triage/output/trained_models | wc -l
#+END_SRC

#+RESULTS:
:RESULTS:
18
:END:

And it will populate the =results= schema in the database, as
commented above, we will get =6= /model groups/:

#+BEGIN_SRC sql
select model_group_id, model_type, model_parameters from results.model_groups;
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                           | model_parameters                           |
|--------------+-------------------------------------+-------------------------------------------|
|            1 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": 1}         |
|            2 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": "sqrt"}    |
|            3 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": null}      |
|            4 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": 1}      |
|            5 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": "sqrt"} |
|            6 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": null}   |
:END:

And =18= /Models/:


#+BEGIN_SRC sql 
select
model_group_id, model_id, train_end_time
from results.models
order by model_group_id, train_end_time asc
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_id | train_end_time        |
|--------------+---------+---------------------|
|            1 |       1 | 2016-01-01 00:00:00 |
|            1 |       7 | 2017-01-01 00:00:00 |
|            1 |      13 | 2018-01-01 00:00:00 |
|            2 |       2 | 2016-01-01 00:00:00 |
|            2 |       8 | 2017-01-01 00:00:00 |
|            2 |      14 | 2018-01-01 00:00:00 |
|            3 |       3 | 2016-01-01 00:00:00 |
|            3 |       9 | 2017-01-01 00:00:00 |
|            3 |      15 | 2018-01-01 00:00:00 |
|            4 |       4 | 2016-01-01 00:00:00 |
|            4 |      10 | 2017-01-01 00:00:00 |
|            4 |      16 | 2018-01-01 00:00:00 |
|            5 |       5 | 2016-01-01 00:00:00 |
|            5 |      11 | 2017-01-01 00:00:00 |
|            5 |      17 | 2018-01-01 00:00:00 |
|            6 |       6 | 2016-01-01 00:00:00 |
|            6 |      12 | 2017-01-01 00:00:00 |
|            6 |      18 | 2018-01-01 00:00:00 |
:END:

From that last query, you should note that the order in which =triage= train
the models is by block (=train_end_time=) from oldest to recent, and
from =model_group=, also in ascending order. It will not go to the
next block, until all the /models groups/ were trained.

You can check with which matrix the models where trained

#+NAME: train_info
#+BEGIN_SRC sql
select
model_id, model_group_id, train_end_time, 
model_hash,train_matrix_uuid
from results.models
order by model_group_id, train_end_time asc
#+End_SRC

#+RESULTS: train_info
:RESULTS:
| model_id | model_group_id | train_end_time        | model_hash                        | train_matrix_uuid                  |
|---------+--------------+---------------------+----------------------------------+----------------------------------|
|       1 |            1 | 2016-01-01 00:00:00 | 44a91980f60b1b1b46c3ca4f56407b43 | 80a88d6a30313393d8a821660208dbda |
|       7 |            1 | 2017-01-01 00:00:00 | 973b622e395d0773d8e1a7625820ac07 | df40aaf329dfdcd4e950f7e58218be39 |
|      13 |            1 | 2018-01-01 00:00:00 | a40dcde3da20123496718c646715f3ed | 19c5aaf3895d5d2f782ed734955b3ab6 |
|       2 |            2 | 2016-01-01 00:00:00 | f6b6ee09da6a601bc0df6bf3f6edd350 | 80a88d6a30313393d8a821660208dbda |
|       8 |            2 | 2017-01-01 00:00:00 | 0aaa6187ea359b1b4a12a14f1cf7bba4 | df40aaf329dfdcd4e950f7e58218be39 |
|      14 |            2 | 2018-01-01 00:00:00 | 7c6b26adfc026acacc65f34f41798f89 | 19c5aaf3895d5d2f782ed734955b3ab6 |
|       3 |            3 | 2016-01-01 00:00:00 | b919b106d2a728edfc248b356c2e6286 | 80a88d6a30313393d8a821660208dbda |
|       9 |            3 | 2017-01-01 00:00:00 | 8a66abb8785d790537e31cfda1da4c72 | df40aaf329dfdcd4e950f7e58218be39 |
|      15 |            3 | 2018-01-01 00:00:00 | 5c7e7cb491bd0494fe6a52c689022e75 | 19c5aaf3895d5d2f782ed734955b3ab6 |
|       4 |            4 | 2016-01-01 00:00:00 | 4b762fd73f7b8fec1426a6391a781800 | 80a88d6a30313393d8a821660208dbda |
|      10 |            4 | 2017-01-01 00:00:00 | d03cf44679a06d530784c20d0183b179 | df40aaf329dfdcd4e950f7e58218be39 |
|      16 |            4 | 2018-01-01 00:00:00 | 79a0ee40119d44b734a603d9a965339e | 19c5aaf3895d5d2f782ed734955b3ab6 |
|       5 |            5 | 2016-01-01 00:00:00 | 54866a5bcc0cf48cfa43e3876238d246 | 80a88d6a30313393d8a821660208dbda |
|      11 |            5 | 2017-01-01 00:00:00 | f6c11a74790e3ceaa94248ed61834f04 | df40aaf329dfdcd4e950f7e58218be39 |
|      17 |            5 | 2018-01-01 00:00:00 | 86e406c438ae80e68665aef2da068ff0 | 19c5aaf3895d5d2f782ed734955b3ab6 |
|       6 |            6 | 2016-01-01 00:00:00 | bbfc6be74ac7696af859c862092a0e00 | 80a88d6a30313393d8a821660208dbda |
|      12 |            6 | 2017-01-01 00:00:00 | a00fbade042d73a28f6ee60996650d32 | df40aaf329dfdcd4e950f7e58218be39 |
|      18 |            6 | 2018-01-01 00:00:00 | 68334513a72301b163927d1fa583f4e3 | 19c5aaf3895d5d2f782ed734955b3ab6 |
:END:

As expected, we have two models per model group. Each model was trained
with the matrix indicated in the column =train_matrix_uuid=. This =uuid=
also is the file name of the stored matrix. The model itself was
stored under the file named with the =model_hash=.

If you want to see in which matrix the model was /tested/ you need to
run the following query

#+NAME: test_info
#+BEGIN_SRC  sql
select distinct 
model_id, 
matrix_uuid -- the test matrix
from results.predictions 
order by model_id
#+END_SRC

#+RESULTS: test_info
:RESULTS:
| model_id | matrix_uuid                       |
|---------+----------------------------------|
|       1 | 72a15fec31a6263d65b05a93d3ca24cf |
|       2 | 72a15fec31a6263d65b05a93d3ca24cf |
|       3 | 72a15fec31a6263d65b05a93d3ca24cf |
|       4 | 72a15fec31a6263d65b05a93d3ca24cf |
|       5 | 72a15fec31a6263d65b05a93d3ca24cf |
|       6 | 72a15fec31a6263d65b05a93d3ca24cf |
|       7 | 9ac1d77c45f375666e9be686c88caef6 |
|       8 | 9ac1d77c45f375666e9be686c88caef6 |
|       9 | 9ac1d77c45f375666e9be686c88caef6 |
|      10 | 9ac1d77c45f375666e9be686c88caef6 |
|      11 | 9ac1d77c45f375666e9be686c88caef6 |
|      12 | 9ac1d77c45f375666e9be686c88caef6 |
|      13 | fba584da9e52f3fa1c8407fa870e00b3 |
|      14 | fba584da9e52f3fa1c8407fa870e00b3 |
|      15 | fba584da9e52f3fa1c8407fa870e00b3 |
|      16 | fba584da9e52f3fa1c8407fa870e00b3 |
|      17 | fba584da9e52f3fa1c8407fa870e00b3 |
|      18 | fba584da9e52f3fa1c8407fa870e00b3 |
:END:

For example, the model =7= was stored as
=/triage/trained_models/= src_sh[:results raw  :export result :dir /docker:root@tutorial_bastion:/]{psql ${FOOD_DB_URL}  -t -P
 format=unaligned  -c 'select model_hash from results.models where model_id = 7'} 973b622e395d0773d8e1a7625820ac07
 df40aaf329dfdcd4e950f7e58218be39
using the standard serialization of sklearn models. This model was
trained with the matrix src_sh[:results raw  :export result :dir /docker:root@tutorial_bastion:/]{psql ${FOOD_DB_URL}  -t -P
 format=unaligned  -c 'select train_matrix_uuid from results.models where model_id = 7'} df40aaf329dfdcd4e950f7e58218be39
 973b622e395d0773d8e1a7625820ac07
 stored in the directory =/triage/matrices=.

Model =7= used the following hyperparameters:

#+BEGIN_SRC sql
select 
model_parameters 
from results.models 
where model_id = 7
#+END_SRC

#+RESULTS:
:RESULTS:
| model_parameters                   |
|-----------------------------------|
| {"max_depth": 1, "max_features": 1} |
:END:


We can visualize the model 

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_baseline.yaml show_model_plot --model 7
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/inspections_baseline.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Generating model image
Plotting tree number 0
Image stored in: 
['/triage/output/images/model_7_tree_0.svg']
#+End_src


[[./images/triage/model_7_tree_0.svg]]

This tree makes kind of sense, if the facility had more than 1.5
inspections related to food poisoning then it will fail the
inspection.

We can also get information about the /model group/

#+BEGIN_SRC sql
select 
model_group_id, model_type, model_config 
from 
results.model_groups 
where model_group_id = 1
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                           | model_config                                                                                      |
|--------------+-------------------------------------+--------------------------------------------------------------------------------------------------|
|            1 | sklearn.tree.DecisionTreeClassifier | {"purpose": "test", "experiment_type": "inspections prioritization", "label_definition": "failed"} |
:END:

The features used by that model are:

#+BEGIN_SRC sql
select 
unnest(feature_list) as features 
from 
results.model_groups 
where model_group_id = 1
#+END_SRC

#+RESULTS:
:RESULTS:
| features                                       |
|------------------------------------------------|
| inspections_entity_id_3month_type_canvass_sum        |
| inspections_entity_id_3month_type_complaint_sum      |
| inspections_entity_id_3month_type_consultation_sum   |
| inspections_entity_id_3month_type_food poisoning_sum |
| inspections_entity_id_3month_type_license_sum        |
| inspections_entity_id_3month_type__NULL_sum          |
| inspections_entity_id_3month_type_tag removal_sum    |
| inspections_entity_id_3month_type_task force_sum     |
| inspections_zip_code_3month_type_canvass_sum         |
| inspections_zip_code_3month_type_complaint_sum       |
| inspections_zip_code_3month_type_consultation_sum    |
| inspections_zip_code_3month_type_food poisoning_sum  |
| inspections_zip_code_3month_type_license_sum         |
| inspections_zip_code_3month_type__NULL_sum           |
| inspections_zip_code_3month_type_tag removal_sum     |
| inspections_zip_code_3month_type_task force_sum      |
:END:

Finally, the performance of the model =7=  are:

#+BEGIN_SRC sql
select
model_id,
metric || parameter as metric,
value,
num_labeled_examples, 
num_labeled_above_threshold,
num_positive_labels
from results.evaluations where model_id = 7
order by num_labeled_above_threshold asc,
metric || parameter
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | metric            |                 value | num_labeled_examples | num_labeled_above_threshold | num_positive_labels |
|---------+-------------------+-----------------------+--------------------+--------------------------+-------------------|
|       7 | precision@5_abs    |                   0.0 |               1173 |                        0 |               269 |
|       7 | recall@5_abs       |                   0.0 |               1173 |                        0 |               269 |
|       7 | precision@10_abs   |                   1.0 |               1173 |                        1 |               269 |
|       7 | recall@10_abs      | 0.0037174721189591076 |               1173 |                        1 |               269 |
|       7 | precision@25_abs   |                   0.4 |               1173 |                        5 |               269 |
|       7 | recall@25_abs      |  0.007434944237918215 |               1173 |                        5 |               269 |
|       7 | precision@5.0_pct  |                  0.34 |               1173 |                       50 |               269 |
|       7 | recall@5.0_pct     |   0.06319702602230483 |               1173 |                       50 |               269 |
|       7 | precision@10.0_pct |   0.28225806451612906 |               1173 |                      124 |               269 |
|       7 | recall@10.0_pct    |   0.13011152416356878 |               1173 |                      124 |               269 |
:END:

The columns  =num_labeled_examples, num_labeled_above_threshold,
num_positive_labels= represents the number of selected entities in the
prediction date which are labeled (there are =1173= entities in the
test matrix with a label (=1= or =0=)), the
number of entities with a positive label above the threshold
(e.g. there are =1= entity with a positive label =1= in the first 10
entities ordered by score) and the number of entities with positive labels among all the
labeled entities (=269= of =1173=) respectively. We can translate this
to english: in our case /label/ mean that between the /as of
date/ (=2017-01-01=) and one month later (until =2017-02-01=) there
were =1173= facilities *inspected* and =269= *failed* the inspection.

We could check that the numbers make sense, the number of /active
facilities/ at =2017-01-01= (the prediction date) is

#+BEGIN_SRC sql
select count(*)
from inspections.active_facilities
where '2017-01-01'::date <@ daterange(start_time, end_time)
#+END_SRC

#+RESULTS:
:RESULTS:
| count |
|-------|
| 19397 |
:END:

And this number matches with the predictions made by the model =7=, as expected.

#+BEGIN_SRC sql
select count(*) from results.predictions where model_id = 7 
#+END_SRC

#+RESULTS:
:RESULTS:
| count |
|-------|
| 19397 |
:END:

The number of /labels/ (=num_labeled_examples= = =1173=) is different,
 because only =1173= facilities were inspected in that time span. so,
 many of the facilities weren't inspected, then their label is =NULL=.


#+BEGIN_SRC sql
select count(distinct entity_id)
from inspections.failed
where outcome_date <@ '[2017-01-01, 2017-02-01)'::daterange
#+END_SRC

#+RESULTS:
:RESULTS:
| count |
|-------|
|  1316 |
:END:

Still far from the =1173=. Do you remember the /states/ table? Using
it to filter we got the correct number:

#+BEGIN_SRC sql
select outcome,count(distinct entity_id)
from inspections.failed
inner join (
      select entity_id
      from inspections.active_facilities
      where '2017-01-01'::date <@ daterange(start_time, end_time)
) as t
using (entity_id)
where outcome_date <@ '[2017-01-01, 2017-02-01)'::daterange
group by rollup(outcome)
#+END_SRC

#+RESULTS:
:RESULTS:
| outcome | count |
|---------+-------|
| f       |  1085 |
| t       |   269 |
| [NULL]  |  1173 |
:END:

Let's assume that this is our best model, Which is the list of 25 facilities to inspect?

#+BEGIN_SRC sql
select *
from results.predictions
where model_id = 7 
order by score desc
limit 25
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | entity_id | as_of_date            |              score | label_value | rank_abs | rank_pct | matrix_uuid                       | test_label_timespan |
|---------+----------+---------------------+--------------------+------------+---------+---------+----------------------------------+-------------------|
|       7 |        1 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |        4 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |        2 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |        6 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |        7 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |        8 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |        9 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |        5 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       11 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       13 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       14 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       15 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       16 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       19 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       20 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       21 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       10 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       23 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       25 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       27 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       28 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       29 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       30 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       31 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
|       7 |       32 | 2017-01-01 00:00:00 | 0.2501382998340402 | [NULL]     | [NULL]  | [NULL]  | 9ac1d77c45f375666e9be686c88caef6 | 1 mon             |
:END:

*** Defining a baseline 

As a second step, lets do a new experiment that defines our
/baseline/. In order to achive this, we will use a similar experiment
config file with the following changes:

#+BEGIN_EXAMPLE yaml
model_comment: 'inspections_baseline'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'inspections prioritization'
  purpose: 'baseline'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'

grid_config:
    'sklearn.dummy.DummyClassifier':
        strategy: [prior,uniform, most_frequent]

model_group_keys:
    - 'label_definition'
    - 'experiment_type'
    - 'purpose'
#+END_EXAMPLE

The complete file is in [[./triage/experiment_config/inspections_baseline.yaml][triage/experiment_config/inspections_baseline.yaml]]

If we execute this experiment, we will get 3 more model groups (one
for each strategy), and the corresponding 6 new models (2 per each
model group).

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_baseline.yaml validate
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/inspections_baseline.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Validating experiment's configuration
Experiment validation ran to completion with no errors

----TIME SPLIT SUMMARY----

Number of time splits: 3
Split index 0:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2015-12-01 00:00:00 (11 total)
            Testing as_of_time range: 2016-01-01 00:00:00 to 2016-01-01 00:00:00 (1 total)


Split index 1:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2016-12-01 00:00:00 (23 total)
            Testing as_of_time range: 2017-01-01 00:00:00 to 2017-01-01 00:00:00 (1 total)


Split index 2:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2017-12-01 00:00:00 (35 total)
            Testing as_of_time range: 2018-01-01 00:00:00 to 2018-01-01 00:00:00 (1 total)


For more detailed information on your time splits, inspect the experiment `split_definitions` property

           The experiment configuration doesn't contain any obvious errors.
           Any error that occurs from now on, possibly will be related to hit the maximum 
           number of columns allowed or collision in
           the column names, both due to PostgreSQL limitations.
    
The experiment looks in good shape. May the force be with you
#+END_SRC



You can execute the experiment as

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_baseline.yaml run
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/inspections_baseline.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Executing experiment
Done
Experiment completed in 0:00:40.563533 seconds
#+END_SRC



#+BEGIN_SRC sql
select model_group_id, model_type, model_parameters from results.model_groups;
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                           | model_parameters                           |
|--------------+-------------------------------------+-------------------------------------------|
|            1 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": 1}         |
|            2 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": "sqrt"}    |
|            3 | sklearn.tree.DecisionTreeClassifier | {"max_depth": 1, "max_features": null}      |
|            4 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": 1}      |
|            5 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": "sqrt"} |
|            6 | sklearn.tree.DecisionTreeClassifier | {"max_depth": null, "max_features": null}   |
|            7 | sklearn.dummy.DummyClassifier       | {"strategy": "prior"}                     |
|            8 | sklearn.dummy.DummyClassifier       | {"strategy": "uniform"}                   |
|            9 | sklearn.dummy.DummyClassifier       | {"strategy": "most_frequent"}              |
:END:

#+BEGIN_SRC sql

with baseline as (
select model_id, model_group_id
from results.models
where model_type ~ 'DummyClassifier'
)

select 
model_group_id, model_id, metric || parameter as metric, value
from results.evaluations
inner join baseline using(model_id)
where
metric || parameter = 'precision@25_abs'
order by metric || parameter, model_id
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_id | metric          |              value |
|--------------+---------+-----------------+--------------------|
|            7 |      19 | precision@25_abs | 0.3333333333333333 |
|            8 |      20 | precision@25_abs | 0.3333333333333333 |
|            9 |      21 | precision@25_abs | 0.3333333333333333 |
|            7 |      22 | precision@25_abs |                0.4 |
|            8 |      23 | precision@25_abs |                0.4 |
|            9 |      24 | precision@25_abs |                0.4 |
|            7 |      25 | precision@25_abs |                0.0 |
|            8 |      26 | precision@25_abs |                0.0 |
|            9 |      27 | precision@25_abs |                0.0 |
:END:


*** A more advanced experiment

Ok, let's add a more complete experiment. First the usual generalities.
Note that we change =experiment_type=

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_label_failed_01.yaml
config_version: 'v3'

model_comment: 'inspections'

user_metadata:
  label_definition: 'failed'
  experiment_type: 'inspections prioritization'
  purpose: 'development'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'
#+END_SRC

As before, we set the =triage= special tables that specifies /outcomes/ (that is call
=events_table=) and the one that specifies /states/. These are the
same, we didn't change anything.

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_label_failed_01.yaml
events_table: inspections.failed

state_config:
    table_name: 'inspections.active_facilities'
    state_filters:
       - 'active'
#+END_SRC

Neither to the temporal configuration:

#+BEGIN_SRC  yaml :tangle ../triage/experiment_config/inspections_label_failed_01.yaml
temporal_config:
    feature_start_time: '2010-01-04'
    feature_end_time: '2018-03-01'
    label_start_time: '2015-02-01'
    label_end_time: '2018-03-01'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '1month'  
    test_label_timespans: ['1y'] #
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'
#+END_SRC

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_label_failed_01.yaml show_temporal_blocks
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/inspections_label_failed_01.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Generating temporal blocks image
Image stored in:
/triage/output/images/inspections.svg
#+End_src


[[./images/triage/inspections.svg]]

The first big change is that we are adding 3 more /features groups/:
=inspections= (we already use this), =risks=, and =results=. Remember
that all of this referes to events in the past, i.e. /How many times the facility was marked with high risk in the previous 3 Months?/,
/Which is the average of failed inspections in the previous year?/

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_label_failed_01.yaml
feature_aggregations:
    -
        prefix: 'inspections'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'type'
                choice_query: 'select distinct type from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'

    -
        prefix: 'risks'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'risk'
                choice_query: 'select distinct risk from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'


    -
        prefix: 'results'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -
                column: 'result'
                choice_query: 'select distinct result from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'

#+END_Src

We want to use all the features groups
(=feature_group_definition=). The training will be made on matrices
with =all= the feature groups, then letting one feature group out, =leave-one-out=,
i.e. one model with =inspections= and =results=, another with
=inspections= and =risks= and another one with =results= and =risks, 
and then just trying models with =inspections= or =results= or =risks= exclusively.

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_label_failed_01.yaml
feature_group_definition:
   prefix: ['inspections', 'results', 'risks']

feature_group_strategies: ['all', 'leave-one-in', 'leave-one-out']
#+END_SRC

Finally, we will try a =RandomForestClassifier=


#+BEGIN_SRC yaml :tangle ../triage/experiment_config/inspections_label_failed_01.yaml
grid_config:
    'sklearn.ensemble.RandomForestClassifier':
        max_features: ['sqrt']
        criterion: ['gini']
        n_estimators: [1000]
        min_samples_leaf: [1]
        min_samples_split: [50]
        class_weight: ['balanced']


model_group_keys:
    - 'label_definition'
    - 'experiment_type'
    - 'purpose'

scoring:
    sort_seed: 1234
    metric_groups:
        -
            metrics: ['precision@', 'recall@']
            thresholds:
                percentiles: [1.0, 2.0, 5.0, 10.0, 25.0, 50.0, 75.0, 95.0, 100.0]
                top_n: [5, 10, 25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000]

#+END_SRC


#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_label_failed_01.yaml validate
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/inspections_label_failed_01.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Validating experiment's configuration
Experiment validation ran to completion with no errors

----TIME SPLIT SUMMARY----

Number of time splits: 2
Split index 0:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2016-01-01 00:00:00 (12 total)
            Testing as_of_time range: 2016-02-01 00:00:00 to 2016-02-01 00:00:00 (1 total)


Split index 1:
            Training as_of_time_range: 2015-02-01 00:00:00 to 2017-01-01 00:00:00 (24 total)
            Testing as_of_time range: 2017-02-01 00:00:00 to 2017-02-01 00:00:00 (1 total)


For more detailed information on your time splits, inspect the experiment `split_definitions` property

           The experiment configuration doesn't contain any obvious errors.
           Any error that occurs from now on, possibly will be related to hit the maximum 
           number of columns allowed or collision in
           the column names, both due to PostgreSQL limitations.
    
The experiment looks in good shape. May the force be with you
#+END_SRC

You can execute the experiment with

#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_label_failed_01.yaml run
#+END_SRC

This will take a looooong time to run.

Well, now we have a lot of models, How can you pick the best one for
you? We will show you that when we model the /Early Intervention System/.
