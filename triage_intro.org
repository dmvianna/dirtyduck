#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Center of Data Science for Public Policy
#+EMAIL: adolfo@uchicago.edu
#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sh   :results output drawer
#+PROPERTY: header-args:ipython   :session food_inspections
#+PROPERTY: header-args:python    :session food_inspections :results output drawer

* Triage

Predictive analytics projects require the coordination of many
different tasks, such as feature generation, classifier training,
evaluation, and list generation. These tasks are complicated in their
own right, but in addition have to be combined in different ways
throughout the course of the project.

=Triage= aims to provide interfaces to these different phases of a
project, such as an Experiment. Each phase is defined by configuration
specific to the needs of the project, and an arrangement of core data
science components that work together to produce the output of that
phase.

The domain  problems that =triage= had in mind are (1) early warning systems
/ early intervention systems and (2) prioritization of resources for
inspections.

=Triage= was created to facilitate the creation of supervised learning
models, in particular classification models with an strong temporal
component in the data.

The temporal component in the data set affects the modeling mainly in
two ways, first, you need to be very careful and avoid /leakage/ of
information in the data, and second, in the possible temporal drifting of the
data. =Triage= solves the first splitting the data in temporal blocks to be
used in the temporal crossvalidation and using those blocks for the
feature generation.

=Triage= uses the concept of /experiment/. An /experiment/ consists in a
series of steps which aim to generate a good model for predicting the
/label/ of an new instance of the data set. The steps are /feature generation/,
/label generation/, /model training/ and /model scoring/. In each of all
this steps, =triage= will take care of the temporal nuances of the data.

You need to specify (via a configuration file) how you want to time
split your data, which combination of machine learning algorithms and
their hyperparameters, which kind of features you want to generate and which
subset of those features you want to try in each model. So, the
experiment consists in try every combination of algorithm,
hyperparameters and feature subset, and evaluate their performance
using a set of metrics (also specified in the config file).

=Triage= will train one model for each block generated, so when the
experiment finishes you will have several models for each algorithm
and selection of hyperparameters. =Triage= calls this a =model_group=.


** Triage interface

=Triage= is very simple to use, but it contains a lot of complex
concepts that we will try to clarify in this section of the tutorial.

For running a =triage= experiment you need the following:

- =triage= installed in your environment (this is already install in the
  docker container). You can verify that =triage= is installed typying
  the following inside an =ipython= session in =bastion=:

#+BEGIN_SRC python
import triage
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

- A database connection

- An experiment config file, we will discuss this file at length in this
  section of the tutorial.

With this three components you can create your =experiment= object and
=run= it. In this tutorial we are providing a =docker= container that
executes =triage= experiments. You can run the container as follows:


#+BEGIN_SRC sh
./tutorial.sh triage
#+END_SRC

#+RESULTS:
:RESULTS:
Usage: triage_experiment [OPTIONS] COMMAND [ARGS]...

Options:
  --config_file PATH    Triage's experiment congiguration file  [required]
  --triage_db TEXT      DB URL, in the form of
                        'postgresql://user:password@host_db:host_port/db',
                        by
                        default it gets this from the environment
                        (TRIAGE_DB_URL)  [required]
  --debug / --no-debug  Do you want a verbose output?
  --help                Show this message and exit.

Commands:
  debug_features
  debug_temporal_blocks
  run
  validate
:END:



*** Configuration file

The experiment configuration file is used to create the experiment
object. Here, you will specify the temporal configuration, the
features to be generated, the labels to learn and the models that you
want to train in your data.

The configuration file is a =yaml= file with the following main sections:


- [[Temporal crossvalidation][temporal_config]] :: Temporal specification of the data, used for
     creating the blocks for temporal crossvalidation.

- =events_table= :: Table that contains the information about the labels
                    to be predicted

- [[Feature engineering][feature_generation]] :: Which spatio-temporal aggregations of the
     columns in the data set do you want to generate as features for
     the models?

- =state_config=  :: Specify which objects are in a given state in a
     particular interval of time, you can use this for filter which
     objects should be included in the training and prediction.

- =model_group_keys= :: How do you want to identify the =model_group= in
     the data

- =grid_config= :: Which combination of hyperparameters and algorithms
                   will be trained and evaluated in the data set?

- =scoring= :: Which metrics will be calculated?

*** The =Experiment= object

Everything revolves around the =experiment= object. You can validate
an =experiment= as follows:

#+BEGIN_SRC sh
./tutorial.sh triage --config_file /code/inspections_test.yaml validate
#+END_SRC

#+RESULTS:
:RESULTS:
Creating experiment object
Experiment loaded
Validating experiment's configuration
:END:

If you are ready for run it, you could start your experiment with:

#+BEGIN_SRC sh
./tutorial.sh triage --config_file /code/inspections_test.yaml run
#+END_SRC

** Temporal crossvalidation

The most acute problems are avoiding leakage and feature generation in
a temporal setting.

=Triage= uses the handy =timechop= library for this purpose. =Timechop=
will build ("chop") the data set in several temporal blocks. These
blocks will be used for creating the features and matrices for
the training and evaluation of the machine learning models.

Timechop requires the following parameters:

- =feature_start_time= :: data aggregated into features begins at this point
# earliest date included in features
- =feature_end_time= :: data aggregated into features is from before this
  point
# latest date included in features
- =label_start_time= :: data aggregated into labels begins at this point
# earliest event date included in any label (event date >= label_start_time)
- =label_end_time= :: data aggregated is from before this point
# event date < label_end_time to be included in any label
- =model_update_frequency= :: amount of time between train/test splits
# how frequently to retrain models (days, months, years)
- =training_as_of_date_frequencies= :: how much time between rows for a
  single entity in a training matrix
# list :: time between rows for same entity in train matrix
- =max_training_histories= :: the maximum amount of history for each
  entity to train on (early matrices may contain less than this time
  if it goes past label/feature start times)
# max length of time for labels included in a train matrix :: default = max (label_start_time to now)
- =training_label_timespans= :: how much time is covered by training
  labels (e.g., outcomes in the next 1 year? 3 days? 2 months?)
  (training prediction span)
# time period across which outcomes are determined in train matrices
- =test_as_of_date_frequencies= :: how much time between rows for a
  single entity in a test matrix
# time between rows for same entity in test matrix  :: inspections -  planning/scheduling frequency, eis = reviewing frequency (default = 1week)
- =test_durations= :: how far into the future should a model be used to
  make predictions (in the typical case of wanting a single prediction
  set immediately after model training, this should be set to 0 days)
(test span)
# length of time included in a test matrix (default = training_prediction_span) inspections = how far out are you scheduling for? eis = model_update_frequency
- =test_label_timespans= :: how much time is covered by test predictions
  (e.g., outcomes in the next 1 year? 3 days? 2 months?)
(test prediction span)
# time period across which outcomes are labeled in test matrices (default for eis = training_prediction_span, inspections = test_data_span)

You can use the docker image provider for getting an image about the
temporal blocks configuration, for example [[file:src/inspections_test.yaml][inspections_test.yaml]] has the following temporal structure:


#+BEGIN_SRC sh
./tutorial.sh triage --config_file /code/inspections_test.yaml show_temporal_blocks
#+END_SRC

#+RESULTS:
:RESULTS:
Creating experiment object
Experiment loaded
Generating temporal blocks image
Image stored in:
/code/inspections_test.png
:END:

[[./src/inspections_test.png]]

** Feature engineering

We will show how to create features, we will create a subset of the
=semantic.events= table: one facility (=entity_id = 9547=), only two
variables (=inspection_type,risk=), and
spatial and temporal dimensions for aggregation (=location, zip_code=, =date=).

For this end, we will create a new =schema= called =triage=

#+BEGIN_SRC sql :tangle ./src/create_triage_schema.sql
create schema if not exists triage;
#+END_SRC

#+RESULTS:

We need a table that represents if the facility is "active" in the
moment of the inspection (we don't want to predict a facility that is
not active). For testing purposes this table only include one
facility.



#+BEGIN_SRC sql :tangle ./src/create_triage_schema.sql
drop table if exists triage.active_facilities cascade;

create table triage.active_facilities as (
select entity_id, facility_type, location, date
    from (select distinct entity_id, facility_type, location from semantic.events where entity_id = 9547) a
    cross join (select distinct date as date from semantic.events) b
);

#+END_SRC

#+RESULTS:

#+BEGIN_SRC sql
select * from triage.active_facilities limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | facility_type | location                                           |       date |
|----------+--------------+----------------------------------------------------+------------|
|     9547 | restaurant   | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | 2016-11-10 |
|     9547 | restaurant   | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | 2015-05-05 |
|     9547 | restaurant   | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | 2013-05-06 |
|     9547 | restaurant   | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | 2017-05-26 |
|     9547 | restaurant   | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | 2011-10-05 |
:END:


Almost all the components of =triage= works with =SQL= tables stored  in
=PostgreSQL= (this is very important to remember), so, let's create our
test table with the =entity_id=  =9547=:

#+BEGIN_SRC  sql :tangle ./src/create_triage_schema.sql
drop table if exists triage.test;

create table triage.test as (
select
inspection, -- event
entity_id,
facility_type,
type as inspection_type, risk, -- variables
violations, -- json array of variables
date, location, zip_code -- spatio temporal dimensions
from semantic.events
where entity_id = '9547'
)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC sql
select entity_id, inspection_type, risk, date, zip_code from triage.test order by date desc  limit 5
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | inspection_type | risk   |       date | zip_code |
|----------+----------------+--------+------------+---------|
|     9547 | complaint      | medium | 2017-02-21 |   60621 |
|     9547 | complaint      | medium | 2017-02-10 |   60621 |
|     9547 | complaint      | medium | 2016-12-22 |   60621 |
|     9547 | complaint      | medium | 2016-08-26 |   60621 |
|     9547 | complaint      | medium | 2016-08-11 |   60621 |
:END:

=triage.test= contains two categorical variables (=inspection_type,risk=),
two differnent groups for aggregation (=location, zip_code=), and the date
when the inspection happened (=date=).

=Collate= is the python library that we will use (and =triage= also) for
controlling the generation of features (including the imputation rules
for each feature generated). =Collate= helps the modeler to
create features based on /spatio-temporal aggregations/ (which is what
we need in our modeling strategy based on *events*)

As a first feature we want to know in a given interval of time, given
one specific date, how many inspections and the flag resulted in "high risk"
after the previous inspection (the =risk= column), happened to the
facility and in the zip code in which the facility operates.

Let's try to construct that in =SQL=:

#+BEGIN_SRC sql
select entity_id, zip_code,
count(*) as inspections,
count(*) filter (where risk='high') as flagged_as_high_risk
from triage.test
group by grouping sets(entity_id, zip_code)
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | zip_code | inspections | flagged_as_high_risk |
|----------+---------+-------------+-------------------|
| 9547     | [NULL]  |          45 |                 0 |
| [NULL]   | 60621   |          45 |                 0 |
:END:

What about if we want to add the proportion of all the inspections
that resulted in be flagged as "high risk"?

#+BEGIN_SRC sql
select entity_id, zip_code,
count(*) as inspections,
count(*) filter (where risk='high') as flagged_as_high_risk,
avg((risk='high')::int) as proportion_of_flags_as_high_risk
from triage.test
group by grouping sets(entity_id, zip_code)
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | zip_code | inspections | flagged_as_high_risk | proportion_of_flags_as_high_risk |
|----------+---------+-------------+-------------------+-----------------------------|
| 9547     | [NULL]  |          45 |                 0 |      0.00000000000000000000 |
| [NULL]   | 60621   |          45 |                 0 |      0.00000000000000000000 |
:END:

But, what if we want to add also "medium" and "low" risk? And note
that we didn't add the temporal interval neither. You can see that the
event this simple set of features will require a very complex =SQL= to
be constructed. For this problem, =collate= has the =Categorical= object.

First note that the previous =SQL= query is composed by three parts:
- The filter ((=risk = 'high')::int=)
- The aggregation function (=avg()=)
- The name of the resulting transformation (=proportion_of_flags_as_high_risk=)

In collate, this aggregated column would be defined as:


#+BEGIN_SRC python
from  triage.component.collate import Categorical

risks = Categorical("risk", # the column
                    ["high", "medium", "low"], # compare to, i.e. 'risk = high', 'risk=low', etc
                    "avg", # aggregation function
                    {'coltype':'categorical', 'all': {'type': 'zero'}} # imputation rules
)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:



Note also that we specify the imputation strategy for how to handle
the null values in the resulting fields, in this example we use the
=mean= value.

We are not still ready for use this aggregated variable as a feature,
we need to take in account the spatio and temporal context.


#+BEGIN_SRC python
import sqlalchemy
from triage.component.collate import  SpacetimeAggregation

# We need a connection to the data base
db_url = f"postgresql://food_user:some_password@0.0.0.0:5434/food"
engine = sqlalchemy.create_engine(db_url, client_encoding='utf8')

db_connection = engine.connect()

st = SpacetimeAggregation([risks], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['entity_id','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1 year"], "zip_code": ["1 year"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.active_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='risks'
)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

The =SpacetimeAggregation= object is in charge of create the
agregations, another way of see it, is that it encapsulates the FROM section of the
query (=from_obj=) as well as the
GROUP BY columns (=groups=).

In the example above it will create features based on individual
restaurants (using =entity_id=) but also /contextual/ features related
to information about the zip code (=zip_code=) in which the facility is
operating.

The state table (=state_table=) specified here should contain the
comprehensive set of facilities and dates for which output should be
generated for them, regardless if they exist in the =from_obj=.

The attribute =intervals= specifies the date range partitioning for the
feature: it will create the aggregation over the past =1 year= for the
grouping given by the =entity_id= nad for the =zip_code=, and
additionally  will give an extra grouping statistic of two months for
the =zip_code=.

Before execute the queries, you could actually look them using the following

#+BEGIN_SRC python
import utils

utils.show_features_queries(st)
#+END_SRC


This will execute queries as the following for the group tables (like =test_risks_zip_code=):

#+BEGIN_EXAMPLE sql
...

SELECT zip_code, '2014-10-08'::date AS date,
avg((risk = 'high')::INT) FILTER (WHERE date >= '2014-10-08'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_high_avg",
avg((risk = 'medium')::INT) FILTER (WHERE date >= '2014-10-08'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_medium_avg",
avg((risk = 'low')::INT) FILTER (WHERE date >= '2014-10-08'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_low_avg"
FROM triage.test
WHERE date < '2014-10-08'AND date >= '2014-10-08'::date - greatest(interval '1 year') GROUP BY zip_code

...
#+END_EXAMPLE

and the next query for the =risks_aggregation= table:

#+BEGIN_EXAMPLE sql
CREATE TABLE "triage"."risks_aggregation" AS (
SELECT * FROM (
SELECT entity_id, zip_code, '2014-10-06'::date AS date
FROM triage.test
WHERE date < '2014-10-06'AND date >= '2014-10-06'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
UNION ALL
SELECT entity_id, zip_code, '2014-10-08'::date AS date
FROM triage.test
WHERE date < '2014-10-08'AND date >= '2014-10-08'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
UNION ALL
SELECT entity_id, zip_code, '2015-01-12'::date AS date
FROM triage.test
WHERE date < '2015-01-12'AND date >= '2015-01-12'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
UNION ALL
SELECT entity_id, zip_code, '2015-10-20'::date AS date
FROM triage.test
WHERE date < '2015-10-20'AND date >= '2015-10-20'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
UNION ALL
SELECT entity_id, zip_code, '2016-10-17'::date AS date
FROM triage.test
WHERE date < '2016-10-17'AND date >= '2016-10-17'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
) t1
LEFT JOIN "triage"."test_risks_entity_id" USING (entity_id, date) LEFT JOIN "triage"."test_risks_zip_code" USING (zip_code, date));
#+END_EXAMPLE

You can create the features tables executing the following:

#+BEGIN_SRC python
st.execute(db_connection) # with a SQLAlchemy engine object
#+END_SRC


#+RESULTS:
:RESULTS:
:END:

This will create 3 tables (One for the =entity_id=, one for =zip_code=
and one for the combination: =entity_id + zip_code=) and one extra
table for the imputated values.

The names of the generated tables are constructed as follows:

#+BEGIN_EXAMPLE
schema.prefix_{group, aggregation}
#+END_EXAMPLE

Inside each of those new tables, the column name will follow this
pattern:

#+BEGIN_EXAMPLE
prefix_group_interval_categorical_operation
#+END_EXAMPLE

For example the tables inside the triage schema are:

#+BEGIN_SRC sql
\dt triage.risks*
#+END_SRC

#+RESULTS:
:RESULTS:
| List of relations |                         |       |          |
|-------------------+-------------------------+-------+----------|
| Schema            | Name                    | Type  | Owner    |
| triage            | risks_aggregation        | table | food_user |
| triage            | risks_aggregation_imputed | table | food_user |
| triage            | risks_entity_id           | table | food_user |
| triage            | risks_zip_code            | table | food_user |
:END:

And inside =test_risk_aggregation= the columns are:

#+BEGIN_SRC sql
\d triage.risks_aggregation
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "triage.risks_aggregation"  |                   |           |
|----------------------------------+-------------------+-----------|
| Column                           | Type              | Modifiers |
| zip_code                          | character varying |           |
| date                             | date              |           |
| entity_id                         | bigint            |           |
| risks_entity_id_1 year_risk_high_avg   | numeric           |           |
| risks_entity_id_1 year_risk_medium_avg | numeric           |           |
| risks_entity_id_1 year_risk_low_avg    | numeric           |           |
| risks_zip_code_1 year_risk_high_avg    | numeric           |           |
| risks_zip_code_1 year_risk_medium_avg  | numeric           |           |
| risks_zip_code_1 year_risk_low_avg     | numeric           |           |
:END:


The =triage.risks_zip_code= table
have two feature columns for every zip code in our table =triage.test=,
looking at the total and average number of complaints in that
=zip_code= over the year prior and 2 months prior to the date in the =date= column.


#+BEGIN_SRC sql
select * from triage.risks_zip_code  order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | risks_zip_code_1 year_risk_high_avg | risks_zip_code_1 year_risk_medium_avg | risks_zip_code_1 year_risk_low_avg |
|---------+------------+-------------------------------+---------------------------------+------------------------------|
|   60621 | 2014-10-06 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2014-10-08 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2015-01-12 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2015-10-20 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2016-10-17 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
:END:

The table =triage.risks_entity_id= contains two feature columns for each
license that describe the total number of complaints
the past one year.

#+BEGIN_SRC sql
select * from triage.risks_entity_id  order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id |       date | risks_entity_id_1 year_risk_high_avg | risks_entity_id_1 year_risk_medium_avg | risks_entity_id_1 year_risk_low_avg |
|----------+------------+--------------------------------+----------------------------------+-------------------------------|
|     9547 | 2014-10-06 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
|     9547 | 2014-10-08 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
|     9547 | 2015-01-12 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
|     9547 | 2015-10-20 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
|     9547 | 2016-10-17 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
:END:

The =triage.risk_aggregation= table joins these results together to make
it easier to look at both zip_code and facility-level effects
for any given facility.

#+BEGIN_SRC sql
select * from triage.risks_aggregation order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | entity_id | risks_entity_id_1 year_risk_high_avg | risks_entity_id_1 year_risk_medium_avg | risks_entity_id_1 year_risk_low_avg | risks_zip_code_1 year_risk_high_avg | risks_zip_code_1 year_risk_medium_avg | risks_zip_code_1 year_risk_low_avg |
|---------+------------+----------+--------------------------------+----------------------------------+-------------------------------+-------------------------------+---------------------------------+------------------------------|
|   60621 | 2014-10-06 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2014-10-08 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2015-01-12 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2015-10-20 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2016-10-17 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
:END:


Finally, the =triage.risks_aggregated_imputed= table fills in null values using the
imputation rules specified in the =Categorical= constructor.

Let's try with =inspection_type=. We want the total of =canvass= and
=complaint= inspections.

#+BEGIN_SRC python

inspection_types = Categorical("inspection_type", # the column
                    ["canvass", "complaint"], # compare to, i.e. 'inspection_type = canvass', etc.
                    "sum", # aggregation function
                    {'coltype':'categorical', 'all': {'type': 'zero'}} # imputation rules
)

st = SpacetimeAggregation([inspection_types], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['entity_id','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.active_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='inspection_type'
)

st.execute(db_connection)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

This will create, as you probably guessed, four new tables:
=inspection_type_{entity_id, zip_code, aggregation, aggregation_imputed}=


Or you can mix the two in one step:

#+BEGIN_SRC python
st = SpacetimeAggregation([risks,inspection_types], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['entity_id','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.active_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='both'
)

st.execute(db_connection)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


Checking the columns inside =triage.both_aggregation= , you will note
that all the previous columns are there (except for the prefix):

#+BEGIN_SRC sql
\d triage.both_aggregation
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "triage.both_aggregation"           |                   |           |
|------------------------------------------+-------------------+-----------|
| Column                                   | Type              | Modifiers |
| zip_code                                  | character varying |           |
| date                                     | date              |           |
| entity_id                                 | bigint            |           |
| both_entity_id_1y_risk_high_avg                | numeric           |           |
| both_entity_id_1y_risk_medium_avg              | numeric           |           |
| both_entity_id_1y_risk_low_avg                 | numeric           |           |
| both_entity_id_1y_inspection_type_canvass_sum   | bigint            |           |
| both_entity_id_1y_inspection_type_complaint_sum | bigint            |           |
| both_zip_code_1y_risk_high_avg                 | numeric           |           |
| both_zip_code_1y_risk_medium_avg               | numeric           |           |
| both_zip_code_1y_risk_low_avg                  | numeric           |           |
| both_zip_code_1y_inspection_type_canvass_sum    | bigint            |           |
| both_zip_code_1y_inspection_type_complaint_sum  | bigint            |           |
:END:


Obviously you could want to create more complicated variables, for
example, we have a =json= column in our =semantic.events= table, as well
as a geographical column: =location=. Let's do create some features
using those.


*** Add number of violations by severity

Our =semantic.events= has a =json= column called =violations=. We will like
to have an idea of how many types of violations were inspected or at
least their severity. One way of do that is shown in the next =SQL= code:


#+BEGIN_SRC sql
select inspection, entity_id, zip_code, array_agg(obj ->> 'severity'),
count(*) filter (where obj ->> 'severity' = 'critical') as critical_violations,
count(*) filter (where obj ->> 'severity' = 'serious') as serious_violations,
count(*) filter (where obj ->> 'severity' = 'minor') as low_violations
from
(select inspection, entity_id, zip_code, jsonb_array_elements(violations::jsonb) as obj from triage.test)
as t1
group by inspection, entity_id, zip_code
limit 5
#+END_SRC

#+RESULTS:
:RESULTS:
| inspection | entity_id | zip_code | array_agg                        | critical_violations | serious_violations | low_violations |
|------------+----------+---------+---------------------------------+--------------------+-------------------+---------------|
|    1076221 |     9547 |   60621 | {minor,minor,minor}             |                  0 |                 0 |             3 |
|    1150580 |     9547 |   60621 | {minor,minor,minor,minor,minor} |                  0 |                 0 |             5 |
|    1150633 |     9547 |   60621 | {minor,minor,minor,minor}       |                  0 |                 0 |             4 |
|    1150878 |     9547 |   60621 | {minor,minor,minor,minor}       |                  0 |                 0 |             4 |
|    1150936 |     9547 |   60621 | {minor,minor,minor}             |                  0 |                 0 |             3 |
:END:

Basically, this code gives us the number of violations inspected by
severity. How about to get the total and proportion of violations in a
facility in the previous year and the average and standard deviation
for the zip code zone?  Note than in this case the variable is not
*categorical*, is a numeric one, fortunately =collate= also provides
support for numerical variables: the =Aggregate= object

#+BEGIN_SRC python
from  triage.component.collate import Aggregate


violations_sql = """
(
select inspection, entity_id, zip_code, date,
count(*) filter (where obj ->> 'severity' = 'critical') as critical_violations,
count(*) filter (where obj ->> 'severity' = 'serious') as serious_violations,
count(*) filter (where obj ->> 'severity' = 'minor') as low_violations
from
(select inspection, entity_id, zip_code, date, jsonb_array_elements(violations::jsonb) as obj from triage.test)
as t1
group by inspection, entity_id, zip_code, date
) as t
"""

critical_violations = Aggregate({'critical': 'critical_violations'}, ['sum', 'avg', 'stddev'], {'coltype':'aggregate', 'all': {'type': 'mean'}})
serious_violations = Aggregate({'serious': 'serious_violations'}, ['sum', 'avg', 'stddev'], {'coltype':'aggregate', 'all': {'type': 'mean'}})
low_violations = Aggregate({'low': 'low_violations'}, ['sum', 'avg', 'stddev'], {'coltype':'aggregate', 'all': {'type': 'mean'}})

st = SpacetimeAggregation([critical_violations, serious_violations, low_violations], # The Categorical object
                          from_obj=violations_sql, # FROM
                          groups=['entity_id','zip_code', 'inspection'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"], "inspection": ["0d"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.active_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='violations'
)

st.execute(db_connection)

#+END_SRC

We can inspect the generated =SQL=:


#+BEGIN_EXAMPLE sql
...

SELECT entity_id, '2014-10-06'::date AS date,
sum(critical_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_critical_sum,
avg(critical_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_critical_avg,
stddev(critical_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_critical_stddev,
sum(serious_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_serious_sum,
avg(serious_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_serious_avg,
stddev(serious_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_serious_stddev,
sum(low_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_low_sum,
 avg(low_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_low_avg,
stddev(low_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_low_stddev
FROM
(
select inspection, entity_id, zip_code, date,
count(*) filter (where obj ->> 'severity' = 'critical') as critical_violations,
count(*) filter (where obj ->> 'severity' = 'serious') as serious_violations,
count(*) filter (where obj ->> 'severity' = 'minor') as low_violations
from
(select inspection, entity_id, zip_code, date, jsonb_array_elements(violations::jsonb) as obj from triage.test)
as t1
group by inspection, entity_id, zip_code, date
) as t
WHERE date < '2014-10-06'AND date >= '2014-10-06'::date - greatest(interval '1y') GROUP BY entity_id

...

#+END_EXAMPLE

You should learn this: it is possible to pass any PostgreSQL =table=
object to =collate= (and henceforth to =triage=), even those which result from a query.


*** Add number of facilities by type in a radius: 1km

Another possible interesting feature is related to the spatial
surroundings of the facility. Is the facility near of schools or
day cares? or Is the facility located in zones with a lot of people
passing by?

In this feature the spatial aggregation is not some socio-political entity
as the zip code or the city limits, but the location
of the facility and the radius that defines the "neighborhood" of the facility.

The following query returns the number of facilities in a radius of 1
km around the facility in our example (=entity_id= 9547).

#+BEGIN_SRC sql
with inspected_facilities as (
    select distinct on (entity_id, location, facility_type) *
    from triage.active_facilities
),

facilities_nearby as (
   select
   a.entity_id, a.location, a.facility_type,
   b.entity_id as other_entity_id,
   b.facility_type as other_facility_type
   from inspected_facilities as a,
   lateral (
       select entity_id, facility_type
       from semantic.entities
       where ST_DWithin(location::geography, a.location::geography, 1000)  -- In meteres, i.e. 1000 m = 1 km
       and entity_id <> a.entity_id
   ) as b
)

select
entity_id,
other_facility_type, count(*) as total
from facilities_nearby
group by
entity_id, other_facility_type
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | other_facility_type               | total |
|----------+---------------------------------+-------|
|     9547 | school                          |    11 |
|     9547 | convenience store               |     2 |
|     9547 | furniture store                 |     1 |
|     9547 | long term care                  |     6 |
|     9547 | wholesale                       |     1 |
|     9547 | daycare (2 - 6 years)           |     3 |
|     9547 | golden diner                    |     1 |
|     9547 | private school                  |     2 |
|     9547 | grocery store                   |    40 |
|     9547 | restaurant                      |    37 |
|     9547 | children's services facility    |     1 |
|     9547 | daycare above and under 2 years |     4 |
|     9547 | gas station                     |     1 |
|     9547 | bakery                          |     1 |
|     9547 | unknown                         |    23 |
:END:

#+BEGIN_SRC python
facilities_nearby_sql = """
(
with inspected_facilities as (
    select distinct on (entity_id, location, facility_type) *
    from triage.active_facilities
),

facilities_nearby as (
   select
   a.entity_id, a.location, a.facility_type,
   b.entity_id as other_entity_id,
   b.facility_type as other_facility_type
   from inspected_facilities as a,
   lateral (
       select entity_id, facility_type
       from semantic.entities
       where ST_DWithin(location::geography, a.location::geography, 1000)  -- In meters i.e. 1000 m = 1 km
       and entity_id <> a.entity_id
   ) as b
)

select
entity_id,
other_facility_type, count(*) as total
from facilities_nearby
group by
entity_id, other_facility_type
)
"""
# TODO: Create the correct aggregate Do I need to pivot the table?
facilities = Aggregate({})

st = SpacetimeAggregation([critical_violations, serious_violations, low_violations], # The Categorical object
                          from_obj=violations_sql, # FROM
                          groups=['entity_id','zip_code', 'inspection'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"], "inspection": ["0d"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='violations'
)

st.execute(db_connection)

#+END_SRC


*** Add number of inspections by type in a radius and in an interval

Based in the previous example, we can modify a little the question an
ask: How many events happened nearby?

#+BEGIN_SRC sql
with inspected_facilities as (
    select distinct on (entity_id, location, facility_type) *
    from triage.active_facilities
),

inspections_nearby as (
   select
   a.entity_id,
   b.inspection, b.type, b.result, b.entity_id as other_entity_id,
   b.facility_type as other_facility_type, b.date
   from inspected_facilities as a,
   lateral (
       select inspection, type, result, entity_id, facility_type, date
       from semantic.events
       where ST_DWithin(location::geography, a.location::geography, 1000)  -- In meteres, i.e. 1000 m = 1 km
       and entity_id <> a.entity_id
   ) as b
)

select * from inspections_nearby limit 10;

-- select
-- entity_id, inspection, type as inpection
-- other_facility_type, count(*) as total
-- from inspections_nearby
-- group by
-- entity_id, other_facility_type
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | inspection | type      | result             | other_entity_id | other_facility_type |       date |
|----------+------------+-----------+--------------------+---------------+-------------------+------------|
|     9547 |     920206 | complaint | pass               |          1811 | grocery store     | 2012-02-24 |
|     9547 |     545345 | complaint | fail               |          1811 | grocery store     | 2011-05-11 |
|     9547 |     670596 | complaint | fail               |          1811 | grocery store     | 2012-02-17 |
|     9547 |     545365 | complaint | pass               |          1811 | grocery store     | 2011-05-19 |
|     9547 |    1955217 | canvass   | pass               |          1811 | grocery store     | 2016-09-08 |
|     9547 |    2081502 | complaint | pass               |          1811 | grocery store     | 2017-09-05 |
|     9547 |     545517 | complaint | pass               |          1811 | grocery store     | 2011-09-08 |
|     9547 |    1386161 | complaint | fail               |          1811 | grocery store     | 2015-05-27 |
|     9547 |    1386181 | complaint | pass w/ conditions |          1811 | grocery store     | 2015-06-04 |
|     9547 |     531602 | canvass   | pass               |          2111 | long term care    | 2011-10-06 |
:END:




QUESTION: Is this the correct SQL? (at least is fast)
IDEA: We could precalculate the distances? And from that filter by date?

#+BEGIN_SRC sql
with inspected_same_day as (
 select
   a.inspection, a.entity_id, a.location, a.facility_type, a.date,
   b.inspection as other_inspection, b.facility_type as other_facility_type, b.location as other_location
   from triage.test as a,
   lateral (
      select inspection, entity_id, location, facility_type, date
      from semantic.events
      where inspection <> a.inspection
      and date = a.date
   ) as b
),

inspections_nearby as (
   select
   inspection, entity_id, location, facility_type,
   other_facility_type, date
   from inspected_same_day
   where
       ST_DWithin(location::geography, other_location::geography, 1000)
)

select
inspection, entity_id, location, facility_type,
other_facility_type, date, count(*)
from inspections_nearby
group by
inspection, entity_id, location, facility_type, other_facility_type, date
limit 10
#+END_SRC

#+RESULTS:
:RESULTS:
| inspection | entity_id | location                                           | facility_type | other_facility_type |       date | count |
|------------+----------+----------------------------------------------------+--------------+-------------------+------------+-------|
|    1150878 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | restaurant        | 2012-10-26 |     1 |
|    1150878 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | school            | 2012-10-26 |     1 |
|    1335951 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | restaurant        | 2013-06-06 |     1 |
|    1335951 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | school            | 2013-06-06 |     2 |
|    1353423 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | grocery store     | 2013-07-01 |     1 |
|    1360667 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | grocery store     | 2013-09-04 |     1 |
|    1591635 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | grocery store     | 2015-12-14 |     1 |
|    1657238 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | restaurant        | 2016-02-25 |     1 |
|     401429 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | gas station       | 2011-01-13 |     1 |
|     401429 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | wholesale         | 2011-01-13 |     1 |
:END:

#+BEGIN_SRC python
inspections_nearby_sql = """
with inspections_nearby as (
with inspected_same_day as (
 select
   a.inspection, a.entity_id, a.location, a.facility_type, a.date,
   b.inspection as other_inspection, b.facility_type as other_facility_type, b.location as other_location
   from triage.test as a,
   lateral (
      select inspection, entity_id, location, facility_type, date
      from semantic.events
      where inspection <> a.inspection
      and date = a.date
   ) as c
),

inspections_nearby as (
   select
   inspection, entity_id, location, facility_type,
   other_facility_type, date
   from inspected_same_day
   where
       ST_DWithin(location::geography, other_location::geography, 1000)
)

select
inspection, entity_id, location, facility_type,
other_facility_type, date, count(*)
from inspections_nearby
group by
inspection, entity_id, location, facility_type, other_facility_type, date
) as b
"""

# TODO: Create the correct aggregate Do I need to pivot the table?
facilities = Aggregate({})

st = SpacetimeAggregation([critical_violations, serious_violations, low_violations], # The Categorical object
                          from_obj=violations_sql, # FROM
                          groups=['entity_id','zip_code', 'inspection'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"], "inspection": ["0d"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='violations'
)

st.execute(db_connection)

#+END_SRC

** Machine lerarning governance: The =RESULTS= schema

While =triage= is executing the experiment, it will create a new schema,
called =results=. This schema has the goal of storing the output of the
models and describing the features, parameters and hyperparameters
used in their training.

The tables contained in =results= are:

#+BEGIN_SRC sql
\dt results.
#+END_SRC

#+RESULTS:
:RESULTS:
| List of relations |                       |       |          |
|-------------------+-----------------------+-------+----------|
| Schema            | Name                  | Type  | Owner    |
| results           | evaluations           | table | food_user |
| results           | experiments           | table | food_user |
| results           | feature_importances    | table | food_user |
| results           | individual_importances | table | food_user |
| results           | list_predictions       | table | food_user |
| results           | model_groups           | table | food_user |
| results           | models                | table | food_user |
| results           | predictions           | table | food_user |
:END:

=model_groups= stores the algorithm (=model_type=), the
hyperparameters (=model_parameters=) and the features shared by a
particular set of models. =models= contains data specific to a model of
the =model_group= (you can use =model_group_id= for linking the model to a
model group) this table also includes temporal information (like
=train_end_time=) and a reference to the  train matrix
(=train_matrix_uuid=). This *UUID* is important
since that is the name of the file in which the matrix is stored.

Lastly, =results.predictions= contains all the /scores/ generated by every
model for every entity. =results.evaluation= stores the value of all the
*metrics* for every model. These metrics were specified in the =scoring=
section in the config file.

** ▶ TODO Audition

** Post-modeling

As the name indicates, *postmodeling* occurs *after* you have modeled
(potentially) thousands of models (different hyperparameters, different
time windows, different algorithms, etc), and using =audition= you /pre/
selected a small number of models.

Now, with the *postmodeling* tools you will be able to select your final
model for using it in /production/.

Triage's postmodeling capabilities include:

- Show the score distribution
- Compare the list generated by a set of models
- Compare the feature importance between a set of models
- Diplay the probability calibration curves
- Error analysis using a decision treee trained in the errors of the model.
- Cross-tab analysis
- Bias analysis

If you want to see it in action go to [[file:inspections.org][Inspections modeling]] or to [[file:eis.org][EIS modeling]].
