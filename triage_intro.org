#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Center of Data Science for Public Policy
#+EMAIL: adolfo@uchicago.edu
#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sh   :results output drawer
#+PROPERTY: header-args:ipython   :session food_inspections


* Triage

Predictive analytics projects require the coordination of many
different tasks, such as feature generation, classifier training,
evaluation, and list generation. These tasks are complicated in their
own right, but in addition have to be combined in different ways
throughout the course of the project.

=Triage= aims to provide interfaces to these different phases of a
project, such as an Experiment. Each phase is defined by configuration
specific to the needs of the project, and an arrangement of core data
science components that work together to produce the output of that
phase.

The domain  problems that =triage= had in mind are (1) early warning systems
/ early intervention systems and (2) prioritization of resources for
inspections.

=Triage= was created to facilitate the creation of supervised learning
models, in particular classification models with an strong temporal
component in the data.

The temporal component in the data set affects the modeling mainly in
two ways, first, you need to be very careful and avoid /leakage/ of
information in the data, and second, in the possible temporal drifting of the
data. =Triage= solves the first splitting the data in temporal blocks to be
used in the temporal crossvalidation and using those blocks for the
feature generation.

=Triage= uses the concept of /experiment/. An /experiment/ consists in a
series of steps which aim to generate a good model for predicting the
/label/ of an new instance of the data set. The steps are /feature generation/,
/label generation/, /model training/ and /model scoring/. In each of all
this steps, =triage= will take care of the temporal nuances of the data.

You need to specify (via a configuration file) how you want to time
split your data, which combination of machine learning algorithms and
their hyperparameters, which kind of features you want to generate and which
subset of those features you want to try in each model. So, the
experiment consists in try every combination of algorithm,
hyperparameters and feature subset, and evaluate their performance
using a set of metrics (also specified in the config file).

=Triage= will train one model for each block generated, so when the
experiment finishes you will have several models for each algorithm
and selection of hyperparameters. =Triage= calls this a =model_group=.

** Triage interface

=Triage= is very simple to use, but it contains a lot of complex
concepts that we will try to clarify in this section of the tutorial.

For running a =triage= experiment you need the following:

- =triage= installed in your environment (this is already install in the
  docker container). You can verify that =triage= is installed (and
  its version) typying the following inside an =ipython= session inside =bastion=:

#+BEGIN_SRC ipython
import triage

triage.__version__
#+END_SRC

#+RESULTS:
:RESULTS:
'2.2.0'
:END:

- A database with (obviously) and two tables (at minimum): one that
  describes the /outcome/ of each event on the database and a table
  that contains the state of each entity.

- An /experiment config file/, this is where the magic happen. We will
  discuss this file at length in this section of the tutorial.

With this three components you can create your =experiment= object and
=run= it. In this tutorial we are providing a =docker= container that
executes =triage= experiments. You can run the container as follows:


#+BEGIN_SRC sh
./tutorial.sh triage --help
#+END_SRC

#+RESULTS:
:RESULTS:
Usage: triage_experiment [OPTIONS] COMMAND [ARGS]...

Options:
  --config_file PATH        Triage's experiment congiguration file name 
                            NOTE:
                            It's assumed that the file is located inside
                            triage/experiment_config)  [required]
  --triage_db TEXT          DB URL, in the form of
                            'postgresql://user:password@host_db:host_port/db',
                            by default it gets this from the environment
                            (TRIAGE_DB_URL)  [required]
  --replace / --no-replace  Triage will (or won't) replace all the matrices
                            and models
  --help                    Show this message and exit.

Commands:
  run
  show_feature_generators
  show_temporal_blocks
  validate
:END:

You already had the database (you were working on it the last two
sections of the tutorial) and the tutorial provide a recent container
with =triage= installed. So, here, like in a real project you just
need to worry about the /experiment's configuration file/. But before,
we need to setup two more Tables.

*** A tale of two Tables

The first thing that =triage= will do is split the time that the data
covers in blocks considering the time horizon for the /label/
(i.e. the thing that we want to predict: /Which facilities will fail an inspection in the following 3 months?/
In the case of *inspection prioritization* or /Would be my restaurant inspected in the following month?/ 
If you are working in a *early warning system* problem.) This time
horizon is calculated from a set of specific dates (=as_of_date= in
triage parlance) that divide the blocks in past (for training the
model) and future (for testing the model).

=Triage= will create those /labels/ using information about the /outcome/ of
the event, taking in account the temporal structure of the data. 
As an example of an /outcome/ consider this  if a inspection is
realized (the event) and the facility fails the inspection (outcome
/true/) or not (outcome /false/). 

So, for a given /as of date/, in our data, for each entity, =triage=
will ask: Are positive outcomes in
the future time horizon? If so, =triage= will generate a positive
/label/ for that specific entity on that /as of date/. Henceforth, we
need to create an outcomes Table.

The table that is needed describe the /states/ of each entity. 
The table  should have columns =entity_id=, =start__time, end_time= and =state=.
The states table allows us to only include rows in your matrices in a
specific state. The rationale of this comes from the need of only
predict for entities in a particular state: Do the restaurant still
open? Do the restaurant is new? etc.

In order of exemplify and explain the working of =triage=, we will
create a subset of the =semantic.events= : one facility (=entity_id= =9581=), only two
variables (=inspection_type, risk=), and
spatial and temporal dimensions for aggregation (=location=,
=zip_code= and =date=).

For this end, we will create a new =schema= called =triage=

#+BEGIN_SRC sql :tangle ./sql/create_testing_triage.sql
create schema if not exists triage;
#+END_SRC

Almost all the components of =triage= works with =SQL= tables stored  in
=PostgreSQL= (this is very important to remember), so, let's create our
test table with the =entity_id=  =9581=:

#+BEGIN_SRC  sql :tangle ./sql/create_testing_triage.sql
drop table if exists triage.test;

create table triage.test as (
select
inspection, -- event
entity_id,
facility_type,
type as inspection_type, risk, -- variables
violations, -- json array of variables
date, location, zip_code -- spatio temporal dimensions
from semantic.events
where entity_id = 9581
)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC sql
select 
entity_id, 
inspection_type, risk, 
date, 
zip_code 
from triage.test 
order by date desc
limit 5
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | inspection_type | risk   |       date | zip_code |
|----------+----------------+--------+------------+---------|
|     9581 | complaint      | medium | 2017-02-21 |   60621 |
|     9581 | complaint      | medium | 2017-02-10 |   60621 |
|     9581 | complaint      | medium | 2016-12-22 |   60621 |
|     9581 | complaint      | medium | 2016-08-26 |   60621 |
|     9581 | complaint      | medium | 2016-08-11 |   60621 |
:END:

=triage.test= contains two categorical variables (=inspection_type,risk=),
two differnent groups for aggregation (=location, zip_code=), and the date
when the inspection happened (=date=).

For this test, we will keep things simple and define the /outcome/ as
=TRUE= if the inspection got a result adverse and =FALSE= Otherwise.

#+BEGIN_SRC sql :tangle ./sql/create_testing_triage.sql
drop table if exists triage.outcomes_9581;

create table triage.outcomes_9581 as (
select 
entity_id, 
date as outcome_date, 
(result = 'fail') as outcome
from semantic.events
where entity_id = 9581
);

#+END_SRC

#+Results:

#+BEGIN_SRC sql
select * from triage.outcomes_9581 limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | outcome_date | outcome |
|----------+-------------+---------|
|     9581 |  2011-04-22 | f       |
|     9581 |  2010-02-24 | f       |
|     9581 |  2016-06-15 | f       |
|     9581 |  2016-02-17 | f       |
|     9581 |  2016-02-25 | f       |
:END:

#+BEGIN_SRC sql
select 
outcome, count(*) 
from triage.outcomes_9581
group by 1;
#+END_SRC

#+RESULTS:
:RESULTS:
| outcome | count |
|---------+-------|
| f       |    39 |
| t       |     6 |
:END:


For the time being, we will only consider one facility /state/: Is the
facility "active" or not? This /state/ represents if the facility is
on business at the moment of the inspection (we don't want to predict
a facility that is not active). 

#+BEGIN_SRC sql :tangle ./sql/create_testing_triage_schema.sql
drop table if exists triage.active_facilities_9581 cascade;

create table triage.active_facilities_9581 as (
    select 
    entity_id, facility_type, location, 
    start_time, 
    case
    when end_time is NULL
    then '2020-01-01'
    else end_time
    end as end_time,
    'active' as state 
    from semantic.entities
);

#+END_SRC

#+Results:

=Triage= doesn't support open date intervals, so we had to impute
=end_time= with the date '2020-01-01'

#+BEGIN_SRC sql
select * from triage.active_facilities_9581 limit 1;
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | facility_type | location                                           |  start_time |    end_time | state  |
|----------+--------------+----------------------------------------------------+------------+------------+--------|
|        1 | newsstand    | 0101000020E6100000B005FE6352EE55C09053AD5BF3ED4440 | 2013-06-06 | 2020-01-01 | active |
:END:



*** Experiment's configuration file

The /experiment configuration file/ is used to create the =experiment=
object. Here, you will specify the temporal configuration, the
features to be generated, the labels to learn and the models that you
want to train in your data.

The configuration file is a =yaml= file with the following main sections:

- [[Temporal crossvalidation][temporal_config]] :: Temporal specification of the data, used for
     creating the blocks for temporal crossvalidation.

- =events_table= :: Table that contains the information about the labels
                    to be predicted. This is the =outcomes= table that
                    we describe earlier.

- [[Feature engineering][feature_generation]] :: Which spatio-temporal aggregations of the
     columns in the data set do you want to generate as features for
     the models?

- =state_config=  :: Specify which objects are in a given state in a
     particular interval of time, you can use this for filter which
     objects should be included in the training and prediction. This
     is the =states= table described above.

- =model_group_keys= :: How do you want to identify the =model_group= in
     the database (so you can run analysis on them)

- =grid_config= :: Which combination of hyperparameters and algorithms
                   will be trained and evaluated in the data set?

- =scoring= :: Which metrics will be calculated?


Two of the more important sections (and the more confusing too) are
=temporal_config= and =feature_generation=. We will explain them at
detail in the next sections.

**** Temporal crossvalidation

The most acute problems are avoiding leakaging information
and feature generation in a temporal setting.

=Triage= uses the handy =timechop= library for this purpose. =Timechop=
will build ("chop") the data set in several temporal blocks. These
blocks will be used for creating the features and matrices for
the training and evaluation of the machine learning models.

Timechop has several parameters, first, you need to specify The 
 limits of your data:

- =feature_start_time= :: data aggregated into features begins at this
     point (earliest date included in features)
- =feature_end_time= :: data aggregated into features is from before this
  point (latest date included in features)
- =label_start_time= :: data aggregated into labels begins at this
     point (earliest event date included in any label (event date >= label_start_time)
- =label_end_time= :: data aggregated is from before this point (event
     date < label_end_time to be included in any label)  

Other parameters controls the /labels/' time horizon, you have two
'knobs', one for training and one for testing.

- =training_label_timespans= :: how much time is covered by training
     labels (e.g., outcomes in the next 1 year? 3 days? 2 months?)
     (training prediction span) 

- =test_label_timespans= :: how much time is covered by test
     prediction (e.g., outcomes in the next 1 year? 3 days? 2 months?)
     (test prediction span)

These parameters will be used, together with the /outcomes/ table to
generate the /labels/. In an *EIS* setting regularly both will have
the same value. For *inspections prioritization* this value is most of
the time equal to =test_durations= and to =model_update_frequency=.

- =model_update_frequency= :: amount of time between train/test splits
     (how frequently to retrain models)

- =test_durations= :: how far into the future should a model be used
     to make predictions (test span)
     *NOTE*: in the typical case of wanting a single
     prediction set immediately after model training, this should be
     set to 0 days

This last parameter is other that differes if the problem is an *EIS*
or an *inspections prioritization*. In the former is recommended to be
equal to =model_update_frequency=,  in the latter is determined by the
organizational process: /how far out are you scheduling for?/.

The equivalent of =test_durations= for the training matrices is =max_training_histories=

- =max_training_histories= :: the maximum amount of history for each
  entity to train on (early matrices may contain less than this time
  if it goes past label/feature start times)

Finally, we should specify how many rows per =entity_id= in the train
 and test matrix

- =training_as_of_date_frequencies= :: how much time between rows for a
  single entity in a training matrix (list time between rows for
  same entity in train matrix) 

- =test_as_of_date_frequencies= :: how much time between rows for a
  single entity in a test matrix (time between rows for same entity in test matrix)


The following images (We will show you how to generate them later)
shows the time blocks of several configurations. We will change one
parameter at the time so you could see how that affects the blocks.

***** ={feature, label}_{end, start}_Time=

The image below shows these ={feature, label}_start_time= equal, and the same for the
={feature, label}_end_time= ones. These parameters show in the image
as dashed vertical black lines. This setup would be our *base*
example.

The plot is divided in two horizontal lines ("Block 0" and "Block
1"). Each line is divided by vertical dashed lines, the grey ones are
the boundaries of the data for features and data for labels, and in
this image they coincide. The black dash lines represents the
beginning and the end of the test set. In the "Block 0" those lines
are =2017= and =2018=, in "Block 1" they are =2016= and =2017=.

The shaded areas (in this image there is just one per block, but you
will see another examples below) represents the span of all the /as of dates/
They start with the oldest /as of date/ and end in the latest. Each
line inside that area represents the span for the label
calculation. Those lines begin at the /as of date/. In each /as of
date/ all the entities will get calculated their features (to the
past) and the labels (to the future). So in the image, we will have
two sets of train/test, in the "Block 0" our entity =9587= will have
13 rows of features,  and 12 on "Block 1". The trainned models will
predict the label using the features calculated in that /as of date/
in the  test data set, the solitary line represents the label's time
horizon in testing.


#+NAME: fig:timechop_1
#+CAPTION: feature and label start, end time equal
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_1.svg]]

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '1y'  
#+END_Example

But they can be different (maybe you have more data for features that
data for labels)

#+NAME: fig:timechop_2
#+CAPTION: feature_start_time different different that label_start_time.
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_2.svg]]


#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2010-01-01'   # <------- The change happened here!
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '1y'  
#+END_EXAMPLE

***** =model_update_frequency= 
From our *base* =temporal_config= example ([[fig:timechop_1]]), we will
change how often we want a new model, so we need more train/test sets:

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '6month' # <------- The change happened here!
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '1y'  
#+END_Example

#+NAME: fig:timechop_3
#+CAPTION: A smaller model_update_frequency (from 1y to 6month) (The number of blocks grew)
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_3.svg]]


***** =max_training_histories=

With this parameter you could get a /growing window/ for training
(depicted in [[fig:timechop_4]]) or as in all the other examples,  
/fixed training windows/.

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month'

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  # <------- The change happened here!
#+END_Example


#+NAME: fig:timechop_4
#+CAPTION: The size of the block is bigger now
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_4.svg]]

***** =_as_of_date_frequencies= and =test_durations=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '3month' # <------- The change happened here!

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_5
#+CAPTION: More rows per entity in the training block
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_5.svg]]

Now, change =test_as_of_date_frequencies=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month' 

    test_durations: '0d'
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '3month'<------- The change happened here!

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_6
#+CAPTION: We should get more rows per entity in the test matrix, but that didn't happen. Why?
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_6.svg]]

Nothing change, that's because the test set doesn't have "space", that
is controlled by =test_durations=, let's move that to to =6month=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month' 

    test_durations: '6month' <------- The change happened here!
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_7
#+CAPTION: The test duration is bigger now, so we got 6 rows (since the "base" frequency is 1 month)
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_7.svg]]

So, now we will move both parameters: =test_durations=, =test_as_of_date_frequencies=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month' 

    test_durations: '6month' <------- The change happened here!
    test_label_timespans: ['1y'] 
    test_as_of_date_frequencies: '3month' <------- and also here!

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_8
#+CAPTION: With more room in testing, now test_as_of_date_frequencies has some effect.
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_8.svg]]

***** =-label_timespans=

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['1y']
    training_as_of_date_frequencies: '1month' 

    test_durations: '0d' 
    test_label_timespans: ['3month']  <------- The change happened here!
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_9
#+CAPTION: The label time horizon in testing is smaller
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_9.svg]]

#+BEGIN_EXAMPLE yaml
temporal_config:
    feature_start_time: '2014-01-01'
    feature_end_time: '2018-01-01'
    label_start_time: '2014-01-02'
    label_end_time: '2018-01-01'

    model_update_frequency: '1y' 
    training_label_timespans: ['3month'] <------- The change happened here!
    training_as_of_date_frequencies: '1month' 

    test_durations: '0d' 
    test_label_timespans: ['1y']  
    test_as_of_date_frequencies: '1month'

    max_training_histories: '10y'  
#+END_Example


#+NAME: fig:timechop_10
#+CAPTION: The label time horizon is smaller in trainning, also, now we have more room for more rows per entity.
#+ATTR_ORG :width 100 :height 100
#+ATTR_HTML :width 600 :height 400
#+ATTR_LATEX :width 400 :height 300
[[./images/timechop_10.svg]]


**** Feature engineering

We will show how to create features using the /experiments config
file/. =triage= for this end, uses =collate=. =Collate= is the python
library that controls the generation of features (including the imputation rules
for each feature generated). =Collate= helps the modeler to
create features based on /spatio-temporal aggregations/ (which is what
we need in our modeling strategy based on *events*)

As a first feature we want to know in a given interval of time, in
a given specific date (remember /as of date/), /how many Inspections
 do each facility had?/ and /how many flags resulted in "high risk"
after the last inspection?/ (the =risk= column), 
happened to that facility and the same questions but aggregated in the
zip code in which the facility operates. 

Let's try to construct that in =SQL=:

#+BEGIN_SRC sql
select entity_id, zip_code,
count(*) as inspections,
count(*) filter (where risk='high') as flagged_as_high_risk
from triage.test
group by grouping sets(entity_id, zip_code)
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | zip_code | inspections | flagged_as_high_risk |
|----------+---------+-------------+-------------------|
| 9581     | [NULL]  |          45 |                 0 |
| [NULL]   | 60621   |          45 |                 0 |
:END:

This query is making an /aggregation/.Note that the previous =SQL=
query is composed by four parts: 
  - The filter ((=risk = 'high')::int=)
  - The aggregation function (=count()=)
  - The name of the resulting transformation (=flagged_as_high_risk=)
  - The context in which it is aggregated (by =entity_id= and =zip_code=).

What about if we want to add the proportion of all the inspections
that resulted in be flagged as "high risk"?

#+BEGIN_SRC sql
select entity_id, zip_code,
count(*) as inspections,
count(*) filter (where risk='high') as flagged_as_high_risk,
avg((risk='high')::int) as proportion_of_flags_as_high_risk
from triage.test
group by grouping sets(entity_id, zip_code)
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | zip_code | inspections | flagged_as_high_risk | proportion_of_flags_as_high_risk |
|----------+---------+-------------+-------------------+-----------------------------|
| 9581     | [NULL]  |          45 |                 0 |      0.00000000000000000000 |
| [NULL]   | 60621   |          45 |                 0 |      0.00000000000000000000 |
:END:

But, what if we want to add also "medium" and "low" risk? And note
that we didn't add the temporal interval neither. You can see that the
event this simple set of features will require a very complex =SQL= to
be constructed.




** Machine learning governance: The =RESULTS= schema

While =triage= is executing the experiment, it will create a new schema,
called =results=. This schema has the goal of storing the output of the
models and describing the features, parameters and hyperparameters
used in their training.

The tables contained in =results= are:

#+BEGIN_SRC sql
\dt results.*
#+END_SRC

#+RESULTS:
:RESULTS:
| List of relations |                       |       |          |
|-------------------+-----------------------+-------+----------|
| Schema            | Name                  | Type  | Owner    |
| results           | evaluations           | table | food_user |
| results           | experiments           | table | food_user |
| results           | feature_importances    | table | food_user |
| results           | individual_importances | table | food_user |
| results           | list_predictions       | table | food_user |
| results           | model_groups           | table | food_user |
| results           | models                | table | food_user |
| results           | predictions           | table | food_user |
:END:

=model_groups= stores the algorithm (=model_type=), the
hyperparameters (=model_parameters=) and the features shared by a
particular set of models. =models= contains data specific to a model of
the =model_group= (you can use =model_group_id= for linking the model to a
model group) this table also includes temporal information (like
=train_end_time=) and a reference to the  train matrix
(=train_matrix_uuid=). This *UUID* is important
since that is the name of the file in which the matrix is stored.

Lastly, =results.predictions= contains all the /scores/ generated by every
model for every entity. =results.evaluation= stores the value of all the
*metrics* for every model. These metrics were specified in the =scoring=
section in the config file.

** Audition

*Audition* is a tool for helping you to select a subset of trained
classifiers from a triage experiment. Often, production-scale experiments
will come up with thousands of trained models, and sifting through all
of those results can be time-consuming even after calculating the
usual basic metrics like precision and recall.

You will be facing questions as:

- Which metrics matter most?
- Should you prioritize the best metric value over time or treat
  recent data as most important?
- Is low metric variance important?

The answers to questions like these may not be obvious up front. *Audition*
introduces a structured, semi-automated way of filtering models based
on what you consider important

** Post-modeling

As the name indicates, *postmodeling* occurs *after* you have modeled
(potentially) thousands of models (different hyperparameters, different
time windows, different algorithms, etc), and using =audition= you /pre/
selected a small number of models.

Now, with the *postmodeling* tools you will be able to select your final
model for using it in /production/.

Triage's postmodeling capabilities include:

- Show the score distribution
- Compare the list generated by a set of models
- Compare the feature importance between a set of models
- Diplay the probability calibration curves
- Error analysis using a decision treee trained in the errors of the model.
- Cross-tab analysis
- Bias analysis

If you want to see *Audition* and *Postmodeling* in action please refer
[[file:inspections.org][Inspections modeling]] or to [[file:eis.org][EIS modeling]] for practical examples.


* What's next?

We will begin with [[file:inspections.org][Inspections problem]], let's go for It

