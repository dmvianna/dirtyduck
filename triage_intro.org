#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Center of Data Science for Public Policy
#+EMAIL: adolfo@uchicago.edu
#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:shell     :results drawer
#+PROPERTY: header-args:ipython   :session food_inspections

* Introduction to the different tasks

   In this tutorial, we have two different goals: (1) *EIS* (Early Information System) and
   (2) to *prioritize inspections*. The entity of interest in both cases is the  /facility/.

   In the *EIS* task, as a facility owner or manager, we want to predict if
   the facility under our control is at /risk/ of been inspected in the
   next time period.

   In the *inspections prioritization* task, we want to generate a list of
   facilities which are /likely/ to have some *critical* or *serious*
   violation /given that/ they are inspected.


* â–¶ TODO Triage [2/5]
 - [ ] Describe triage
 - [ ] Describe the problem that triage tries to solve
 - [X] Describe Timechop
 - [X] Describe Collate
 - [ ] Describe the results schema

Predictive analytics projects require the coordination of many
different tasks, such as feature generation, classifier training,
evaluation, and list generation. These tasks are complicated in their
own right, but in addition have to be combined in different ways
throughout the course of the project.

Triage aims to provide interfaces to these different phases of a
project, such as an Experiment. Each phase is defined by configuration
specific to the needs of the project, and an arrangement of core data
science components that work together to produce the output of that
phase.


** The problem to solve

The most acute problems are avoiding leakage and feature generation in
a temporal setting

** Temporal crossvalidation

Avoid leakage ...

Enter =timechop=

Timechop requires the following parameters:

- =feature_start_time= - data aggregated into features begins at this point
# earliest date included in features
- =feature_end_time= - data aggregated into features is from before this
  point
# latest date included in features
- =label_start_time= - data aggregated into labels begins at this point
# earliest event date included in any label (event date >= label_start_time)
- =label_end_time= - data aggregated is from before this point
# event date < label_end_time to be included in any label
- =model_update_frequency= - amount of time between train/test splits
# how frequently to retrain models (days, months, years)
- =training_as_of_date_frequencies= - how much time between rows for a
  single entity in a training matrix
# list - time between rows for same entity in train matrix
- =max_training_histories= - the maximum amount of history for each
  entity to train on (early matrices may contain less than this time
  if it goes past label/feature start times)
# max length of time for labels included in a train matrix - default = max (label_start_time to now)
- =training_label_timespans= - how much time is covered by training
  labels (e.g., outcomes in the next 1 year? 3 days? 2 months?)
  (training prediction span)
# time period across which outcomes are determined in train matrices
- =test_as_of_date_frequencies= - how much time between rows for a
  single entity in a test matrix
# time between rows for same entity in test matrix  - inspections -  planning/scheduling frequency, eis = reviewing frequency (default = 1week)
- =test_durations= - how far into the future should a model be used to
  make predictions (in the typical case of wanting a single prediction
  set immediately after model training, this should be set to 0 days)
(test span)
# length of time included in a test matrix (default = training_prediction_span) inspections = how far out are you scheduling for? eis = model_update_frequency
- =test_label_timespans= - how much time is covered by test predictions
  (e.g., outcomes in the next 1 year? 3 days? 2 months?)
(test prediction span)
# time period across which outcomes are labeled in test matrices (default for eis = training_prediction_span, inspections = test_data_span)

In the particular case of *inspections*,

- =test_as_of_date_frequencies= is planning/scheduling frequency
- =test_durations= is how far out are you scheduling for?
- =test_label_timespan= is equal to =test_durations=

/What are you inspecting?/ (people, places, other)
/How far do you want to predict?/ (e.g. 1 mo, 6mo, 12 mo, etc)
/How often do you want to update the list?/ (e.g. 1 mo, 6mo, 12 mo, etc)


#+BEGIN_EXAMPLE
    model_update_frequency='1year',
    training_label_timespans='3month',
    training_as_of_date_frequencies='3month',
    max_training_histories='2year',

    test_durations='3month',
    test_label_timespans='3month',
    test_as_of_date_frequencies='1month'
#+END_EXAMPLE


#+BEGIN_SRC python
import utils

utils.show_timechop(utils.chopper)
#+END_SRC

#+RESULTS:
: None


With that configuration our time splits looks like:

[[file:timechop.png][Timechop blocks for inspection]]



** Feature engineering

We will show how to create features, we will use the same subset (one
facility) and only one variable, and some spatial and temporal
dimensions.

For this, we will create a new =schema=

#+BEGIN_SRC sql
create schema if not exists triage;
#+END_SRC

#+RESULTS:

We need a table that represents if the facility is "active" in the
moment of the inspection (we don't want to predict a facility that is
not active). We don't want to complicate the calculation here, so, we
will assume that all the facilities are active in *every inspection*


#+BEGIN_SRC sql
drop table if exists triage.all_facilities cascade;

create table triage.all_facilities as (
    select license_num, date
    from (select distinct license_num from semantic.events) a
    cross join (select distinct date as date from semantic.events) b
-- select
-- distinct license_num,
-- 'active'::text as state,
-- min(date) as start,
-- max(date) as end
-- from semantic.events
-- group by license_num
) ;


create index on triage.all_facilities(license_num, date);
#+END_SRC

#+RESULTS:


#+BEGIN_SRC sql
select * from triage.all_facilities limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num |       date |
|------------+------------|
|    2506828 | 2016-11-10 |
|    2506828 | 2015-05-05 |
|    2506828 | 2013-05-06 |
|    2506828 | 2015-12-24 |
|    2506828 | 2017-05-26 |
:END:

#+BEGIN_SRC sql
select count(*) from triage.all_facilities
#+END_SRC

#+RESULTS:
:RESULTS:
|    count |
|----------|
| 57683187 |
:END:


Almost all the components of =triage= works with =SQL= tables stored  in
=PostgreSQL= (this is very important to remember), so, let's create our
test table with the =license_num= =1974745=:

#+BEGIN_SRC  sql
drop table if exists triage.test;

create table triage.test as (
select
license_num,  -- entity
type as inspection_type, risk, -- variables
violations, -- json array of variables
date, location, zip_code -- spatio temporal dimensions
from semantic.events
where license_num = 1974745
)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC sql
select license_num, inspection_type, risk, date, zip_code from triage.test order by date desc  limit 5
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num | inspection_type | risk |       date | zip_code |
|------------+----------------+------+------------+---------|
|    1974745 | canvass        | high | 2016-10-17 |   60612 |
|    1974745 | canvass        | high | 2015-10-20 |   60612 |
|    1974745 | complaint      | high | 2015-01-12 |   60612 |
|    1974745 | canvass        | high | 2014-10-08 |   60612 |
|    1974745 | canvass        | high | 2014-10-06 |   60612 |
:END:

=triage.test= contains two categorical variables (=inspection_type,risk=),
two differnent groups for aggregation (=location, zip_code=), and the date
when the inspection happened (=date=).

=Collate= is the python library that we will use (and =triage= also) for
controlling the generation of features (including the imputation rules
for each feature generated). =Collate= helps the modeler to
create features based on /spatio-temporal aggregations/ (which is what
we need in our modeling strategy based on *events*)

As a first feature we want to know in a given interval of time, given
one specific date, how many inspections and the flag resulted in "high risk"
after the previous inspection (the =risk= column), happened to the
facility and in the zip code in which the facility operates.

Let's try to construct that in =SQL=:

#+BEGIN_SRC sql
select license_num, zip_code,
count(*) as inspections,
count(*) filter (where risk='high') as flagged_as_high_risk
from triage.test
group by grouping sets(license_num, zip_code)
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num | zip_code | inspections | flagged_as_high_risk |
|------------+---------+-------------+-------------------|
| 1974745    | [NULL]  |          57 |                14 |
| [NULL]     | 60612   |          57 |                14 |
:END:

What about if we want to add the proportion of all the inspections
that resulted in be flagged as "high risk"?

#+BEGIN_SRC sql
select license_num, zip_code,
count(*) as inspections,
count(*) filter (where risk='high') as flagged_as_high_risk,
avg((risk='high')::int) as proportion_of_flags_as_high_risk
from triage.test
group by grouping sets(license_num, zip_code)
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num | zip_code | inspections | flagged_as_high_risk | proportion_of_flags_as_high_risk |
|------------+---------+-------------+-------------------+-----------------------------|
| 1974745    | [NULL]  |          57 |                14 |      0.24561403508771929825 |
| [NULL]     | 60612   |          57 |                14 |      0.24561403508771929825 |
:END:

But, what if we want to add also "medium" and "low" risk? And note
that we didn't add the temporal interval neither. You can see that the
event this simple set of features will require a very complex =SQL= to
be constructed. For this problem, =collate= has the =Categorical= object.

First note that the previous =SQL= query is composed by three parts:
- The filter ((=risk = 'high')::int=)
- The aggregation function (=avg()=)
- The name of the resulting transformation (=proportion_of_flags_as_high_risk=)

In collate, this aggregated column would be defined as:


#+BEGIN_SRC python
from  triage.component.collate import Categorical

risks = Categorical("risk", # the column
                    ["high", "medium", "low"], # compare to, i.e. 'risk = high', 'risk=low', etc
                    "avg", # aggregation function
                    {'coltype':'aggregate', 'all': {'type': 'mean'}} # imputation rules
)
#+END_SRC



Note also that we specify the imputation strategy for how to handle
the null values in the resulting fields, in this example we use the
=mean= value.

We are not still ready for use this aggregated variable as a feature,
we need to take in account the spatio and temporal context.


#+BEGIN_SRC python

import sqlalchemy
from triage.component.collate import  SpacetimeAggregation

# We need a connection to the data base
db_url = f"postgresql://food_user:some_password@0.0.0.0:5434/food"
engine = sqlalchemy.create_engine(db_url, client_encoding='utf8')

db_connection = engine.connect()


st = SpacetimeAggregation([risks], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['license_num','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"license_num": ["1 year"], "zip_code": ["1 year"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='license_num', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='test_risks'
)
#+END_SRC

The =SpacetimeAggregation= object is in charge of create the
agregations, another way of see it, is that it encapsulates the FROM section of the
query (=from_obj=) as well as the
GROUP BY columns (=groups=).

In the example above it will create features based on individual
restaurants (using =license_num=) but also /contextual/ features related
to information about the zip code (=zip_code=) in which the facility is
operating.

The state table (=state_table=) specified here should contain the
comprehensive set of facilities and dates for which output should be
generated for them, regardless if they exist in the =from_obj=.

The attribute =intervals= specifies the date range partitioning for the
feature: it will create the aggregation over the past =1 year= for the
grouping given by the =license_num= nad for the =zip_code=, and
additionally  will give an extra grouping statistic of two months for
the =zip_code=.

Before execute the queries, you could actually look them using the following

#+BEGIN_SRC python
import utils

utils.show_features_queries(st)
#+END_SRC

This will execute queries as the following for the group tables (like =test_risks_zip_code=):

#+BEGIN_EXAMPLE sql

...

SELECT zip_code, '2016-10-17'::date AS date,
avg((risk = 'high')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_high_avg",
avg((risk = 'medium')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_medium_avg",
avg((risk = 'low')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_low_avg",
avg((risk = 'high')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '2 year') AS "test_risks_zip_code_2 year_risk_high_avg",
avg((risk = 'medium')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '2 year') AS "test_risks_zip_code_2 year_risk_medium_avg",
avg((risk = 'low')::INT) FILTER (WHERE date >= '2016-10-17'::date - interval '2 year') AS "test_risks_zip_code_2 year_risk_low_avg"
FROM triage.test
WHERE date < '2016-10-17'AND date >= '2016-10-17'::date - greatest(interval '1 year',interval '2 year') GROUP BY zip_code

...

#+END_EXAMPLE


and the next query for the =test_risks_aggregation= table:

#+BEGIN_EXAMPLE sql
CREATE TABLE "triage"."both_aggregation" AS (SELECT * FROM (SELECT license_num, zip_code, '2014-10-06'::date AS date
FROM triage.test
WHERE date < '2014-10-06'AND date >= '2014-10-06'::date - greatest(interval '1y') GROUP BY license_num, zip_code
UNION ALL
SELECT license_num, zip_code, '2014-10-08'::date AS date
FROM triage.test
WHERE date < '2014-10-08'AND date >= '2014-10-08'::date - greatest(interval '1y') GROUP BY license_num, zip_code
UNION ALL
SELECT license_num, zip_code, '2015-01-12'::date AS date
FROM triage.test
WHERE date < '2015-01-12'AND date >= '2015-01-12'::date - greatest(interval '1y') GROUP BY license_num, zip_code
UNION ALL
SELECT license_num, zip_code, '2015-10-20'::date AS date
FROM triage.test
WHERE date < '2015-10-20'AND date >= '2015-10-20'::date - greatest(interval '1y') GROUP BY license_num, zip_code
UNION ALL
SELECT license_num, zip_code, '2016-10-17'::date AS date
FROM triage.test
WHERE date < '2016-10-17'AND date >= '2016-10-17'::date - greatest(interval '1y') GROUP BY license_num, zip_code) t1
LEFT JOIN "triage"."both_license_num" USING (license_num, date)
LEFT JOIN "triage"."both_zip_code" USING (zip_code, date));
#+END_EXAMPLE

You can create the features tables executing the following:

#+BEGIN_SRC python
st.execute(db_connection) # with a SQLAlchemy engine object
#+END_SRC


#+RESULTS:
:RESULTS:
:END:

This will create 3 tables (One for the =license_num=, one for =zip_code=
and one for the combination: =license_num + zip_code=) and one extra
table for the imputated values.

The names of the generated tables are constructed as follows:

#+BEGIN_EXAMPLE
schema.prefix_{group, aggregation}
#+END_EXAMPLE

Inside each of those new tables, the column name will follow this
pattern:

#+BEGIN_EXAMPLE
prefix_group_interval_categorical_operation
#+END_EXAMPLE

For example the tables inside the triage schema are:

#+BEGIN_SRC sql
\dt triage.test_risks*
#+END_SRC

#+RESULTS:
:RESULTS:
| List of relations |                             |       |          |
|-------------------+-----------------------------+-------+----------|
| Schema            | Name                        | Type  | Owner    |
| triage            | test_risks_aggregation        | table | food_user |
| triage            | test_risks_aggregation_imputed | table | food_user |
| triage            | test_risks_license_num         | table | food_user |
| triage            | test_risks_zip_code            | table | food_user |
:END:

And inside =test_risk_aggregation= the columns are:

#+BEGIN_SRC sql
\d triage.test_risks_aggregation
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "triage.test_risks_aggregation"                 |                   |           |
|-----------------------------------------------------+-------------------+-----------|
| Column                                              | Type              | Modifiers |
| zip_code                                             | character varying |           |
| date                                                | date              |           |
| license_num                                          | numeric           |           |
| test_risks_license_num_1 year_inspection_type_canvass_sum   | bigint            |           |
| test_risks_license_num_1 year_inspection_type_complaint_sum | bigint            |           |
| test_risks_zip_code_1 year_inspection_type_canvass_sum      | bigint            |           |
| test_risks_zip_code_1 year_inspection_type_complaint_sum    | bigint            |           |
:END:


The =triage.test_risks_zip_code= table
have two feature columns for every zip code in our table =triage.test=,
looking at the total and average number of complaints in that
=zip_code= over the year prior and 2 months prior to the date in the =date= column.


#+BEGIN_SRC sql
select * from triage.test_risks_zip_code  order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | test_risks_zip_code_1 year_risk_high_avg | test_risks_zip_code_1 year_risk_medium_avg | test_risks_zip_code_1 year_risk_low_avg | test_risks_zip_code_2 year_risk_high_avg | test_risks_zip_code_2 year_risk_medium_avg | test_risks_zip_code_2 year_risk_low_avg |
|---------+------------+-----------------------------------+-------------------------------------+----------------------------------+-----------------------------------+-------------------------------------+----------------------------------|
|   60612 | 2014-10-06 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2014-10-08 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2015-01-12 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2015-10-20 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2016-10-17 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
:END:

The table =triage.test_risks_license_num= contains two feature columns for each
license that describe the total number of complaints
the past one year.

#+BEGIN_SRC sql
select * from triage.test_risks_license_num  order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| license_num |       date | test_risks_license_num_1 year_risk_high_avg | test_risks_license_num_1 year_risk_medium_avg | test_risks_license_num_1 year_risk_low_avg |
|------------+------------+--------------------------------------+----------------------------------------+-------------------------------------|
|    1974745 | 2014-10-06 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
|    1974745 | 2014-10-08 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
|    1974745 | 2015-01-12 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
|    1974745 | 2015-10-20 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
|    1974745 | 2016-10-17 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |
:END:

The =triage.test_aggregation= table joins these results together to make
it easier to look at both zip_code and facility-level effects
for any given facility.

#+BEGIN_SRC sql
select * from triage.test_risks_aggregation order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | license_num | test_risks_license_num_1 year_risk_high_avg | test_risks_license_num_1 year_risk_medium_avg | test_risks_license_num_1 year_risk_low_avg | test_risks_zip_code_1 year_risk_high_avg | test_risks_zip_code_1 year_risk_medium_avg | test_risks_zip_code_1 year_risk_low_avg | test_risks_zip_code_2 year_risk_high_avg | test_risks_zip_code_2 year_risk_medium_avg | test_risks_zip_code_2 year_risk_low_avg |
|---------+------------+------------+--------------------------------------+----------------------------------------+-------------------------------------+-----------------------------------+-------------------------------------+----------------------------------+-----------------------------------+-------------------------------------+----------------------------------|
|   60612 | 2014-10-06 |    1974745 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2014-10-08 |    1974745 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2015-01-12 |    1974745 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2015-10-20 |    1974745 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
|   60612 | 2016-10-17 |    1974745 |               1.00000000000000000000 |                 0.00000000000000000000 |              0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |            1.00000000000000000000 |              0.00000000000000000000 |           0.00000000000000000000 |
:END:


Finally, the =triage.test_risks_aggregated_imputed= table fills in null values using the
imputation rules specified in the =Categorical= constructor.

#+BEGIN_SRC python

inspection_types = Categorical("inspection_type", # the column
                    ["canvass", "complaint"], # compare to, i.e. 'inspection_type = canvass', etc.
                    "sum", # aggregation function
                    {'coltype':'aggregate', 'all': {'type': 'mean'}} # imputation rules
)

st = SpacetimeAggregation([inspection_types], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['license_num','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"license_num": ["1y"], "zip_code": ["1y"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='license_num', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='inspection_type'
)

st.execute(db_connection)
#+END_SRC

This will create, as you probbaly guessed, four new tables:
=inspection_type_{license_num, zip_code, aggregatio, aggregation_imputed}=


Or you can mix the two in one step:

#+BEGIN_SRC python
st = SpacetimeAggregation([risks,inspection_types], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['license_num','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"license_num": ["1y"], "zip_code": ["1y"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='license_num', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='both'
)

#+END_SRC


Checking the columns inside =triage.both_aggregation= , you will note
that all the previous columns are there (except for the prefix):

#+BEGIN_SRC sql
\d triage.both_aggregation
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "triage.both_aggregation"             |                   |           |
|--------------------------------------------+-------------------+-----------|
| Column                                     | Type              | Modifiers |
| zip_code                                    | character varying |           |
| date                                       | date              |           |
| license_num                                 | numeric           |           |
| both_license_num_1y_risk_high_avg                | numeric           |           |
| both_license_num_1y_risk_medium_avg              | numeric           |           |
| both_license_num_1y_risk_low_avg                 | numeric           |           |
| both_license_num_1y_inspection_type_canvass_sum   | bigint            |           |
| both_license_num_1y_inspection_type_complaint_sum | bigint            |           |
| both_zip_code_1y_risk_high_avg                   | numeric           |           |
| both_zip_code_1y_risk_medium_avg                 | numeric           |           |
| both_zip_code_1y_risk_low_avg                    | numeric           |           |
| both_zip_code_1y_inspection_type_canvass_sum      | bigint            |           |
| both_zip_code_1y_inspection_type_complaint_sum    | bigint            |           |
:END:



Obviously you could want to create more complicated variables, for
example


*** Add number of violations by severity

#+BEGIN_SRC sql
select inspection, result, array_agg(obj ->> 'severity'),
count(*) filter (where obj ->> 'severity' = 'critical') as critical_violations,
count(*) filter (where obj ->> 'severity' = 'serious') as serious_violations,
count(*) filter (where obj ->> 'severity' = 'minor') as low_violations
from
(select inspection, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events limit 20)
as t1
group by inspection, result
#+END_SRC

#+RESULTS:
:RESULTS:
| inspection | result             | array_agg                                       | critical_violations | serious_violations | low_violations |
|------------+--------------------+------------------------------------------------+--------------------+-------------------+---------------|
|     100215 | pass w/ conditions | {serious}                                      |                  0 |                 1 |             0 |
|     100209 | fail               | {critical,minor,minor,minor,minor,minor,minor} |                  1 |                 0 |             6 |
|     104236 | fail               | {serious,serious,minor,minor}                  |                  0 |                 2 |             2 |
|     100214 | pass               | {serious}                                      |                  0 |                 1 |             0 |
|     100211 | fail               | {critical,serious}                             |                  1 |                 1 |             0 |
|     100212 | fail               | {critical,serious}                             |                  1 |                 1 |             0 |
|     100213 | pass               | {critical,serious}                             |                  1 |                 1 |             0 |
|     100210 | pass               | {NULL}                                         |                  0 |                 0 |             0 |
:END:


*** Add number of facilities by type in a radius: 1km
#+BEGIN_SRC sql
with facilities_nearby as (
   select a.license_num, a.location, a.facility_type, b.facility_type as other_facility_type
   from semantic.entities as a,
   lateral (
       select facility_type
       from semantic.entities
       where ST_DWithin(location::geography, a.location::geography, 1000)
       and license_num <> a.license_num
   ) as b
)

select
license_num, location,
facility_type, other_facility_type, count(*)
from facilities_nearby
group by
license_num, location, facility_type, other_facility_type;
#+END_SRC

#+RESULTS:


*** Add number of inspections by type in a radius and in an interval
#+BEGIN_SRC sql
with inspections_nearby as (
   select
   a.inspection, a.license_num, a.location, a.facility_type, a.result,
   b.inspection as other_inspection, b.license_num as other_license_num, b.facility_type as other_facility_type, b.result as other_result
   from semantic.events as a,
   lateral (
       select inspection, license_num, facility_type, result
       from semantic.events
       where
       ST_DWithin(location::geography, a.location::geography, 1000)
       and inspection <> a.inspection
   ) as b limit 100
)

select
inspection, license_num, location, facility_type, result,
other_result, other_facility_type, count(*)
from inspections_nearby
group by
inspection, license_num, location, facility_type, result, other_result, other_facility_type;

#+END_SRC
