#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Adolfo De Unánue
#+EMAIL: adolfo@uchicago.edu
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:shell     :results drawer
#+PROPERTY: header-args:ipython   :session food_inspections

* Intro

The [[https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5][Chicago's Food Inspections data set]] is well know, and it has been
used in several examples around the web.




* What do you need for this tutorial?

[[http://www.docker.com][Docker]] and [[https://docs.docker.com/compose/][Docker Compose]] installed. That's it.
Use the links for installing instructions.

* ▶ TODO Description of the problem to solve
:PROPERTIES:
- Describe the EIS problem and the Inspections problem
- Describe the technical problems that could happen: temporal data,
  leaking, etc.
:END:

Actually, We want to solve two problems: an /Early intervention system/ (*EIS*)
and a /Inspection prioritization/.


We will use the /same/ data set, first, we will be the restaurant's
owner, and we want to know: /Will my restaurant be inspected in the/
/next X period of time?/ Where $X$ could be 1 month, 1 week, 1 year,
etc.

Knowing the answer to this question, allows you (as the restaurant's
owner) to be prepared and take the pertinent actions.


The second scenario, is the following:  you work for the Chicago's
government, and you try
to prioritize your resources (i.e. your inspection workforce), since
they are limited. So, you will use the data (the same data set,
remember) for answering the next
question: /Which X restaurants are likely to violate some rule in the
following Y period of time?/  In this case maybe you are interested not
in all the violations but in the more grave.

* Infrastructure

Besides data, in most of the data science projects you will need some
other tools, for example a place to store the data (a database
management system), a way
for putting your model to work (an API) and a interface for looking
the performace of your trained models (for this tutorial we are proposing [[https://github.com/dssg/tyra][tyra]])

We are proving a little script for managing all the infrastructure in
a (hopefully) transparent way.

#+BEGIN_SRC shell
./manage.sh
#+END_SRC

#+RESULTS:
:RESULTS:
Usage: ./manage.sh {start|stop|build|rebuild|run|logs|status}
:END:



We need to create the infrastructure so, =start= it

#+BEGIN_SRC shell
./manage.sh start
#+END_SRC

#+RESULTS:
:RESULTS:
Step 1/6 : FROM python:3.6
 ---> 41397f4f2887
Step 2/6 : RUN apt-get -y update &&     apt-get install unzip &&     wget https://github.com/dssg/tyra/archive/master.zip &&     unzip master.zip
 ---> Using cache
 ---> e1143f2a9899
Step 3/6 : WORKDIR /tyra-master
 ---> Using cache
 ---> 6af515bb7df4
Step 4/6 : ADD default_profile.yaml /tyra-master
 ---> 2b1d62cbcaeb
Removing intermediate container b4e6d66f09d6
Step 5/6 : RUN pip install -r requirements.txt
 ---> Running in 1f3e7aa3dc07
Collecting Flask==0.10.1 (from -r requirements.txt (line 1))
  Downloading Flask-0.10.1.tar.gz (544kB)
Collecting SQLAlchemy==1.0.14 (from -r requirements.txt (line 2))
  Downloading SQLAlchemy-1.0.14.tar.gz (4.8MB)
Collecting pandas==0.18.1 (from -r requirements.txt (line 3))
  Downloading pandas-0.18.1.tar.gz (7.3MB)
Collecting PyYAML==3.11 (from -r requirements.txt (line 4))
  Downloading PyYAML-3.11.zip (371kB)
Collecting psycopg2==2.5 (from -r requirements.txt (line 5))
  Downloading psycopg2-2.5.tar.gz (703kB)
Collecting testing.postgresql (from -r requirements.txt (line 6))
  Downloading testing.postgresql-1.3.0-py2.py3-none-any.whl
Collecting pytest==2.9.2 (from -r requirements.txt (line 7))
  Downloading pytest-2.9.2-py2.py3-none-any.whl (162kB)
Collecting numpy (from -r requirements.txt (line 8))
  Downloading numpy-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)
Collecting scipy (from -r requirements.txt (line 9))
  Downloading scipy-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (48.2MB)
Collecting scikit-learn (from -r requirements.txt (line 10))
  Downloading scikit_learn-0.18.2-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)
Collecting flask_sqlalchemy (from -r requirements.txt (line 11))
  Downloading Flask_SQLAlchemy-2.2-py2.py3-none-any.whl
Collecting flask-login (from -r requirements.txt (line 12))
  Downloading Flask_Login-0.4.0-py2.py3-none-any.whl
Collecting codecov (from -r requirements.txt (line 13))
  Downloading codecov-2.0.9-py2.py3-none-any.whl
Collecting pytest-cov (from -r requirements.txt (line 14))
  Downloading pytest_cov-2.5.1-py2.py3-none-any.whl
Collecting tox (from -r requirements.txt (line 15))
  Downloading tox-2.7.0-py2.py3-none-any.whl (49kB)
Collecting gunicorn (from -r requirements.txt (line 16))
  Downloading gunicorn-19.7.1-py2.py3-none-any.whl (111kB)
Collecting Werkzeug>=0.7 (from Flask==0.10.1->-r requirements.txt (line 1))
  Downloading Werkzeug-0.12.2-py2.py3-none-any.whl (312kB)
Collecting Jinja2>=2.4 (from Flask==0.10.1->-r requirements.txt (line 1))
  Downloading Jinja2-2.9.6-py2.py3-none-any.whl (340kB)
Collecting itsdangerous>=0.21 (from Flask==0.10.1->-r requirements.txt (line 1))
  Downloading itsdangerous-0.24.tar.gz (46kB)
Collecting python-dateutil>=2 (from pandas==0.18.1->-r requirements.txt (line 3))
  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)
Collecting pytz>=2011k (from pandas==0.18.1->-r requirements.txt (line 3))
  Downloading pytz-2017.2-py2.py3-none-any.whl (484kB)
Collecting pg8000>=1.10 (from testing.postgresql->-r requirements.txt (line 6))
  Downloading pg8000-1.10.6-py2.py3-none-any.whl
Collecting testing.common.database (from testing.postgresql->-r requirements.txt (line 6))
  Downloading testing.common.database-2.0.1-py2.py3-none-any.whl
Collecting py>=1.4.29 (from pytest==2.9.2->-r requirements.txt (line 7))
  Downloading py-1.4.34-py2.py3-none-any.whl (84kB)
Collecting coverage (from codecov->-r requirements.txt (line 13))
  Downloading coverage-4.4.1-cp36-cp36m-manylinux1_x86_64.whl (196kB)
Collecting requests>=2.7.9 (from codecov->-r requirements.txt (line 13))
  Downloading requests-2.18.3-py2.py3-none-any.whl (88kB)
Collecting virtualenv>=1.11.2; python_version != "3.2" (from tox->-r requirements.txt (line 15))
  Downloading virtualenv-15.1.0-py2.py3-none-any.whl (1.8MB)
Collecting pluggy<1.0,>=0.3.0 (from tox->-r requirements.txt (line 15))
  Downloading pluggy-0.4.0-py2.py3-none-any.whl
Collecting MarkupSafe>=0.23 (from Jinja2>=2.4->Flask==0.10.1->-r requirements.txt (line 1))
  Downloading MarkupSafe-1.0.tar.gz
Collecting six>=1.5 (from python-dateutil>=2->pandas==0.18.1->-r requirements.txt (line 3))
  Downloading six-1.10.0-py2.py3-none-any.whl
Collecting chardet<3.1.0,>=3.0.2 (from requests>=2.7.9->codecov->-r requirements.txt (line 13))
  Downloading chardet-3.0.4-py2.py3-none-any.whl (133kB)
Collecting idna<2.6,>=2.5 (from requests>=2.7.9->codecov->-r requirements.txt (line 13))
  Downloading idna-2.5-py2.py3-none-any.whl (55kB)
Collecting certifi>=2017.4.17 (from requests>=2.7.9->codecov->-r requirements.txt (line 13))
  Downloading certifi-2017.7.27.1-py2.py3-none-any.whl (349kB)
Collecting urllib3<1.23,>=1.21.1 (from requests>=2.7.9->codecov->-r requirements.txt (line 13))
  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)
Building wheels for collected packages: Flask, SQLAlchemy, pandas, PyYAML, psycopg2, itsdangerous, MarkupSafe
  Running setup.py bdist_wheel for Flask: started
  Running setup.py bdist_wheel for Flask: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/b6/09/65/5fcf16f74f334a215447c26769e291c41883862fe0dc7c1430
  Running setup.py bdist_wheel for SQLAlchemy: started
  Running setup.py bdist_wheel for SQLAlchemy: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/b6/33/08/eb7224f6053d6abd5bbdd41628505b5af881a6b7828060a6b3
  Running setup.py bdist_wheel for pandas: started
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: still running...
  Running setup.py bdist_wheel for pandas: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/9a/8c/95/ceb8f988caf19dd90c4c587eea0ee1665c3bb6af73b3ca8264
  Running setup.py bdist_wheel for PyYAML: started
  Running setup.py bdist_wheel for PyYAML: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/4a/bf/14/d79994d19a59d4f73efdafb8682961f582d45ed6b459420346
  Running setup.py bdist_wheel for psycopg2: started
  Running setup.py bdist_wheel for psycopg2: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/8a/cd/5c/07cb5d2af9c2272b98e721b16dd065254429b943ab408abc7f
  Running setup.py bdist_wheel for itsdangerous: started
  Running setup.py bdist_wheel for itsdangerous: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/fc/a8/66/24d655233c757e178d45dea2de22a04c6d92766abfb741129a
  Running setup.py bdist_wheel for MarkupSafe: started
  Running setup.py bdist_wheel for MarkupSafe: finished with status 'done'
  Stored in directory: /root/.cache/pip/wheels/88/a7/30/e39a54a87bcbe25308fa3ca64e8ddc75d9b3e5afa21ee32d57
Successfully built Flask SQLAlchemy pandas PyYAML psycopg2 itsdangerous MarkupSafe
Installing collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask, SQLAlchemy, six, python-dateutil, pytz, numpy, pandas, PyYAML, psycopg2, pg8000, testing.common.database, testing.postgresql, py, pytest, scipy, scikit-learn, flask-sqlalchemy, flask-login, coverage, chardet, idna, certifi, urllib3, requests, codecov, pytest-cov, virtualenv, pluggy, tox, gunicorn
Successfully installed Flask-0.10.1 Jinja2-2.9.6 MarkupSafe-1.0 PyYAML-3.11 SQLAlchemy-1.0.14 Werkzeug-0.12.2 certifi-2017.7.27.1 chardet-3.0.4 codecov-2.0.9 coverage-4.4.1 flask-login-0.4.0 flask-sqlalchemy-2.2 gunicorn-19.7.1 idna-2.5 itsdangerous-0.24 numpy-1.13.1 pandas-0.18.1 pg8000-1.10.6 pluggy-0.4.0 psycopg2-2.5 py-1.4.34 pytest-2.9.2 pytest-cov-2.5.1 python-dateutil-2.6.1 pytz-2017.2 requests-2.18.3 scikit-learn-0.18.2 scipy-0.19.1 six-1.10.0 testing.common.database-2.0.1 testing.postgresql-1.3.0 tox-2.7.0 urllib3-1.22 virtualenv-15.1.0
 ---> 5d98e6fb6417
Removing intermediate container 1f3e7aa3dc07
Step 6/6 : ENTRYPOINT python run_webapp.py
 ---> Running in 76b4117d23ef
 ---> 54beb6b5c802
Removing intermediate container 76b4117d23ef
Successfully built 54beb6b5c802
Successfully tagged tutorial/tyra:latest
:END:



This will take some minutes the first time.

You can check that everything is running smoothly with =status=

#+BEGIN_SRC shell
./manage.sh status
#+END_SRC

#+RESULTS:
:RESULTS:
        Name                       Command              State                           Ports
----------------------------------------------------------------------------------------------------------------------
food_db                 docker-entrypoint.sh postgres   Up      0.0.0.0:5434->5432/tcp
tutorial_api            python app.py                   Up      0.0.0.0:32770->5000/tcp
tutorial_reverseproxy   nginx -g daemon off;            Up      80/tcp, 0.0.0.0:8081->8081/tcp, 0.0.0.0:8090->8090/tcp
tutorial_tyra           python run_webapp.py            Up      0.0.0.0:5001->5001/tcp
:END:


You can type in your browser [[http://0.0.0.0:5001]] and you will see the
login page from *Tyra*.


Now the database is running, its named =food_db=, the single table in
there is called =inspections=

Let's check the =schema= of =trips= table using the following command in =psql=

#+BEGIN_SRC sql
\dS+ inspections
#+END_SRC

#+RESULTS:
| Table "public.inspections" |                   |           |          |              |             |
|----------------------------+-------------------+-----------+----------+--------------+-------------|
| Column                     | Type              | Modifiers | Storage  | Stats target | Description |
| inspection                 | character varying | not null  | extended |              |             |
| dba_name                    | character varying |           | extended |              |             |
| aka_name                    | character varying |           | extended |              |             |
| license_num                 | numeric           |           | main     |              |             |
| facility_type               | character varying |           | extended |              |             |
| risk                       | character varying |           | extended |              |             |
| address                    | character varying |           | extended |              |             |
| city                       | character varying |           | extended |              |             |
| state                      | character varying |           | extended |              |             |
| zip                        | character varying |           | extended |              |             |
| date                       | date              |           | plain    |              |             |
| type                       | character varying |           | extended |              |             |
| results                    | character varying |           | extended |              |             |
| violations                 | character varying |           | extended |              |             |
| latitude                   | numeric           |           | main     |              |             |
| longitude                  | numeric           |           | main     |              |             |
| location                   | character varying |           | extended |              |             |

* Data

** Downloading

#+BEGIN_SRC shell :dir data
  curl "https://data.cityofchicago.org/api/views/4ijn-s7e5/rows.csv?accessType=DOWNLOAD" > inspections.csv
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC shell :dir data
  wc -l inspections.csv
#+END_SRC

#+RESULTS:
:RESULTS:
374918 inspections.csv
:END:

Ok, the data is now in =/data=, we can check how many rows the dataset contains

** Uploading to our database
Assuming that you are already inside =bastion=, run the following


#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results raw drawer
psql ${FOOD_DB_URL} -c 'select count(*) from inspections'
#+END_SRC

#+RESULTS:
:RESULTS:
 count
-------
     0
(1 row)

:END:



#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results raw drawer
ls -la /data
#+END_SRC

#+RESULTS:
:RESULTS:
total 399976
drwxrwxr-x  2 1000 1000      4096 Jul 27 15:07 .
drwxr-xr-x 68 root root      4096 Aug  7 02:08 ..
-rw-rw-r--  1 1000 1000         0 Jun 27 03:48 .gitkeep
-rw-rw-r--  1 1000 1000 194520987 Jun 27 13:23 2013-08-Citi-Bike-trip-data.csv
-rw-rw-r--  1 1000 1000  32090869 Jan 18  2017 201308-citibike-tripdata.zip
-rw-rw-r--  1 1000 1000 182944074 Jul 27 15:39 inspections.csv
:END:

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/
psql ${FOOD_DB_URL} -c "\copy inspections FROM '/data/inspections.csv' WITH HEADER CSV"
#+END_SRC

#+RESULTS:
: COPY 152252

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results org drawer
psql ${FOOD_DB_URL} -c 'select * from inspections limit 1'
#+END_SRC

#+RESULTS:
:RESULTS:
 inspection |      dba_name      |      aka_name      | license_num | facility_type |     risk      |     address     |  city   | state |  zip  |    date    |         type          | results | violations |      latitude      |     longitude      |                 location
------------+--------------------+--------------------+-------------+---------------+---------------+-----------------+---------+-------+-------+------------+-----------------------+---------+------------+--------------------+--------------------+------------------------------------------
 2071411    | BULL DOG ALE HOUSE | BULL DOG ALE HOUSE |     2517130 | Restaurant    | Risk 1 (High) | 901 S STATE ST  | CHICAGO | IL    | 60605 | 2017-07-25 | License Re-Inspection | Pass    |            | 41.870502077951755 | -87.62734617773437 | (41.870502077951755, -87.62734617773437)
(1 row)

:END:

You could see the meaning of the columns [[https://data.cityofchicago.org/api/assets/BAD5301B-681A-4202-9D25-51B2CAE672FF?download=true][here]].

** ▶ TODO Transforming the data

For tackling a Machine Learning problem you need to identify the
*entities* of your problem domain, and if your problem involves time,
how those entities changes.

In this tutorial, we have two different goals: an *EIS* and an
*Inpections prioritization*, the entity in which we are interested in
both cases is the *restaurant*, ...

The *outcome* is what differ between those two projects. For *EIS* the
outcome is *inspected*, for *Inspections*, the outcome is *violation found*.

Let's see the data and try to see how it needs to be transformed.


Remember that the data that we have is one inspection per row.
We will check the result of the inspections:

 #+BEGIN_SRC sql :results table drawer
   select
   results, count(*)
   from
   inspections
   group by
   results;
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 | results              | count |
 |----------------------+-------|
 | Fail                 | 29508 |
 | Pass w/ Conditions   | 14412 |
 | Not Ready            |   773 |
 | No Entry             |  4185 |
 | Out of Business      | 13692 |
 | Business Not Located |    60 |
 | Pass                 | 89622 |
 :END:

We will map =Fail=, =Not Ready=, =No Entry= to a =True= (i.e. a violation was
discovered), we will remove =Out of Business= and =Business Not Located=
from the database, and all the other options (=Pass w/Condition= and
=Pass)=  will become =False= (i.e. no violation was discovered).

 #+BEGIN_SRC sql :tangle ./src/create_violations_table.sql
   drop table if exists violations;

   create table violations as
          select inspection::int  as entity_id,
                 date as outcome_date,
                 zip,
                 risk,
                 type as inspection_type,
                 facility_type,
                 license_num::varchar as license,
                 dba_name as business_name,
                 aka_name as aka,
                 results,
                 case
                      when left(results, 4) = 'Pass' then FALSE
                      else TRUE
                 end as outcome,
                 (regexp_matches(violation[1],'^(\d+)\.'))[1]::varchar as violation_type ,
                 violation[1] as violation_description,
                 violation[2] as violation_comment
          from
                        (
          select
                 inspection,
                 date, zip, risk, facility_type, dba_name, aka_name, results, license_num, type,
                 regexp_split_to_array(regexp_split_to_table(violations, '\| '),'- Comments') as violation
           from inspections
           where lower(results) !~ '.*business*.'
          ) b;
 #+END_SRC


 Now we will create a =violations= table, for this we will use the

 #+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results org drawer
   psql ${FOOD_DB_URL} < /code/create_violations_table.sql
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 DROP TABLE
 SELECT 562853
 :END:

 #+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results org drawer
   psql ${FOOD_DB_URL} -c 'select count(*) from violations'
 #+END_SRC

 #+RESULTS:
 :RESULTS:
  count
 --------
  562853
 (1 row)

 :END:


 #+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results org drawer
   psql ${FOOD_DB_URL} -c 'select * from violations limit 5'
 #+END_SRC

 #+RESULTS:
 :RESULTS:
  entity_id | outcome_date |  zip  |      risk       |    inspection_type    | facility_type | license |   business_name    |        aka         | results | outcome | violation_type |                                                            violation_description                                                            |                                                                           violation_comment
 -----------+--------------+-------+-----------------+-----------------------+---------------+---------+--------------------+--------------------+---------+---------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    2071410 | 2017-07-25   | 60605 | Risk 1 (High)   | License Re-Inspection | Restaurant    | 2517129 | BULL DOG ALE HOUSE | BULL DOG ALE HOUSE | Pass    | f       | 8              | 8. SANITIZING RINSE FOR EQUIPMENT AND UTENSILS:  CLEAN, PROPER TEMPERATURE, CONCENTRATION, EXPOSURE TIME                                    | : ABATED. DISH MACHINES SANITIZES AT 100PPM OF CHLORINE.
    2071410 | 2017-07-25   | 60605 | Risk 1 (High)   | License Re-Inspection | Restaurant    | 2517129 | BULL DOG ALE HOUSE | BULL DOG ALE HOUSE | Pass    | f       | 9              | 9. WATER SOURCE: SAFE, HOT & COLD UNDER CITY PRESSURE                                                                                       | : ABATED. HOT WATER WAS PROVIDED.
    2071410 | 2017-07-25   | 60605 | Risk 1 (High)   | License Re-Inspection | Restaurant    | 2517129 | BULL DOG ALE HOUSE | BULL DOG ALE HOUSE | Pass    | f       | 18             | 18. NO EVIDENCE OF RODENT OR INSECT OUTER OPENINGS PROTECTED/RODENT PROOFED, A WRITTEN LOG SHALL BE MAINTAINED AVAILABLE TO THE INSPECTORS  | : ABATED. DOOR IS RODENT/INSECT PROOFED.
    2071412 | 2017-07-25   | 60640 | Risk 2 (Medium) | License               | Grocery Store | 2542856 | WILSON GROCERY     | WILSON GROCERY     | Pass    | f       | 32             | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED                                                        | : MUST NOT USE TAPE AS A MEANS OF REPAIR ON THE EXTERIOR OF THE MEAT DISPLAY COOLER.
    2071412 | 2017-07-25   | 60640 | Risk 2 (Medium) | License               | Grocery Store | 2542856 | WILSON GROCERY     | WILSON GROCERY     | Pass    | f       | 34             | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED                                   | : FLOOR UNDER THE KITCHEN THREE COMPARTMENT SINK GREASE TRAP WITH EXCESSIVE GREASE. FLOOR OF THE WALK-IN COOLER WITH DIRT AND FOOD SPILLAGE. MUST CLEAN AND MAINTAIN.
 (5 rows)

 :END:

 Ok, everything seems correct. =:)=


* Using triage (finally)

With the data sitting in our database, we can start our analysis.

** ▶ TODO The experiment concept

** ▶ TODO Cross temporal validation and Timechop

/We need to add some images here/



** The =inspections-training.yaml= file
:PROPERTIES:
:header-args:yaml: :tangle ./src/inspections-training.yaml
:END:

This is the unique point of entry for using =triage=, basically in this
file, you will specify,  how you want to do the temporal
cross-validation, how to generate the labels, how to generate the
features, which models you want to run, and finally,  which are the
metrics you are interested.

You can check the final configuration in =./src/inspections-training.yaml=

Let's go by piece by piece


*** Experiment metadata

#+BEGIN_SRC yaml
# EXPERIMENT METADATA
# model_comment (optional) will end up in the model_comment column of the
# models table for each model created in this experiment
model_comment: 'test'
#+END_SRC

*** Time splitting

For this section we will need get some info about the time span of our
data,


#+BEGIN_SRC sql
select
min(date)::date as modeling_start_time,
max(date)::date as modeling_end_time
from inspections;
#+END_SRC

#+RESULTS:
| modeling_start_time | modeling_end_time |
|-------------------+-----------------|
|        2010-01-04 |      2017-07-25 |



#+BEGIN_SRC yaml
# TIME SPLITTING
# The time window to look at, and how to divide the window into
# train/test splits
temporal_config:
    beginning_of_time: '2010-01-04' # earliest date included in features
    modeling_start_time: '2016-01-04' # earliest date in any model
    modeling_end_time: '2017-07-25' # all dates in any model are < this date
    update_window: '3month' # how frequently to retrain models
    train_example_frequency: '1month' # time between rows for same entity in train matrix
    test_example_frequency: '1month' # time between rows for same entity in test matrix
    train_durations: ['6month'] # length of time included in a train matrix
    test_durations: ['1month'] # length of time included in a test matrix
    train_label_windows: ['1month'] # time period across which outcomes are labeled in train matrices
    test_label_windows: ['1month'] # time period across which outcomes are labeled in test matrices
#+END_SRC

*** Label generation

#+BEGIN_SRC yaml
# LABEL GENERATION
# Information needed to generate labels
#
# An events table is expected, with the columns:
#   entity_id - an identifier for which the labels are applied to
#   outcome_date - The date at which some outcome was known
#   outcome - A boolean outcome
# These are used to generate appropriate labels for each train/test split
events_table: 'violations'
#+END_SRC

*** Feature generation

#+BEGIN_SRC yaml
  # FEATURE GENERATION
  # The aggregate features to generate for each train/test split
  #
  # Implemented by wrapping collate: https://github.com/dssg/collate
  # Most terminology here is taken directly from collate
  #
  # Each entry describes a collate.SpacetimeAggregation object, and the
  # arguments needed to create it. Generally, each of these entries controls
  # the features from one source table, though in the case of multiple groups
  # may result in multiple output tables
  feature_aggregations:
      -
          # prefix given to the resultant tables
          prefix: 'violation_type'
          # from_obj is usually a source table but can be an expression, such as
          # a join (ie 'cool_stuff join other_stuff using (stuff_id)')
          from_obj: 'violations'
          # The date column to use for specifying which records to include
          # in temporal features. It is important that the column used specifies
          # the date at which the event is known about, which may be different
          # from the date the event happened.
          knowledge_date_column: 'outcome_date'

          # aggregates and categoricals define the actual features created. So
          # at least one is required
          #
          # Aggregates of numerical columns. Each quantity is a number of some
          # sort, and the list of metrics are applied to each quantity
          # aggregates:
          #     -
          #         quantity: 'homeless::INT'
          #         metrics:
          #             - 'count'
          #             - 'sum'
          #
          # Categorical features. The column given can be of any type, but the
          # choices must comparable to that type for equality within SQL
          # The result will be one feature for each choice/metric combination
          categoricals:
              -
                  column: 'violation_type'
                  choice_query: 'select distinct violation_type from violations'
                  metrics:
                      - 'count'
          # The time intervals over which to aggregate features
          intervals:
              - '1 week'
          # A list of different columns to separately group by
          groups:
              - 'entity_id'   ## This is the ID of the entity
#+END_SRC

*** Feature grouping

#+BEGIN_SRC yaml
  # FEATURE GROUPING
  # define how to group features and generate combinations
  # feature_group_definition allows you to create groups/subset of your features
  # by different criteria.
  # for instance, 'tables' allows you to send a list of collate feature tables
  # 'prefix' allows you to specify a list of feature name prefixes
  feature_group_definition:
      tables: ['violation_type_entity_id']

  # strategies for generating combinations of groups
  # available: all, leave-one-out, leave-one-in
  feature_group_strategies: ['all']
#+END_SRC

*** Model grouping

#+BEGIN_SRC yaml
  # MODEL GROUPING
  # Model groups are aimed at defining models which are equivalent across time splits.
  # By default, the classifier module name, hyperparameters, and feature names are used.
  #
  # model_group_keys defines a list of *additional* matrix metadata keys that
  # should be considered when creating a model group
  model_group_keys: []
  #    - 'train_duration'
  #    - 'train_label_window'
  #    - 'train_example_frequency'
#+END_SRC

*** Grid configuration
#+BEGIN_SRC yaml
  # GRID CONFIGURATION
  # The classifier/hyperparameter combinations that should be trained
  #
  # Each top-level key should be a class name, importable from triage. sklearn is
  # available, and if you have another classifier package you would like available,
  # contribute it to requirements.txt
  #
  # Each lower-level key is a hyperparameter name for the given classifier, and
  # each value is a list of potential values. All possible combinations of
  # classifiers and hyperparameters are trained.
  grid_config:
      'sklearn.ensemble.RandomForestClassifier':
          max_features: ['sqrt']
          criterion: ['gini', 'entropy']
          n_estimators: [100, 1000, 5000]
          min_samples_split: [10, 20, 50, 100]
          max_depth: [10, 20, 50, 100]
#+END_SRC

*** Model scoring

#+BEGIN_SRC yaml
# MODEL SCORING
# How each trained model is scored
#
# Each entry in 'metric_groups' needs a list of one of the metrics defined in
# triage.scoring.ModelScorer.available_metrics (contributions welcome!)
# Depending on the metric, either thresholds or parameters
#
# Parameters specify any hyperparameters needed. For most metrics,
# which are simply wrappers of sklearn functions, these
# are passed directly to sklearn.
#
# Thresholds are more specific: The list is subset and only the
# top percentile or top n entities are scored
#
# sort_seed, if passed, will seed the random number generator for each model's
# metric creation phase. This affects how entities with the same probabilities
# are sorted
scoring:
    metric_groups:
        -
            metrics: ['precision@', 'recall@', 'fpr@']
            thresholds:
                percentiles: [1.0, 2.0, 5.0, 10.0, 25.0]
                top_n: [25, 75, 150, 300, 500, 1000, 1500]

#+END_SRC


#+BEGIN_SRC ipython :tangle ./src/run.py
  import sqlalchemy
  import yaml

  from catwalk.storage import FSModelStorageEngine
  from triage.experiments import SingleThreadedExperiment

  with open('inspections-training.yaml') as f:
      experiment_config = yaml.load(f)

  experiment = SingleThreadedExperiment(
      config=experiment_config,
      db_engine=sqlalchemy.create_engine('postgresql://food_user:goli0808@food_db:5432/food'),
      model_storage_class=FSModelStorageEngine,
      project_path='./triage-generated'
  )

  experiment.run()
#+END_SRC


#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/code :results org drawer
  python run.py
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


* ▶ TODO Looking the results at Tyra


* What's next?

- Routing based on the inspection list?
- Add more data sources?

* Appendix: What are all those files?

* Appendix: Getting help

* Additional DBs

- [[https://data.cityofchicago.org/Community-Economic-Development/Business-Licenses/r5kz-chrr][Business Licenses]]
- Food Inspections
- [[https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2][Crime]]
- Garbage Cart Complaints
- [[https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Sanitation-Code-Complaints/me59-5fac][Sanitation Complaints]]
- Weather
- Sanitarian Information


* Questions

- How do I control the logging? I just want to see the info messages,
  not all

- How to interpret the table "event"?

- How to use additional tables?

- Could you draw an example of the temporal setting?
